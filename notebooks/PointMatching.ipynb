{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import to_numpy_array\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "import spaces\n",
    "import matplotlib.cm as cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image1, image2):\n",
    "    \"\"\"\n",
    "    Process two images and return a plot of the matching keypoints using matplotlib.\n",
    "    \"\"\"\n",
    "    if image1 is None or image2 is None:\n",
    "        return None\n",
    "\n",
    "    images = [image1, image2]\n",
    "    inputs = processor(images, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(model.device)\n",
    "    print(\n",
    "        \"Model is on device: \",\n",
    "        model.device,\n",
    "        \"and inputs are on device: \",\n",
    "        inputs[\"pixel_values\"].device,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    image_sizes = [[(image.height, image.width) for image in images]]\n",
    "    outputs = processor.post_process_keypoint_matching(\n",
    "        outputs, image_sizes, threshold=0.2\n",
    "    )\n",
    "    output = outputs[0]\n",
    "\n",
    "    image1 = to_numpy_array(image1)\n",
    "    image2 = to_numpy_array(image2)\n",
    "\n",
    "    height0, width0 = image1.shape[:2]\n",
    "    height1, width1 = image2.shape[:2]\n",
    "\n",
    "    # Create matplotlib figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Display images side by side\n",
    "    composite_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n",
    "    composite_image[:height0, :width0] = image1\n",
    "    composite_image[:height1, width0:width0+width1] = image2\n",
    "    \n",
    "    ax.imshow(composite_image)\n",
    "    \n",
    "    # Create colormap\n",
    "    colormap = cm.RdYlGn\n",
    "    \n",
    "    # Get keypoints\n",
    "    keypoints0_x, keypoints0_y = output[\"keypoints0\"].unbind(1)\n",
    "    keypoints1_x, keypoints1_y = output[\"keypoints1\"].unbind(1)\n",
    "    \n",
    "    # Plot matches\n",
    "    for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n",
    "        keypoints0_x,\n",
    "        keypoints0_y,\n",
    "        keypoints1_x,\n",
    "        keypoints1_y,\n",
    "        output[\"matching_scores\"],\n",
    "    ):\n",
    "        color_val = matching_score.item()\n",
    "        rgba_color = colormap(color_val)\n",
    "        \n",
    "        # Plot the line connecting the keypoints\n",
    "        ax.plot(\n",
    "            [keypoint0_x.item(), keypoint1_x.item() + width0],\n",
    "            [keypoint0_y.item(), keypoint1_y.item()],\n",
    "            color=rgba_color,\n",
    "            linewidth=1.5,\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Plot the keypoints\n",
    "        ax.plot(\n",
    "            keypoint0_x.item(), \n",
    "            keypoint0_y.item(), \n",
    "            'o', \n",
    "            color=rgba_color, \n",
    "            markersize=4\n",
    "        )\n",
    "        ax.plot(\n",
    "            keypoint1_x.item() + width0, \n",
    "            keypoint1_y.item(), \n",
    "            'o', \n",
    "            color=rgba_color, \n",
    "            markersize=4\n",
    "        )\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_xlim(0, width0 + width1)\n",
    "    ax.set_ylim(max(height0, height1), 0)  # Invert y-axis for image coordinates\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n",
    "model = AutoModel.from_pretrained(\"ETH-CVG/lightglue_superpoint\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n",
    "image1 = Image.open(requests.get(url_image1, stream=True).raw)\n",
    "url_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n",
    "image2 = Image.open(requests.get(url_image2, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_fig = process_images(image1, image2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralnet10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
