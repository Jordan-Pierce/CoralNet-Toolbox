{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorer Tool with Transformer Models\n",
    "\n",
    "This notebook demonstrates how to use transformer models with the Explorer tool in CoralNet-Toolbox.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Explorer tool now supports transformer-based feature extractors from HuggingFace, including:\n",
    "- DINOv2 (Small, Base, Large)\n",
    "- DINOv3 ConvNext\n",
    "- CLIP models\n",
    "- BioCLIP (biology-focused)\n",
    "- MariCLIP (marine-focused)\n",
    "- ResNet models\n",
    "- Swin Transformers\n",
    "- Vision Transformers (ViT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, ensure you have the transformers library installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers if not already installed\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using DINOv2 for Feature Extraction\n",
    "\n",
    "Here's how to use DINOv2 to extract features from coral images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\nfrom PIL import Image\nimport numpy as np\n\n# Initialize the feature extractor\nfeature_extractor = pipeline(\n    model=\"facebook/dinov2-small\",\n    task=\"image-feature-extraction\",\n    device=-1  # Use CPU, set to 0 for GPU\n)\n\n# Load an example image (replace with your coral image)\n# For demonstration, we'll create a simple test image\ntest_image = Image.new('RGB', (224, 224), color='blue')\n\n# Extract features\nfeatures = feature_extractor(test_image)\n\n# Convert to numpy array, handling GPU tensors properly\nif isinstance(features, list):\n    feature_tensor = features[0] if len(features) > 0 else features\nelse:\n    feature_tensor = features\n\n# Move to CPU if needed before converting to numpy\nif hasattr(feature_tensor, 'cpu'):\n    feature_vector = feature_tensor.cpu().numpy().flatten()\nelse:\n    feature_vector = np.array(feature_tensor).flatten()\n\nprint(f\"Feature vector shape: {feature_vector.shape}\")\nprint(f\"Feature vector sample: {feature_vector[:5]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformer Models in the Explorer GUI\n",
    "\n",
    "### Step 1: Launch CoralNet-Toolbox\n",
    "```bash\n",
    "coralnet-toolbox\n",
    "```\n",
    "\n",
    "### Step 2: Open the Explorer Tool\n",
    "Navigate to the Explorer tool from the main window.\n",
    "\n",
    "### Step 3: Select a Transformer Model\n",
    "1. In the Model Settings panel, select **Category: Transformer Model**\n",
    "2. Choose a model from the dropdown:\n",
    "   - **DINOv2 (Small)**: Fast, good for general feature extraction\n",
    "   - **DINOv2 (Base)**: Balanced performance\n",
    "   - **DINOv2 (Large)**: Best quality, slower\n",
    "   - **BioCLIP**: Optimized for biological images\n",
    "   - **MariCLIP**: Optimized for marine imagery\n",
    "   - **CLIP models**: Good for multi-modal understanding\n",
    "\n",
    "### Step 4: Apply Embedding\n",
    "Click \"Apply Embedding\" to extract features and visualize your data.\n",
    "\n",
    "## Model Recommendations\n",
    "\n",
    "### For Coral Reef Images:\n",
    "1. **BioCLIP**: Best for biological/ecological features\n",
    "2. **MariCLIP**: Specifically trained on marine data\n",
    "3. **DINOv2**: Strong general-purpose features\n",
    "\n",
    "### For Speed:\n",
    "- DINOv2 (Small)\n",
    "- ResNet-50\n",
    "- Swin Transformer (Tiny)\n",
    "\n",
    "### For Quality:\n",
    "- DINOv2 (Large)\n",
    "- ViT (Large)\n",
    "- Swin Transformer (Base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Models\n",
    "\n",
    "Here's how to compare features from different transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different models\nmodels_to_test = [\n    \"facebook/dinov2-small\",\n    \"microsoft/resnet-50\",\n    # Add more models as needed\n]\n\nresults = {}\n\nfor model_name in models_to_test:\n    print(f\"\\nTesting {model_name}...\")\n    try:\n        # Initialize pipeline\n        extractor = pipeline(\n            model=model_name,\n            task=\"image-feature-extraction\",\n            device=-1\n        )\n        \n        # Extract features\n        features = extractor(test_image)\n        \n        # Process features with proper GPU tensor handling\n        if isinstance(features, list):\n            feature_tensor = features[0] if len(features) > 0 else features\n        else:\n            feature_tensor = features\n        \n        # Move to CPU if needed before converting to numpy\n        if hasattr(feature_tensor, 'cpu'):\n            feature_vec = feature_tensor.cpu().numpy().flatten()\n        else:\n            feature_vec = np.array(feature_tensor).flatten()\n        \n        results[model_name] = feature_vec\n        print(f\"  ✓ Feature shape: {feature_vec.shape}\")\n        \n    except Exception as e:\n        print(f\"  ✗ Error: {e}\")\n\n# Compare feature dimensions\nprint(\"\\nFeature Dimensions Summary:\")\nfor model_name, features in results.items():\n    print(f\"  {model_name}: {features.shape[0]} dimensions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Using Transformer Models\n",
    "\n",
    "1. **Memory Management**: Transformer models can be memory-intensive. Monitor your GPU/CPU usage.\n",
    "\n",
    "2. **Batch Processing**: The Explorer tool automatically batches images for efficient processing.\n",
    "\n",
    "3. **Feature Caching**: Features are automatically cached to avoid recomputation.\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - Use smaller models (e.g., DINOv2 Small) for initial exploration\n",
    "   - Switch to larger models for final analysis when quality matters most\n",
    "\n",
    "5. **GPU Acceleration**: If available, the tool will automatically use GPU for faster processing.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### If models fail to load:\n",
    "- Ensure `transformers` is installed: `pip install transformers`\n",
    "- Check internet connection (models download on first use)\n",
    "- Verify sufficient disk space for model caching\n",
    "\n",
    "### For memory issues:\n",
    "- Use smaller models\n",
    "- Reduce batch size\n",
    "- Close other applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}