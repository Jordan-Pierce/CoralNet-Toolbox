{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reef Futures Notebook\n",
    "\n",
    "This Notebook is used to prepare the data used for the talk, \"Multi-view Label Classification in Viscore and TagLab for Precise and Efficient Estimates of Coral Cover using Large-area Imaging Data\". "
   ],
   "id": "229da4d82c909e7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "d032b753634b3e03"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n"
   ],
   "id": "575b4c1357502898",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Functions",
   "id": "59fac80c2f5ccd78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def filter_orthomosaic_csv_file(csv_file, mapping_dict):\n",
    "    \"\"\"\n",
    "    Filter the CSV Files based on the mapping dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dfs = pd.read_csv(csv_file)\n",
    "    filtered_dfs['Label'] = filtered_dfs['Label'].map(lambda x: mapping_dict[x]['ML'] if x in mapping_dict else x)\n",
    "    filtered_dfs['Name'] = filtered_dfs['Name'].map(os.path.basename)\n",
    "    \n",
    "    filtered_dfs.drop(filtered_dfs[filtered_dfs['Label'] == 'drop'].index, inplace=True)\n",
    "    filtered_dfs = filtered_dfs[['Name', 'Label', 'Row', 'Column']]\n",
    "    filtered_dfs.dropna(inplace=True)\n",
    "    filtered_dfs.reset_index(drop=True, inplace=True)\n",
    "    filtered_dfs['Row'] = filtered_dfs['Row'].astype(int)\n",
    "    filtered_dfs['Column'] = filtered_dfs['Column'].astype(int)\n",
    "    \n",
    "    return filtered_dfs\n",
    "\n",
    "def filter_image_csv_file_old(csv_file, mapping_dict, rand_sub_ceil=1.0, reprojection_error=0.5, view_index=10, view_count=1):\n",
    "    \"\"\"\n",
    "    Filter the CSV Files based on the mapping dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dfs = pd.read_csv(csv_file)\n",
    "    filtered_dfs['Label'] = filtered_dfs['Label'].map(lambda x: mapping_dict[x]['ML'] if x in mapping_dict else x)\n",
    "    filtered_dfs['Name'] = filtered_dfs['Name'].map(os.path.basename)\n",
    "\n",
    "    filtered_dfs.drop(filtered_dfs[filtered_dfs['Label'] == 'drop'].index, inplace=True)\n",
    "\n",
    "    mask = (\n",
    "            (filtered_dfs['RandSubCeil'] <= rand_sub_ceil) &\n",
    "            (filtered_dfs['ReprojectionError'] <= reprojection_error) &\n",
    "            (filtered_dfs['ViewIndex'] <= view_index) &\n",
    "            (filtered_dfs['ViewCount'] >= view_count)\n",
    "    )\n",
    "    filtered_dfs = filtered_dfs[mask]\n",
    "\n",
    "    filtered_dfs = filtered_dfs[['Name', 'Label', 'Row', 'Column']]\n",
    "    filtered_dfs.dropna(inplace=True)\n",
    "    filtered_dfs.reset_index(drop=True, inplace=True)\n",
    "    filtered_dfs['Row'] = filtered_dfs['Row'].astype(int)\n",
    "    filtered_dfs['Column'] = filtered_dfs['Column'].astype(int)\n",
    "\n",
    "    return filtered_dfs\n",
    "\n",
    "def filter_image_csv_file(csv_file, mapping_dict, views=1):\n",
    "    \"\"\"\n",
    "    Filter the CSV Files based on the mapping dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dfs = pd.read_csv(csv_file)\n",
    "    filtered_dfs['Label'] = filtered_dfs['Label'].map(lambda x: mapping_dict[x]['ML'] if x in mapping_dict else x)\n",
    "    filtered_dfs['Name'] = filtered_dfs['Name'].map(os.path.basename)\n",
    "    \n",
    "    filtered_dfs.drop(filtered_dfs[filtered_dfs['Label'] == 'drop'].index, inplace=True)\n",
    "    \n",
    "    filtered = []\n",
    "    for dot in filtered_dfs['Dot'].unique():\n",
    "        subset = filtered_dfs[filtered_dfs['Dot'] == dot]\n",
    "        # Calculate the mean of the reprojection error\n",
    "        mean = subset['ReprojectionError'].mean()\n",
    "        # Filter the subset to get only rows with reprojection error less than the mean\n",
    "        subset = subset[subset['ReprojectionError'] <= mean]\n",
    "        # Get the new mean and std\n",
    "        std = subset['ReprojectionError'].std()\n",
    "        mean = subset['ReprojectionError'].mean()\n",
    "        # Subset to get only rows within +/- one standard deviation of the mean\n",
    "        subset = subset[(subset['ReprojectionError'] >= (mean - std)) & \n",
    "                        (subset['ReprojectionError'] <= (mean + std))]\n",
    "        # Sort based on reprpjection error and ViewIndex, ascending\n",
    "        subset = subset.sort_values(['ReprojectionError', 'ViewIndex'], ascending=[True, True])\n",
    "        # Get the first N views\n",
    "        subset = subset.head(views)\n",
    "        filtered.append(subset)\n",
    "        \n",
    "    filtered_dfs = pd.concat(filtered)\n",
    "    filtered_dfs.dropna(inplace=True)\n",
    "    filtered_dfs.reset_index(drop=True, inplace=True)\n",
    "    filtered_dfs['Row'] = filtered_dfs['Row'].astype(int)\n",
    "    filtered_dfs['Column'] = filtered_dfs['Column'].astype(int)\n",
    "    \n",
    "    return filtered_dfs"
   ],
   "id": "eb96e366150b6b24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_class_mapping_json(dataframe, directory):\n",
    "    \"\"\"\n",
    "    Outputs a JSON file with the class mapping.\n",
    "    Class mapping is a dictionary with the class name as the\n",
    "    key, which contains a dictionary containing short_code_label,\n",
    "    long_code_label, id (uuid), and color [R, G, B].\n",
    "    \"\"\"\n",
    "    class_mapping = {}\n",
    "    for label in dataframe['Label'].unique():\n",
    "        if label not in class_mapping:\n",
    "            class_mapping[label] = {\n",
    "                \"short_label_code\": label,\n",
    "                \"long_label_code\": label,\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"color\": [np.random.randint(0, 255) for _ in range(3)] + [255]\n",
    "            }\n",
    "    with open(f\"{directory}/class_mapping.json\", \"w\") as f:\n",
    "        json.dump(class_mapping, f, indent=4)"
   ],
   "id": "a44c9d8139e90f56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_dataframes(dataframes, split, directory):\n",
    "    \"\"\"\n",
    "    Combines a list of dataframes and saves them as a CSV file.\n",
    "    Plots a distribution of the labels.\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(dataframes)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    combined_df['Label'].value_counts().plot(kind='bar', title=f\"{split} Label Distribution\")\n",
    "    plt.savefig(f\"{directory}/{split}_label_distribution.png\")\n",
    "    \n",
    "def save_dataframe(dataframe, site_name, directory):\n",
    "    \"\"\"\n",
    "    Save the dataframe to a CSV file.\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(f\"{directory}/{site_name}.csv\", index=False)"
   ],
   "id": "525ed0ac33bc592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_cropped_image(rasterio_src, annotation_size, row, column):\n",
    "    half_size = annotation_size // 2\n",
    "    pixel_x, pixel_y = int(column), int(row)\n",
    "    window = Window(\n",
    "        col_off=max(0, pixel_x - half_size),\n",
    "        row_off=max(0, pixel_y - half_size),\n",
    "        width=min(rasterio_src.width - (pixel_x - half_size), annotation_size),\n",
    "        height=min(rasterio_src.height - (pixel_y - half_size), annotation_size)\n",
    "    )\n",
    "    \n",
    "    data = rasterio_src.read(window=window)\n",
    "    \n",
    "    if data.shape[0] == 3:\n",
    "        data = np.transpose(data, (1, 2, 0))\n",
    "    elif data.shape[0] == 1:\n",
    "        data = np.squeeze(data)\n",
    "    \n",
    "    if data.dtype != np.uint8:\n",
    "        data = ((data - data.min()) / (data.max() - data.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(data)\n",
    "\n",
    "def process_annotation_chunk(src, annotations_chunk, size, dataset_dir, split):\n",
    "    results = []\n",
    "    for _, annotation in annotations_chunk.iterrows():\n",
    "        label = annotation['Label']\n",
    "        row, column = annotation['Row'], annotation['Column']\n",
    "        output_path = os.path.join(dataset_dir, split, label)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        output_filename = f\"{label}_{row}_{column}_{uuid.uuid4()}.jpg\"\n",
    "        full_output_path = os.path.join(output_path, output_filename)\n",
    "        \n",
    "        try:\n",
    "            cropped_image = create_cropped_image(src, size, row, column)\n",
    "            if cropped_image:\n",
    "                results.append((cropped_image, full_output_path))\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue processing annotation at row {row}, column {column}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_image_batch(image_path, annotations, size, dataset_dir, split, chunk_size=100):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        for i in range(0, len(annotations), chunk_size):\n",
    "            chunk = annotations.iloc[i:i+chunk_size]\n",
    "            chunk_results = process_annotation_chunk(src, chunk, size, dataset_dir, split)\n",
    "            save_images(chunk_results)\n",
    "            del chunk_results\n",
    "            gc.collect()\n",
    "\n",
    "def save_images(cropped_images):\n",
    "    for image, path in cropped_images:\n",
    "        try:\n",
    "            image.save(path, format='JPEG', optimize=True, quality=85)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue saving image {path}: {e}\")\n",
    "            png_path = path.replace(\".jpg\", \".png\")\n",
    "            image.save(png_path, optimize=True)\n",
    "        finally:\n",
    "            image.close()\n",
    "\n",
    "def create_dataset(image_paths, dataframe, size, class_labels, dataset_dir, split, chunk_size=1000):\n",
    "    \n",
    "    # Create blank JPEGs for each class\n",
    "    for label in class_labels:\n",
    "        os.makedirs(f\"{dataset_dir}/{split}/{label}\", exist_ok=True)\n",
    "        img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "        Image.fromarray(img).save(f\"{dataset_dir}/{split}/{label}/NULL.jpg\", format='JPEG', optimize=True, quality=85)\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count() // 2) as executor:\n",
    "        futures = []\n",
    "        for image_path in image_paths:\n",
    "            image_basename = os.path.basename(image_path)\n",
    "            annotations = dataframe[dataframe['Name'] == image_basename]\n",
    "            future = executor.submit(process_image_batch, \n",
    "                                     image_path, \n",
    "                                     annotations, \n",
    "                                     size, \n",
    "                                     dataset_dir, \n",
    "                                     split,\n",
    "                                     chunk_size)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {split} images\"):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'An image generated an exception: {exc}')"
   ],
   "id": "e6c044724cea227c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Directory Structure",
   "id": "4e02f1ce79f99278"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "root = rf\"W:/MIR_AI\"\n",
    "study_dir = f\"{root}/preliminary_study_30\"\n",
    "\n",
    "singleview_dir = f\"{study_dir}/SingleView\"\n",
    "singleview_dataset_dir = f\"{singleview_dir}/Datasets\"\n",
    "singleview_classification_dir = f\"{singleview_dir}/Classification\"\n",
    "singleview_evaluation_dir = f\"{singleview_dir}/Evaluation\"\n",
    "singleview_filtered_dir = f\"{singleview_dir}/Filtered\"\n",
    "os.makedirs(singleview_dataset_dir, exist_ok=True)\n",
    "os.makedirs(singleview_filtered_dir, exist_ok=True)\n",
    "os.makedirs(singleview_classification_dir, exist_ok=True)\n",
    "os.makedirs(singleview_evaluation_dir, exist_ok=True)\n",
    "\n",
    "multiview_dir = f\"{study_dir}/MultiView\"\n",
    "multiview_dataset_dir = f\"{multiview_dir}/Datasets\"\n",
    "multiview_classification_dir = f\"{multiview_dir}/Classification\"\n",
    "multiview_evaluation_dir = f\"{multiview_dir}/Evaluation\"\n",
    "multiview_filtered_dir = f\"{multiview_dir}/Filtered\"\n",
    "os.makedirs(multiview_dataset_dir, exist_ok=True)\n",
    "os.makedirs(multiview_filtered_dir, exist_ok=True)\n",
    "os.makedirs(multiview_classification_dir, exist_ok=True)\n",
    "os.makedirs(multiview_evaluation_dir, exist_ok=True)\n",
    "\n",
    "orthomosaic_dir = f\"{study_dir}/OrthoView\"\n",
    "orthomosaic_dataset_dir = f\"{orthomosaic_dir}/Datasets\"\n",
    "orthomosaic_classification_dir = f\"{orthomosaic_dir}/Classification\"\n",
    "orthomosaic_evaluation_dir = f\"{orthomosaic_dir}/Evaluation\"\n",
    "orthomosaic_filtered_dir = f\"{orthomosaic_dir}/Filtered\"\n",
    "os.makedirs(orthomosaic_dataset_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_filtered_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_classification_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_evaluation_dir, exist_ok=True)"
   ],
   "id": "f1330c7e289f4573",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mapping_file = f\"{root}/Reef_Futures_Mapping_v1.json\"\n",
    "assert os.path.exists(mapping_file), \"ERROR: Mapping file not found.\"\n",
    "\n",
    "mapping_dict = json.load(open(mapping_file))\n",
    "print(f\"Num Keys: {len(mapping_dict)}\")\n",
    "\n",
    "# Find the amount that are being discarded\n",
    "num_drop = len([k for k, v in mapping_dict.items() if v['ML'] == 'drop'])\n",
    "print(f\"Num Valid Keys: {len(mapping_dict) - num_drop}\")\n",
    "print(f\"Num Dropped Keys: {num_drop}\")"
   ],
   "id": "c778bc8fb6183faa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Site Splits\n",
    "\n",
    "Split the sites (n=???) into training / testing sets (80/20). "
   ],
   "id": "8e8ab5e0f06c9a93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assert that All_Sites.txt exists\n",
    "all_sites_txt = rf\"{root}/All_Sites_Preliminary_30.txt\"\n",
    "assert os.path.exists(all_sites_txt), f\"ERROR: Missing {all_sites_txt}\"\n",
    "\n",
    "# Open the txt file, store as a list\n",
    "with open(all_sites_txt) as f:\n",
    "    all_sites_list = f.read().splitlines()\n",
    "    \n",
    "assert all_sites_list, f\"ERROR: No data found in {all_sites_txt}\"\n",
    "num_sites = len(all_sites_list)\n",
    "print(\"Num Sites: \", num_sites)\n",
    "print(\"Sites: \", all_sites_list)"
   ],
   "id": "264b525a0133d753",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the ratios\n",
    "training_ratio, validation_ratio, testing_ratio = 0.70, 0.15, 0.15\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert training_ratio + validation_ratio + testing_ratio == 1.0, \"Ratios must sum to 1.\"\n",
    "\n",
    "# Split into training and temp (validation + testing)\n",
    "training_sites_list, temp_sites_list = train_test_split(all_sites_list, test_size=(1 - training_ratio), random_state=42)\n",
    "\n",
    "# Split the temp set into validation and testing with equal sizes\n",
    "validation_sites_list, testing_sites_list = train_test_split(temp_sites_list, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the number of sites in each set to verify the ratios\n",
    "print(f\"Num Training Sites: {len(training_sites_list)}\")\n",
    "print(f\"Num Validation Sites: {len(validation_sites_list)}\")\n",
    "print(f\"Num Testing Sites: {len(testing_sites_list)}\")"
   ],
   "id": "ef75b96129312c0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training and Testing (Images)",
   "id": "ad29c537da51d5a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_image_data_dict = {site: {} for site in training_sites_list}\n",
    "validation_image_data_dict = {site: {} for site in validation_sites_list}\n",
    "testing_image_data_dict = {site: {} for site in testing_sites_list}\n",
    "\n",
    "image_folders = glob.glob(rf\"{root}/images/*\")\n",
    "image_folders = [f for f in image_folders for site in all_sites_list if site in f]\n",
    "num_image_folders = len(image_folders)\n",
    "missing_image_folders = []\n",
    "\n",
    "for site in all_sites_list:\n",
    "    found = False\n",
    "    for image_folder in image_folders:\n",
    "        if site in image_folder:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_image_folders.append(site)\n",
    "        \n",
    "assert num_image_folders == num_sites, f\"ERROR: Missing Image Folders: {missing_image_folders}\"\n",
    "print(f\"Num Image Folders: {num_image_folders}\")\n",
    "\n",
    "for image_folder in image_folders:\n",
    "    site = os.path.basename(image_folder).split(\"_JPEG\")[0]\n",
    "    if site in training_sites_list:\n",
    "        training_image_data_dict[site]['image_folder'] = image_folder\n",
    "    elif site in validation_sites_list:\n",
    "        validation_image_data_dict[site]['image_folder'] = image_folder\n",
    "    elif site in testing_sites_list:\n",
    "        testing_image_data_dict[site]['image_folder'] = image_folder\n",
    "\n",
    "image_point_files = glob.glob(rf\"{root}/image_points/*.csv\")\n",
    "image_point_files = [f for f in image_point_files for site in all_sites_list if site in f]\n",
    "num_image_point_files = len(image_point_files)\n",
    "missing_image_point_files = []\n",
    "\n",
    "for site in all_sites_list:\n",
    "    found = False\n",
    "    for image_point_file in image_point_files:\n",
    "        if site in image_point_file:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_image_point_files.append(site)\n",
    "\n",
    "assert num_image_point_files == num_sites, f\"ERROR: Missing Image Point Files: {missing_image_point_files}\"\n",
    "print(f\"Num Image Point Files: {num_image_point_files}\")\n",
    "\n",
    "for image_point_file in image_point_files:\n",
    "    site = os.path.basename(image_point_file).split(\".csv\")[0]\n",
    "    if site in training_sites_list:\n",
    "        training_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "    elif site in validation_sites_list:\n",
    "        validation_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "    elif site in testing_sites_list:\n",
    "        testing_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "\n",
    "print(f\"Num Training Data Dict: {len(training_image_data_dict)}\")\n",
    "print(f\"Num Validation Data Dict: {len(validation_image_data_dict)}\")\n",
    "print(f\"Num Testing Data Dict: {len(testing_image_data_dict)}\")"
   ],
   "id": "2d18e59b8502bc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training and Testing (Orthomosaics)",
   "id": "b5a5c49d955b3375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_orthomosaic_data_dict = {site: {} for site in training_sites_list}\n",
    "validation_orthomosaic_data_dict = {site: {} for site in validation_sites_list}\n",
    "testing_orthomosaic_data_dict = {site: {} for site in testing_sites_list}\n",
    "\n",
    "orthomosaic_files = glob.glob(rf\"{root}/orthomosaics/*.tif*\")\n",
    "orthomosaic_files = [f for f in orthomosaic_files for site in all_sites_list if site in f]\n",
    "num_orthomosaic_files = len(orthomosaic_files)\n",
    "missing_orthomosaic_files = []\n",
    "\n",
    "for site in all_sites_list:\n",
    "    found = False\n",
    "    for orthomosaic_file in orthomosaic_files:\n",
    "        if site in orthomosaic_file:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_orthomosaic_files.append(site)\n",
    "        \n",
    "assert num_orthomosaic_files == num_sites, f\"ERROR: Missing Orthomosaic Files: {missing_orthomosaic_files}\"\n",
    "print(f\"Num Orthomosaic Files: {num_orthomosaic_files}\")\n",
    "\n",
    "for site in training_sites_list:\n",
    "    for orthomosaic_file in orthomosaic_files:\n",
    "        if site in orthomosaic_file:\n",
    "            training_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "            \n",
    "for site in validation_sites_list:\n",
    "    for orthomosaic_file in orthomosaic_files:\n",
    "        if site in orthomosaic_file:\n",
    "            validation_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "\n",
    "for site in testing_sites_list:\n",
    "    for orthomosaic_file in orthomosaic_files:\n",
    "        if site in orthomosaic_file:\n",
    "            testing_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "        \n",
    "orthomosaic_point_files = glob.glob(rf\"{root}/orthomosaic_points/**/*.csv\", recursive=True)\n",
    "orthomosaic_point_files = [f for f in orthomosaic_point_files for site in all_sites_list if site in f]\n",
    "num_orthomosaic_point_files = len(orthomosaic_point_files)\n",
    "missing_orthomosaic_point_files = []\n",
    "\n",
    "for site in all_sites_list:\n",
    "    found = False\n",
    "    for orthomosaic_point_file in orthomosaic_point_files:\n",
    "        if site in orthomosaic_point_file:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        missing_orthomosaic_point_files.append(site)\n",
    "        \n",
    "assert num_orthomosaic_point_files >= num_sites, f\"ERROR: Missing Orthomosaic Point Files: {missing_orthomosaic_point_files}\"\n",
    "print(f\"Num Orthomosaic Point Files: {num_orthomosaic_point_files}\")\n",
    "\n",
    "for site in training_sites_list:\n",
    "    for orthomosaic_point_file in orthomosaic_point_files:\n",
    "        if site in orthomosaic_point_file:\n",
    "            training_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "            \n",
    "for site in validation_sites_list:\n",
    "    for orthomosaic_point_file in orthomosaic_point_files:\n",
    "        if site in orthomosaic_point_file:\n",
    "            validation_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "            \n",
    "for site in testing_sites_list:\n",
    "    for orthomosaic_point_file in orthomosaic_point_files:\n",
    "        if site in orthomosaic_point_file:\n",
    "            testing_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "\n",
    "print(f\"Num Training Orthomosaic Data Dict: {len(training_orthomosaic_data_dict)}\")\n",
    "print(f\"Num Validation Orthomosaic Data Dict: {len(validation_orthomosaic_data_dict)}\")\n",
    "print(f\"Num Testing Orthomosaic Data Dict: {len(testing_orthomosaic_data_dict)}\")"
   ],
   "id": "16abb825fc063fab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating Singleview Datasets",
   "id": "135884b5e710059b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotation_size = 336\n",
    "index_view = 1\n",
    "view_count = 1\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    training_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    validation_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "\n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    testing_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "    \n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Image Dataframe: {sum([len(data['dataframe']) for data in training_image_data_dict.values()])}\")\n",
    "print(f\"Validation Image Dataframe: {sum([len(data['dataframe']) for data in validation_image_data_dict.values()])}\")\n",
    "print(f\"Testing Image Dataframe: {sum([len(data['dataframe']) for data in testing_image_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "singleview_dataframes_combined = []\n",
    "\n",
    "for site, data in training_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in testing_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "singleview_dataframes_combined = pd.concat(singleview_dataframes_combined)\n",
    "create_class_mapping_json(singleview_dataframes_combined, singleview_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_image_data_dict.values()], \"train\", singleview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_image_data_dict.values()], \"val\", singleview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_image_data_dict.values()], \"test\", singleview_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, singleview_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, singleview_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, singleview_dataset_dir, \"test\")"
   ],
   "id": "4c12a826d0f4dd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating Multiview Datasets",
   "id": "5779fd8aa53fac6f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "annotation_size = 336\n",
    "index_view = 10\n",
    "view_count = 1\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=10)\n",
    "    training_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=10)\n",
    "    validation_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], mapping_dict, views=10)\n",
    "    testing_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Image Dataframe: {sum([len(data['dataframe']) for data in training_image_data_dict.values()])}\")\n",
    "print(f\"Validation Image Dataframe: {sum([len(data['dataframe']) for data in validation_image_data_dict.values()])}\")\n",
    "print(f\"Testing Image Dataframe: {sum([len(data['dataframe']) for data in testing_image_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "multiview_dataframes_combined = []\n",
    "\n",
    "for site, data in training_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in testing_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "multiview_dataframes_combined = pd.concat(multiview_dataframes_combined)\n",
    "create_class_mapping_json(multiview_dataframes_combined, multiview_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_image_data_dict.values()], \"train\", multiview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_image_data_dict.values()], \"val\", multiview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_image_data_dict.values()], \"test\", multiview_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, multiview_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, multiview_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    image_paths = glob.glob(f\"{data['image_folder']}/*\")\n",
    "    create_dataset(image_paths, dataframe, annotation_size, class_labels, multiview_dataset_dir, \"test\")"
   ],
   "id": "bc4b93a5076d1dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating Orthomosaic Datasets",
   "id": "3390e8c4214c95ae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "annotation_size = 512\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_orthomosaic_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    training_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_orthomosaic_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    validation_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "        \n",
    "for site, data in tqdm(testing_orthomosaic_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    testing_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "\n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in training_orthomosaic_data_dict.values()])}\")\n",
    "print(f\"Validation Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in validation_orthomosaic_data_dict.values()])}\")\n",
    "print(f\"Testing Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in testing_orthomosaic_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "orthomosaic_dataframes_combined = []\n",
    "\n",
    "for site, data in training_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "for site, data in testing_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "orthomosaic_dataframes_combined = pd.concat(orthomosaic_dataframes_combined)\n",
    "create_class_mapping_json(orthomosaic_dataframes_combined, orthomosaic_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_orthomosaic_data_dict.values()], \"train\", orthomosaic_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_orthomosaic_data_dict.values()], \"val\", orthomosaic_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_orthomosaic_data_dict.values()], \"test\", orthomosaic_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_orthomosaic_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_orthomosaic_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_orthomosaic_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"test\")"
   ],
   "id": "1e18e34cced1cbda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Done.\")",
   "id": "5119390104a7c816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f6ffb1ab38116e01",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
