{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1f0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLOE\n",
    "from ultralytics.models.yolo.yoloe import YOLOEVPSegPredictor, YOLOEVPDetectPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b84852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a YOLOE segmentation model\n",
    "model = YOLOE(\"yoloe-11s-seg.pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f916cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Define prompts for reference images ---\n",
    "# Prompts for the first reference image\n",
    "visual_prompts_1 = dict(\n",
    "    bboxes=np.array(\n",
    "        [\n",
    "            [215, 392, 350, 864],  # Box enclosing person\n",
    "        ],\n",
    "    ),\n",
    "    cls=np.array(\n",
    "        [\n",
    "            0,  # ID to be assigned for person\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Prompts for a second reference image (using bus.jpg again for simplicity)\n",
    "visual_prompts_2 = dict(\n",
    "    bboxes=np.array(\n",
    "        [\n",
    "            [41, 391, 246, 905],  # Box enclosing person\n",
    "        ],\n",
    "    ),\n",
    "    cls=np.array(\n",
    "        [\n",
    "            0,  # ID to be assigned for person\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# # Prompts for a second reference image (using bus.jpg again for simplicity)\n",
    "visual_prompts_3 = dict(\n",
    "    bboxes=np.array(\n",
    "        [\n",
    "            [1, 535, 82, 879],  # Box enclosing person\n",
    "        ],\n",
    "    ),\n",
    "    cls=np.array(\n",
    "        [\n",
    "            0,  # ID to be assigned for person\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Prompts for a second reference image (using bus.jpg again for simplicity)\n",
    "visual_prompts_4 = dict(\n",
    "    bboxes=np.array(\n",
    "        [\n",
    "            [666, 385, 807, 875],  # Box enclosing person\n",
    "        ],\n",
    "    ),\n",
    "    cls=np.array(\n",
    "        [\n",
    "            0,  # ID to be assigned for person\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29631e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Manually create a predictor instance ---\n",
    "results = model.predict(\n",
    "    \"bus.jpg\",\n",
    "    visual_prompts=visual_prompts_1,\n",
    "    predictor=YOLOEVPDetectPredictor,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb754bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = YOLOEVPDetectPredictor()\n",
    "predictor.setup_model(\"yoloe-11s-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c936e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_prompts([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.get_vpe(\"bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb72761",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Generate and collect VPEs from reference images ---\n",
    "import time\n",
    "\n",
    "vpe_list = []\n",
    "print(\"Generating VPEs from reference images...\")\n",
    "\n",
    "# Generate VPE for the first image\n",
    "start = time.time()\n",
    "model.predictor.set_prompts(visual_prompts_1)\n",
    "vpe1 = model.predictor.get_vpe(\"bus.jpg\")\n",
    "elapsed = time.time() - start\n",
    "vpe_list.append(vpe1)\n",
    "print(f\"Generated VPE from the first prompt. Time: {elapsed:.3f} s\")\n",
    "\n",
    "# Generate VPE for the second image\n",
    "start = time.time()\n",
    "model.predictor.set_prompts(visual_prompts_2)\n",
    "vpe2 = model.predictor.get_vpe(\"bus.jpg\")\n",
    "elapsed = time.time() - start\n",
    "vpe_list.append(vpe2)\n",
    "print(f\"Generated VPE from the second prompt. Time: {elapsed:.3f} s\")\n",
    "\n",
    "# Generate VPE for the third image\n",
    "start = time.time()\n",
    "model.predictor.set_prompts(visual_prompts_3)\n",
    "vpe3 = model.predictor.get_vpe(\"bus.jpg\")\n",
    "elapsed = time.time() - start\n",
    "vpe_list.append(vpe3)\n",
    "print(f\"Generated VPE from the third prompt. Time: {elapsed:.3f} s\")\n",
    "\n",
    "# Generate VPE for the fourth image\n",
    "start = time.time()\n",
    "model.predictor.set_prompts(visual_prompts_4)\n",
    "vpe4 = model.predictor.get_vpe(\"bus.jpg\")\n",
    "elapsed = time.time() - start\n",
    "vpe_list.append(vpe4)\n",
    "print(f\"Generated VPE from the fourth prompt. Time: {elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ae83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ea2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Aggregate the VPEs ---\n",
    "# Combine the list of VPE tensors and average them\n",
    "final_vpe = torch.cat(vpe_list).mean(dim=0, keepdim=True)\n",
    "\n",
    "# Normalize the final embedding\n",
    "final_vpe = torch.nn.functional.normalize(final_vpe, p=2, dim=-1)\n",
    "print(\"\\nSuccessfully combined VPEs into a single, normalized embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff02f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Visualize VPEs using PCA ---\n",
    "\n",
    "# Convert tensors to numpy arrays for PCA\n",
    "vpe_arrays = [vpe.detach().cpu().numpy().squeeze() for vpe in vpe_list]\n",
    "final_vpe_array = final_vpe.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# Stack all embeddings for PCA\n",
    "all_vpes = np.vstack(vpe_arrays + [final_vpe_array])\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "vpes_2d = pca.fit_transform(all_vpes)\n",
    "\n",
    "# Create a simple scatter plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot individual VPEs\n",
    "plt.scatter(vpes_2d[0, 0], vpes_2d[0, 1], color='blue', s=100, label='VPE 1')\n",
    "plt.scatter(vpes_2d[1, 0], vpes_2d[1, 1], color='green', s=100, label='VPE 2')\n",
    "plt.scatter(vpes_2d[2, 0], vpes_2d[2, 1], color='orange', s=100, label='VPE 3')\n",
    "plt.scatter(vpes_2d[3, 0], vpes_2d[3, 1], color='purple', s=100, label='VPE 4')\n",
    "\n",
    "# Plot the final (averaged) VPE\n",
    "plt.scatter(vpes_2d[4, 0], vpes_2d[4, 1], color='red', s=200, marker='*', label='Final VPE')\n",
    "\n",
    "# Add labels and information\n",
    "plt.title('PCA Visualization of Visual Prompt Embeddings', fontsize=15)\n",
    "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Add a text annotation with information about the original dimensionality\n",
    "orig_dim = vpe_list[0].shape[1]\n",
    "plt.figtext(0.5, 0.01, \n",
    "            f\"Original dimension: {orig_dim} â†’ Reduced to 2D\\n\"\n",
    "            f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\",\n",
    "            ha='center')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to accommodate the text\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206724d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Run inference using the aggregated VPE ---\n",
    "\n",
    "# Directly set the final tensor as the prompt for the predictor.\n",
    "# The `inference` method is designed to use this tensor.\n",
    "model.is_fused = lambda: False\n",
    "model.set_classes([\"person\"], final_vpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\"zidane.jpg\",\n",
    "                visual_prompts=[],\n",
    "                predictor=YOLOEVPDetectPredictor)\n",
    "\n",
    "print(\"Inference complete. Results are generated.\")\n",
    "# You can now process or display the 'results'.\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model is used with .predict, it's predictor is no longer a YOLOEVPDetectPredictor!\n",
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe632566",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\"bus.jpg\")\n",
    "\n",
    "print(\"Inference complete. Results are generated.\")\n",
    "# You can now process or display the 'results'.\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3da890",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(\"people_1.jpg\")\n",
    "\n",
    "print(\"Inference complete. Results are generated.\")\n",
    "# You can now process or display the 'results'.\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf640a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model), type(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00ed7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e77de69",
   "metadata": {},
   "source": [
    "### Exporting Final VPEs and Re-importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final_vpe tensor to disk\n",
    "\n",
    "# Move to CPU before saving to ensure compatibility when loading\n",
    "final_vpe_cpu = final_vpe.cpu()\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(final_vpe_cpu, \"vpe.pt\")\n",
    "print(f\"Saved VPE tensor to person_vpe.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import the VPE tensor\n",
    "loaded_vpe = torch.load(\"vpe.pt\")\n",
    "\n",
    "# Move to the appropriate device if needed (e.g., GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_vpe = loaded_vpe.to(device)\n",
    "\n",
    "print(f\"Loaded VPE tensor from disk. Shape: {loaded_vpe.shape}\")\n",
    "print(f\"Device: {loaded_vpe.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815dd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a YOLOE segmentation model\n",
    "model_2 = None\n",
    "model_2 = YOLOE(\"yoloe-11s-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb452984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for the first reference image\n",
    "visual_prompts_1 = dict(\n",
    "    bboxes=np.array(\n",
    "        [\n",
    "            [215, 392, 350, 864],  # Box enclosing person\n",
    "        ],\n",
    "    ),\n",
    "    cls=np.array(\n",
    "        [\n",
    "            0,  # ID to be assigned for person\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_2.predict(\n",
    "    \"bus.jpg\",\n",
    "    visual_prompts=visual_prompts_1,\n",
    "    predictor=YOLOEVPDetectPredictor,\n",
    ")\n",
    "\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d66619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Run inference using the aggregated VPE ---\n",
    "\n",
    "# Directly set the final tensor as the prompt for the predictor.\n",
    "# The `inference` method is designed to use this tensor.\n",
    "model_2.is_fused = lambda: False\n",
    "model_2.set_classes([\"person\"], loaded_vpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e730bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_2.predict(\"bus.jpg\",\n",
    "                          visual_prompts=[],  # Providing new visual prompts this way will overwrite the set VPEs \n",
    "                          predictor=YOLOEVPDetectPredictor,  # Okay to keep\n",
    "                          )\n",
    "\n",
    "print(\"Inference complete. Results are generated.\")\n",
    "# You can now process or display the 'results'.\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_2.predict(\"zidane.jpg\",\n",
    "                          visual_prompts=[],  # Providing new visual prompts this way will overwrite the set VPEs \n",
    "                          predictor=YOLOEVPDetectPredictor,  # Okay to keep\n",
    "                          )\n",
    "\n",
    "print(\"Inference complete. Results are generated.\")\n",
    "# You can now process or display the 'results'.\n",
    "results[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89b3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralnet10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
