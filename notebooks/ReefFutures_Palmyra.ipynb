{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229da4d82c909e7c",
   "metadata": {},
   "source": [
    "# Reef Futures Notebook\n",
    "\n",
    "This Notebook is used to prepare the data used for the talk, \"Multi-view Label Classification in Viscore and TagLab for Precise and Efficient Estimates of Coral Cover using Large-area Imaging Data\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032b753634b3e03",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "575b4c1357502898",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import io\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59fac80c2f5ccd78",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "30882830-f0e4-41f6-9df6-3233bc6d14c8",
   "metadata": {},
   "source": [
    "def get_path_from_image_point_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the first line (header)\n",
    "        header = next(file).strip().split(',')\n",
    "        \n",
    "        # Read the second line (first data row)\n",
    "        first_data_line = next(file).strip()\n",
    "        \n",
    "        # Skip empty rows and grab the next non-empty row\n",
    "        while not first_data_line:\n",
    "            first_data_line = next(file).strip()\n",
    "        \n",
    "        # Split the non-empty row into a list\n",
    "        first_data_line = first_data_line.split(',')\n",
    "    \n",
    "    # Create a DataFrame with the header and the first non-empty data row\n",
    "    df = pd.DataFrame([first_data_line], columns=header)\n",
    "    \n",
    "    return os.path.dirname(df['Name'][0])\n",
    "\n",
    "\n",
    "def fix_orthomosaic_csv_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Modify the header line (line 0) by appending a comma\n",
    "    lines[0] = lines[0].strip() + ',\\n'\n",
    "    \n",
    "    # Join the lines back together to form a single string\n",
    "    modified_csv_string = ''.join(lines)\n",
    "    \n",
    "    # Use pandas to read the modified string as a CSV\n",
    "    df = pd.read_csv(io.StringIO(modified_csv_string))\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb96e366150b6b24",
   "metadata": {},
   "source": [
    "def filter_orthomosaic_csv_file(csv_file, mapping_dict):\n",
    "    \"\"\"\n",
    "    Filter the CSV Files based on the mapping dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dfs = fix_orthomosaic_csv_file(csv_file)\n",
    "    filtered_dfs['Label'] = filtered_dfs['Label'].map(lambda x: mapping_dict[x])\n",
    "    filtered_dfs['Name'] = os.path.basename(csv_file).replace(\".csv\", \".tif\")\n",
    "    \n",
    "    filtered_dfs.drop(filtered_dfs[filtered_dfs['Label'] == 'drop'].index, inplace=True)\n",
    "    filtered_dfs = filtered_dfs[['Name', 'Label', 'Row', 'Column']]\n",
    "    filtered_dfs.dropna(inplace=True)\n",
    "    filtered_dfs.reset_index(drop=True, inplace=True)\n",
    "    filtered_dfs['Row'] = filtered_dfs['Row'].astype(int)\n",
    "    filtered_dfs['Column'] = filtered_dfs['Column'].astype(int)\n",
    "    \n",
    "    return filtered_dfs\n",
    "\n",
    "def filter_image_csv_file(csv_file, images_root, mapping_dict, views=1):\n",
    "    \"\"\"\n",
    "    Filter the CSV Files based on the mapping dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dfs = pd.read_csv(csv_file)\n",
    "    # Contains the full path, not just the basename\n",
    "    filtered_dfs['Name'] = [f\"{images_root}/{name}\" for name in filtered_dfs['Name'].values]\n",
    "    filtered_dfs['Name'] = [n.replace(\"\\\\\", \"/\") for n in filtered_dfs['Name'].values]\n",
    "    \n",
    "    filtered_dfs['Label'] = filtered_dfs['Label'].map(lambda x: mapping_dict[x])\n",
    "    filtered_dfs.drop(filtered_dfs[filtered_dfs['Label'] == 'drop'].index, inplace=True)\n",
    "    \n",
    "    filtered = []\n",
    "    for dot in filtered_dfs['Dot'].unique():\n",
    "        subset = filtered_dfs[filtered_dfs['Dot'] == dot]\n",
    "        # Calculate the mean of the reprojection error\n",
    "        mean = subset['ReprojectionError'].mean()\n",
    "        # Filter the subset to get only rows with reprojection error less than the mean\n",
    "        subset = subset[subset['ReprojectionError'] <= mean]\n",
    "        # Get the new mean and std\n",
    "        std = subset['ReprojectionError'].std()\n",
    "        mean = subset['ReprojectionError'].mean()\n",
    "        # Subset to get only rows within +/- one standard deviation of the mean\n",
    "        subset = subset[(subset['ReprojectionError'] >= (mean - std)) & \n",
    "                        (subset['ReprojectionError'] <= (mean + std))]\n",
    "        # Sort based on reprpjection error and ViewIndex, ascending\n",
    "        subset = subset.sort_values(['ReprojectionError', 'ViewIndex'], ascending=[True, True])\n",
    "        # Get the first N views\n",
    "        subset = subset.head(views)\n",
    "        filtered.append(subset)\n",
    "        \n",
    "    filtered_dfs = pd.concat(filtered)\n",
    "    filtered_dfs.dropna(inplace=True)\n",
    "    filtered_dfs.reset_index(drop=True, inplace=True)\n",
    "    filtered_dfs['Row'] = filtered_dfs['Row'].astype(int)\n",
    "    filtered_dfs['Column'] = filtered_dfs['Column'].astype(int)\n",
    "    \n",
    "    return filtered_dfs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f61500a-40bc-49ae-8e20-42af45bccda1",
   "metadata": {},
   "source": [
    "def create_class_mapping_json(dataframe, directory):\n",
    "    \"\"\"\n",
    "    Outputs a JSON file with the class mapping.\n",
    "    Class mapping is a dictionary with the class name as the\n",
    "    key, which contains a dictionary containing short_code_label,\n",
    "    long_code_label, id (uuid), and color [R, G, B].\n",
    "    \"\"\"\n",
    "    class_mapping = {}\n",
    "    for label in dataframe['Label'].unique():\n",
    "        if label not in class_mapping:\n",
    "            class_mapping[label] = {\n",
    "                \"short_label_code\": label,\n",
    "                \"long_label_code\": label,\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"color\": [np.random.randint(0, 255) for _ in range(3)] + [255]\n",
    "            }\n",
    "    with open(f\"{directory}/class_mapping.json\", \"w\") as f:\n",
    "        json.dump(class_mapping, f, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "525ed0ac33bc592",
   "metadata": {},
   "source": [
    "def plot_dataframes(dataframes, split, directory):\n",
    "    \"\"\"\n",
    "    Combines a list of dataframes and saves them as a CSV file.\n",
    "    Plots a distribution of the labels.\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(dataframes)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    combined_df['Label'].value_counts().plot(kind='bar', title=f\"{split} Label Distribution\")\n",
    "    plt.savefig(f\"{directory}/{split}_label_distribution.png\")\n",
    "    \n",
    "def save_dataframe(dataframe, site_name, directory):\n",
    "    \"\"\"\n",
    "    Save the dataframe to a CSV file.\n",
    "    \"\"\"\n",
    "    dataframe.to_csv(f\"{directory}/{site_name}.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6c044724cea227c",
   "metadata": {},
   "source": [
    "def create_cropped_image(rasterio_src, annotation_size, row, column):\n",
    "    half_size = annotation_size // 2\n",
    "    pixel_x, pixel_y = int(column), int(row)\n",
    "    window = Window(\n",
    "        col_off=max(0, pixel_x - half_size),\n",
    "        row_off=max(0, pixel_y - half_size),\n",
    "        width=min(rasterio_src.width - (pixel_x - half_size), annotation_size),\n",
    "        height=min(rasterio_src.height - (pixel_y - half_size), annotation_size)\n",
    "    )\n",
    "    \n",
    "    data = rasterio_src.read(window=window)\n",
    "    \n",
    "    if data.shape[0] == 3:\n",
    "        data = np.transpose(data, (1, 2, 0))\n",
    "    elif data.shape[0] == 1:\n",
    "        data = np.squeeze(data)\n",
    "    \n",
    "    if data.dtype != np.uint8:\n",
    "        data = ((data - data.min()) / (data.max() - data.min()) * 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(data)\n",
    "\n",
    "def process_annotation_chunk(src, annotations_chunk, size, dataset_dir, split):\n",
    "    results = []\n",
    "    for _, annotation in annotations_chunk.iterrows():\n",
    "        label = annotation['Label']\n",
    "        row, column = annotation['Row'], annotation['Column']\n",
    "        output_path = os.path.join(dataset_dir, split, label)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        output_filename = f\"{label}_{row}_{column}_{uuid.uuid4()}.JPEG\"\n",
    "        full_output_path = os.path.join(output_path, output_filename)\n",
    "        \n",
    "        try:\n",
    "            cropped_image = create_cropped_image(src, size, row, column)\n",
    "            if cropped_image:\n",
    "                results.append((cropped_image, full_output_path))\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue processing annotation at row {row}, column {column}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_image_batch(image_path, annotations, size, dataset_dir, split, chunk_size=1000):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        for i in range(0, len(annotations), chunk_size):\n",
    "            chunk = annotations.iloc[i:i+chunk_size]\n",
    "            chunk_results = process_annotation_chunk(src, chunk, size, dataset_dir, split)\n",
    "            save_images(chunk_results)\n",
    "            del chunk_results\n",
    "            gc.collect()\n",
    "\n",
    "def save_images(cropped_images):\n",
    "    for image, path in cropped_images:\n",
    "        try:\n",
    "            image.save(path, format='JPEG', optimize=True, quality=85)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue saving image {path}: {e}\")\n",
    "            png_path = path.split(\".\")[0] + \".PNG\"\n",
    "            image.save(png_path, optimize=True)\n",
    "        finally:\n",
    "            image.close()\n",
    "\n",
    "def create_orthomosaic_dataset(image_paths, dataframe, size, class_labels, dataset_dir, split, chunk_size=1000):\n",
    "    \n",
    "    # Create blank JPEGs for each class\n",
    "    for label in class_labels:\n",
    "        os.makedirs(f\"{dataset_dir}/{split}/{label}\", exist_ok=True)\n",
    "        img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "        Image.fromarray(img).save(f\"{dataset_dir}/{split}/{label}/NULL.JPEG\", format='JPEG', optimize=True, quality=85)\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count() // 2) as executor:\n",
    "        futures = []\n",
    "        for image_path in image_paths:\n",
    "            annotations = dataframe\n",
    "            future = executor.submit(process_image_batch, \n",
    "                                     image_path, \n",
    "                                     annotations, \n",
    "                                     size, \n",
    "                                     dataset_dir, \n",
    "                                     split,\n",
    "                                     chunk_size)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {split} images\"):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'An image generated an exception: {exc}')\n",
    "\n",
    "def create_image_dataset(dataframe, size, class_labels, dataset_dir, split, chunk_size=1000):\n",
    "    \n",
    "    # Create blank JPEGs for each class\n",
    "    for label in class_labels:\n",
    "        os.makedirs(f\"{dataset_dir}/{split}/{label}\", exist_ok=True)\n",
    "        img = np.zeros((size, size, 3), dtype=np.uint8)\n",
    "        Image.fromarray(img).save(f\"{dataset_dir}/{split}/{label}/NULL.JPEG\", format='JPEG', optimize=True, quality=85)\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count() // 2) as executor:\n",
    "        futures = []\n",
    "        for image_path in dataframe['Name'].unique():\n",
    "            annotations = dataframe[dataframe['Name'] == image_path]\n",
    "            future = executor.submit(process_image_batch, \n",
    "                                     image_path, \n",
    "                                     annotations, \n",
    "                                     size, \n",
    "                                     dataset_dir, \n",
    "                                     split,\n",
    "                                     chunk_size)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {split} images\"):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'An image generated an exception: {exc}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e02f1ce79f99278",
   "metadata": {},
   "source": [
    "#### Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1330c7e289f4573",
   "metadata": {},
   "source": [
    "root = r\"L:/Analysis/Jordan_AI\"\n",
    "study_dir = f\"{root}/preliminary_study_30\"\n",
    "\n",
    "singleview_dir = f\"{study_dir}/SingleView\"\n",
    "singleview_dataset_dir = f\"{singleview_dir}/Datasets\"\n",
    "singleview_classification_dir = f\"{singleview_dir}/Classification\"\n",
    "singleview_evaluation_dir = f\"{singleview_dir}/Evaluation\"\n",
    "singleview_filtered_dir = f\"{singleview_dir}/Filtered\"\n",
    "os.makedirs(singleview_dataset_dir, exist_ok=True)\n",
    "os.makedirs(singleview_filtered_dir, exist_ok=True)\n",
    "os.makedirs(singleview_classification_dir, exist_ok=True)\n",
    "os.makedirs(singleview_evaluation_dir, exist_ok=True)\n",
    "\n",
    "multiview_dir = f\"{study_dir}/MultiView\"\n",
    "multiview_dataset_dir = f\"{multiview_dir}/Datasets\"\n",
    "multiview_classification_dir = f\"{multiview_dir}/Classification\"\n",
    "multiview_evaluation_dir = f\"{multiview_dir}/Evaluation\"\n",
    "multiview_filtered_dir = f\"{multiview_dir}/Filtered\"\n",
    "os.makedirs(multiview_dataset_dir, exist_ok=True)\n",
    "os.makedirs(multiview_filtered_dir, exist_ok=True)\n",
    "os.makedirs(multiview_classification_dir, exist_ok=True)\n",
    "os.makedirs(multiview_evaluation_dir, exist_ok=True)\n",
    "\n",
    "orthomosaic_dir = f\"{study_dir}/OrthoView\"\n",
    "orthomosaic_dataset_dir = f\"{orthomosaic_dir}/Datasets\"\n",
    "orthomosaic_classification_dir = f\"{orthomosaic_dir}/Classification\"\n",
    "orthomosaic_evaluation_dir = f\"{orthomosaic_dir}/Evaluation\"\n",
    "orthomosaic_filtered_dir = f\"{orthomosaic_dir}/Filtered\"\n",
    "os.makedirs(orthomosaic_dataset_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_filtered_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_classification_dir, exist_ok=True)\n",
    "os.makedirs(orthomosaic_evaluation_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f954b227-6008-4918-94c9-79f4cb8fae4a",
   "metadata": {},
   "source": [
    "mapping_file = f\"{root}/palmyra_vpi_to_ml.csv\"\n",
    "assert os.path.exists(mapping_file), \"ERROR: Mapping file not found.\"\n",
    "\n",
    "mapping_dict = pd.read_csv(mapping_file)\n",
    "mapping_dict = mapping_dict.set_index('Label')['new_label'].to_dict()\n",
    "print(f\"Num Keys: {len(mapping_dict)}\")\n",
    "\n",
    "# Find the amount that are being discarded\n",
    "num_drop = len([k for k, v in mapping_dict.items() if v == 'drop'])\n",
    "print(f\"Num Valid Keys: {len(mapping_dict) - num_drop}\")\n",
    "print(f\"Num Dropped Keys: {num_drop}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e8ab5e0f06c9a93",
   "metadata": {},
   "source": [
    "#### Site Splits\n",
    "\n",
    "Split the sites (n=???) into training / testing sets (80/20). "
   ]
  },
  {
   "cell_type": "code",
   "id": "264b525a0133d753",
   "metadata": {},
   "source": [
    "# Assert that All_Sites.txt exists\n",
    "all_sites_txt = rf\"{root}/All_Sites_Preliminary_30.txt\"\n",
    "assert os.path.exists(all_sites_txt), f\"ERROR: Missing {all_sites_txt}\"\n",
    "\n",
    "# Open the txt file, store as a list\n",
    "with open(all_sites_txt) as f:\n",
    "    all_sites_list = f.read().splitlines()\n",
    "    \n",
    "assert all_sites_list, f\"ERROR: No data found in {all_sites_txt}\"\n",
    "num_sites = len(all_sites_list)\n",
    "print(\"Num Sites: \", num_sites)\n",
    "print(\"Sites: \", all_sites_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef75b96129312c0d",
   "metadata": {},
   "source": [
    "# Define the ratios\n",
    "training_ratio, validation_ratio, testing_ratio = 0.70, 0.15, 0.15\n",
    "\n",
    "# Ensure the ratios sum to 1\n",
    "assert training_ratio + validation_ratio + testing_ratio == 1.0, \"Ratios must sum to 1.\"\n",
    "\n",
    "# Split into training and temp (validation + testing)\n",
    "training_sites_list, temp_sites_list = train_test_split(all_sites_list, test_size=(1 - training_ratio), random_state=42)\n",
    "\n",
    "# Split the temp set into validation and testing with equal sizes\n",
    "validation_sites_list, testing_sites_list = train_test_split(temp_sites_list, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the number of sites in each set to verify the ratios\n",
    "print(f\"Num Training Sites: {len(training_sites_list)}\")\n",
    "print(f\"Num Validation Sites: {len(validation_sites_list)}\")\n",
    "print(f\"Num Testing Sites: {len(testing_sites_list)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad29c537da51d5a7",
   "metadata": {},
   "source": [
    "#### Training and Testing (Images)"
   ]
  },
  {
   "cell_type": "code",
   "id": "852140b9-5f6b-4055-be81-e61950a06cba",
   "metadata": {},
   "source": [
    "images_root = r\"L:/Raw Images\"\n",
    "\n",
    "# Get all the point file paths\n",
    "image_points_root = f\"{root}/image_points/\"\n",
    "image_point_files = glob.glob(f\"{image_points_root}/*.csv\")\n",
    "\n",
    "# Dictionaries\n",
    "training_image_data_dict = {site: {} for site in training_sites_list}\n",
    "validation_image_data_dict = {site: {} for site in validation_sites_list}\n",
    "testing_image_data_dict = {site: {} for site in testing_sites_list}\n",
    "\n",
    "# Use the point file paths to get the image folder paths\n",
    "for image_point_file in image_point_files:\n",
    "    for site in all_sites_list:\n",
    "        if site in image_point_file:\n",
    "            # Add the folder to the set\n",
    "            if site in training_sites_list:\n",
    "                training_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "            elif site in validation_sites_list:\n",
    "                validation_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "            elif site in testing_sites_list:\n",
    "                testing_image_data_dict[site]['image_point_file'] = image_point_file\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(f\"Num Training Data Dict: {len(training_image_data_dict)}\")\n",
    "print(f\"Num Validation Data Dict: {len(validation_image_data_dict)}\")\n",
    "print(f\"Num Testing Data Dict: {len(testing_image_data_dict)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ee5cb6f-97de-499c-aba3-56e8f23dabf0",
   "metadata": {},
   "source": [
    "testing_image_data_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5a5c49d955b3375",
   "metadata": {},
   "source": [
    "#### Training and Testing (Orthomosaics)"
   ]
  },
  {
   "cell_type": "code",
   "id": "16abb825fc063fab",
   "metadata": {},
   "source": [
    "orthomosaic_files = glob.glob(rf\"{root}/orthomosaics/*.tif\")\n",
    "orthomosaic_point_files = glob.glob(rf\"{root}/orthomosaic_points/*.csv\")\n",
    "\n",
    "training_orthomosaic_data_dict = {site: {} for site in training_sites_list}\n",
    "validation_orthomosaic_data_dict = {site: {} for site in validation_sites_list}\n",
    "testing_orthomosaic_data_dict = {site: {} for site in testing_sites_list}\n",
    "\n",
    "for orthomosaic_point_file in orthomosaic_point_files:\n",
    "    orthomosaic_file = orthomosaic_point_file.replace(\"ortho_dots_\", \"vpi_ortho_\")\n",
    "    orthomosaic_file = orthomosaic_file.replace(\".csv\" , \"_r1.tif\")\n",
    "    orthomosaic_file = orthomosaic_file.replace(\"orthomosaic_points\", \"orthomosaics\")\n",
    "\n",
    "    if not os.path.exists(orthomosaic_file):\n",
    "        print(f\"Error: File does not exists {orthomosaic_file}\") \n",
    "        \n",
    "    for site in all_sites_list:\n",
    "        if site in orthomosaic_file:\n",
    "            # Add the folder to the set\n",
    "            if site in training_sites_list:\n",
    "                training_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "                training_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "            elif site in validation_sites_list:\n",
    "                validation_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "                validation_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "            elif site in testing_sites_list:\n",
    "                testing_orthomosaic_data_dict[site]['orthomosaic_file'] = orthomosaic_file\n",
    "                testing_orthomosaic_data_dict[site]['orthomosaic_point_file'] = orthomosaic_point_file\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "print(f\"Num Training Orthomosaic Data Dict: {len(training_orthomosaic_data_dict)}\")\n",
    "print(f\"Num Validation Orthomosaic Data Dict: {len(validation_orthomosaic_data_dict)}\")\n",
    "print(f\"Num Testing Orthomosaic Data Dict: {len(testing_orthomosaic_data_dict)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05b1b212-44c9-4657-9863-247ff9b575eb",
   "metadata": {},
   "source": [
    "testing_orthomosaic_data_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "135884b5e710059b",
   "metadata": {},
   "source": [
    "#### Creating Singleview Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c12a826d0f4dd1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "annotation_size = 336\n",
    "index_view = 1\n",
    "view_count = 1\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    training_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    validation_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "\n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=1)\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    testing_image_data_dict[site]['dataframe'] = dataframe\n",
    "    save_dataframe(dataframe, site, singleview_filtered_dir)\n",
    "    \n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Image Dataframe: {sum([len(data['dataframe']) for data in training_image_data_dict.values()])}\")\n",
    "print(f\"Validation Image Dataframe: {sum([len(data['dataframe']) for data in validation_image_data_dict.values()])}\")\n",
    "print(f\"Testing Image Dataframe: {sum([len(data['dataframe']) for data in testing_image_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "singleview_dataframes_combined = []\n",
    "\n",
    "for site, data in training_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in testing_image_data_dict.items():\n",
    "    singleview_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "singleview_dataframes_combined = pd.concat(singleview_dataframes_combined)\n",
    "create_class_mapping_json(singleview_dataframes_combined, singleview_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_image_data_dict.values()], \"train\", singleview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_image_data_dict.values()], \"val\", singleview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_image_data_dict.values()], \"test\", singleview_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, singleview_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, singleview_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, singleview_dataset_dir, \"test\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5779fd8aa53fac6f",
   "metadata": {},
   "source": [
    "#### Creating Multiview Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc4b93a5076d1dac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "annotation_size = 336\n",
    "index_view = 10\n",
    "view_count = 1\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=10)\n",
    "    training_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=10)\n",
    "    validation_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_image_csv_file(data['image_point_file'], images_root, mapping_dict, views=10)\n",
    "    testing_image_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, multiview_filtered_dir)\n",
    "    \n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Image Dataframe: {sum([len(data['dataframe']) for data in training_image_data_dict.values()])}\")\n",
    "print(f\"Validation Image Dataframe: {sum([len(data['dataframe']) for data in validation_image_data_dict.values()])}\")\n",
    "print(f\"Testing Image Dataframe: {sum([len(data['dataframe']) for data in testing_image_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "multiview_dataframes_combined = []\n",
    "\n",
    "for site, data in training_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in testing_image_data_dict.items():\n",
    "    multiview_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "multiview_dataframes_combined = pd.concat(multiview_dataframes_combined)\n",
    "create_class_mapping_json(multiview_dataframes_combined, multiview_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_image_data_dict.values()], \"train\", multiview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_image_data_dict.values()], \"val\", multiview_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_image_data_dict.values()], \"test\", multiview_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_image_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, multiview_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_image_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, multiview_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_image_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_image_dataset(dataframe, annotation_size, class_labels, multiview_dataset_dir, \"test\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3390e8c4214c95ae",
   "metadata": {},
   "source": [
    "#### Creating Orthomosaic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e18e34cced1cbda",
   "metadata": {},
   "source": [
    "annotation_size = 512\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for site, data in tqdm(training_orthomosaic_data_dict.items(), desc=\"Filtering Training Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    training_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "    \n",
    "for site, data in tqdm(validation_orthomosaic_data_dict.items(), desc=\"Filtering Validation Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    validation_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "        \n",
    "for site, data in tqdm(testing_orthomosaic_data_dict.items(), desc=\"Filtering Testing Point Data\"):\n",
    "    dataframe = filter_orthomosaic_csv_file(data['orthomosaic_point_file'], mapping_dict)\n",
    "    testing_orthomosaic_data_dict[site]['dataframe'] = dataframe\n",
    "    class_labels.extend(dataframe['Label'].unique())\n",
    "    save_dataframe(dataframe, site, orthomosaic_filtered_dir)\n",
    "\n",
    "class_labels = list(set(class_labels))\n",
    "print(f\"Num Class Labels: {len(class_labels)}\")\n",
    "\n",
    "print(f\"Training Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in training_orthomosaic_data_dict.values()])}\")\n",
    "print(f\"Validation Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in validation_orthomosaic_data_dict.values()])}\")\n",
    "print(f\"Testing Orthomosaic Dataframe: {sum([len(data['dataframe']) for data in testing_orthomosaic_data_dict.values()])}\")\n",
    "\n",
    "# Create the class mapping JSON\n",
    "orthomosaic_dataframes_combined = []\n",
    "\n",
    "for site, data in training_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "for site, data in validation_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "\n",
    "for site, data in testing_orthomosaic_data_dict.items():\n",
    "    orthomosaic_dataframes_combined.append(data['dataframe'])\n",
    "    \n",
    "orthomosaic_dataframes_combined = pd.concat(orthomosaic_dataframes_combined)\n",
    "create_class_mapping_json(orthomosaic_dataframes_combined, orthomosaic_dataset_dir)\n",
    "\n",
    "# Save the dataframes\n",
    "plot_dataframes([data['dataframe'] for data in training_orthomosaic_data_dict.values()], \"train\", orthomosaic_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in validation_orthomosaic_data_dict.values()], \"val\", orthomosaic_dataset_dir)\n",
    "plot_dataframes([data['dataframe'] for data in testing_orthomosaic_data_dict.values()], \"test\", orthomosaic_dataset_dir)\n",
    "\n",
    "# Create the datasets\n",
    "for site, data in tqdm(training_orthomosaic_data_dict.items(), desc=\"Creating Training Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_orthomosaic_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"train\")\n",
    "    \n",
    "for site, data in tqdm(validation_orthomosaic_data_dict.items(), desc=\"Creating Validation Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_orthomosaic_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"val\")\n",
    "    \n",
    "for site, data in tqdm(testing_orthomosaic_data_dict.items(), desc=\"Creating Testing Dataset\"):\n",
    "    dataframe = data['dataframe']\n",
    "    create_orthomosaic_dataset([data['orthomosaic_file']], dataframe, annotation_size, class_labels, orthomosaic_dataset_dir, \"test\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5119390104a7c816",
   "metadata": {},
   "source": [
    "print(\"Done.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "527b340d-9bf7-41c4-be91-ed770e917fd7",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coralnet10",
   "language": "python",
   "name": "coralnet10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
