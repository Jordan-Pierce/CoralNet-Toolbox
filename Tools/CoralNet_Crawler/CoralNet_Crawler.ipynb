{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c477702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e34b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_soup(url):\n",
    "    \"\"\"\n",
    "    Takes a URL and returns a BeautifulSoup object for the HTML at the URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the webpage to be converted to a BeautifulSoup object.\n",
    "        \n",
    "    Returns:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object for the HTML at the URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML, store in soup\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    \"\"\"\n",
    "    Crawl a coralnet image page and get the associated image path url and the url of the next image page.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the coralnet image page to crawl.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the image page URL, image path URL, and the URL of the next image page.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_name = None\n",
    "    image_page_url = url\n",
    "    image_path_url = None\n",
    "    next_image_page_url = None\n",
    "    \n",
    "    # From the provided image page url, get the associated image path url\n",
    "    soup = url_to_soup(url)\n",
    "    \n",
    "    # Find the div element with id=\"original_image_container\" and style=\"display:none;\"\n",
    "    orginal_image_container = soup.find('div', id='original_image_container', style='display:none;')\n",
    "    \n",
    "    # Find the img element within the div and get the value of the src attribute\n",
    "    image_path_url = orginal_image_container.find('img').get('src')\n",
    "    \n",
    "    # Now, get the next page's url\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        # check if the text of the <a> tag contains \"Next\"\n",
    "        if \"Next\" in a_tag.text:\n",
    "            # Get the value of the href attribute and prepend the CORALNET_URL\n",
    "            next_image_page_url = CORALNET_URL + a_tag.get('href')\n",
    "            \n",
    "        # Else, it return None and we know we're at the end\n",
    "        \n",
    "    # Finally, get the name of the image, because when downloaded it might not match\n",
    "    image_name = soup.find('title').text.split(\" |\")[0]\n",
    "    \n",
    "    \n",
    "    return image_name, image_page_url, image_path_url, next_image_page_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb005222",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"\n",
    "    This is the main function of the script. It initializes the necessary constants and lists,\n",
    "    converts the first image page URL to soup, and then crawls through all the image pages \n",
    "    to get the image page URLs and image path URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constant for the coralnet url\n",
    "    CORALNET_URL = \"https://coralnet.ucsd.edu\"\n",
    "\n",
    "    # A list containing the urls to all the image pages and a list containing \n",
    "    # the the urls to all the images hosted on amazon\n",
    "    image_names = []\n",
    "    image_page_urls = []\n",
    "    image_path_urls = []\n",
    "\n",
    "    # The source id of the source you want to download all the images from\n",
    "    source_id = 2687\n",
    "    source_url = CORALNET_URL + \"/source/\" + str(source_id) + \"/browse/images/\"\n",
    "\n",
    "    # Convert the webpage to soup\n",
    "    soup = url_to_soup(source_url)\n",
    "\n",
    "    # Grab the first image page url\n",
    "    images_divs = soup.find('span', class_='thumb_wrapper')\n",
    "    next_image_page_url = CORALNET_URL + images_divs.find_all('a')[0].get(\"href\")\n",
    "    \n",
    "    # Crawl across all image page urls, grabbing the image path url as well as the next page url\n",
    "    # continue doing this until the end of the source project image pages, where there is no next.\n",
    "    while next_image_page_url != None:\n",
    "    \n",
    "        image_name, image_page_url, image_path_url, next_image_page_url = crawl(next_image_page_url)\n",
    "        \n",
    "        image_names.append(image_name)\n",
    "        image_page_urls.append(image_page_url)\n",
    "        image_path_urls.append(image_path_url)\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(image_names, image_page_urls, image_path_urls)),\n",
    "                     columns=['image_name', 'image_page', 'image_url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80b4c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_page</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_6182.JPG</td>\n",
       "      <td>https://coralnet.ucsd.edu/image/2072717/view/</td>\n",
       "      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_6187.JPG</td>\n",
       "      <td>https://coralnet.ucsd.edu/image/2072719/view/</td>\n",
       "      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_6190.JPG</td>\n",
       "      <td>https://coralnet.ucsd.edu/image/2072718/view/</td>\n",
       "      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name                                     image_page  \\\n",
       "0  IMG_6182.JPG  https://coralnet.ucsd.edu/image/2072717/view/   \n",
       "1  IMG_6187.JPG  https://coralnet.ucsd.edu/image/2072719/view/   \n",
       "2  IMG_6190.JPG  https://coralnet.ucsd.edu/image/2072718/view/   \n",
       "\n",
       "                                           image_url  \n",
       "0  https://coralnet-production.s3.amazonaws.com:4...  \n",
       "1  https://coralnet-production.s3.amazonaws.com:4...  \n",
       "2  https://coralnet-production.s3.amazonaws.com:4...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79e0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coralnet] *",
   "language": "python",
   "name": "conda-env-coralnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
