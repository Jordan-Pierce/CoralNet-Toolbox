{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e941c5b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be1f26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Configurations\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Constant for the CoralNet url\n",
        "CORALNET_URL = \"https://coralnet.ucsd.edu\"\n",
        "\n",
        "# URL of the login page\n",
        "LOGIN_URL = \"https://coralnet.ucsd.edu/accounts/login/\"\n",
        "\n",
        "# The source id of the source you want to download all the images from\n",
        "SOURCE_ID = 2112\n",
        "\n",
        "# Constant URLs for getting images, labelset, and annotations\n",
        "SOURCE_URL = CORALNET_URL + \"/source/\" + str(SOURCE_ID)\n",
        "IMAGE_URL = SOURCE_URL + \"/browse/images/\"\n",
        "LABELSET_URL = SOURCE_URL + \"/export/labelset/\"\n",
        "\n",
        "# The directory to store the output\n",
        "SOURCE_DIR = \"./\" + str(SOURCE_ID) + \"/\"\n",
        "IMAGE_DIR = SOURCE_DIR + \"images/\"\n",
        "\n",
        "# Creating the directories\n",
        "os.makedirs(SOURCE_DIR, exist_ok=True)\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "# Set the username and password for your CoralNet account locally, that way\n",
        "# credentials never need to be entered in the script (wherever it is).\n",
        "\n",
        "# Be sure to restart the terminal when first setting environmental variables\n",
        "# for them to be saved!\n",
        "USERNAME = os.getenv('CORALNET_USERNAME')\n",
        "PASSWORD = os.getenv('CORALNET_PASSWORD')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b33a79c",
      "metadata": {
        "gather": {
          "logged": 1677881365253
        }
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Functions\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def url_to_soup(url):\n",
        "    \"\"\"\n",
        "    Takes a URL and returns a BeautifulSoup object for the HTML at the URL.\n",
        "\n",
        "    Args: url (str): The URL of the webpage to be converted to a\n",
        "    BeautifulSoup object.\n",
        "\n",
        "    Returns:\n",
        "        soup (BeautifulSoup): The BeautifulSoup object for the HTML at the URL.\n",
        "    \"\"\"\n",
        "\n",
        "    # Send an HTTP GET request to the URL and store the response\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML, store in soup\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    return soup\n",
        "\n",
        "\n",
        "def crawl(page_url):\n",
        "    \"\"\"\n",
        "    Crawl a coralnet image page and get the associated image path url and\n",
        "    the url of the next image page.\n",
        "\n",
        "    Args:\n",
        "        page_url (str): The URL of the coralnet image page to crawl.\n",
        "\n",
        "    Returns: tuple: A tuple containing the image page URL, image path URL,\n",
        "    and the URL of the next image page.\n",
        "    \"\"\"\n",
        "\n",
        "    image_name = None\n",
        "    image_page_url = page_url\n",
        "    image_path_url = None\n",
        "    next_image_page_url = None\n",
        "\n",
        "    # From the provided image page url, get the associated image path url\n",
        "    soup = url_to_soup(page_url)\n",
        "\n",
        "    # Find the div element with id=\"original_image_container\" and\n",
        "    # style=\"display:none;\"\n",
        "    orginal_image_container = soup.find('div',\n",
        "                                        id='original_image_container',\n",
        "                                        style='display:none;')\n",
        "\n",
        "    # Find the img element within the div and get the value of the src\n",
        "    # attribute\n",
        "    image_path_url = orginal_image_container.find('img').get('src')\n",
        "\n",
        "    # Now, get the next page's url\n",
        "    for a_tag in soup.find_all('a'):\n",
        "        # check if the text of the <a> tag contains \"Next\"\n",
        "        if \"Next\" in a_tag.text:\n",
        "            # Get the value of the href attribute and prepend the CORALNET_URL\n",
        "            next_image_page_url = CORALNET_URL + a_tag.get('href')\n",
        "\n",
        "        # Else, it returns None, and we know we're at the end of the images\n",
        "\n",
        "    # Finally, get the name of the image, because when downloaded it might\n",
        "    # not match\n",
        "    image_name = soup.find('title').text.split(\" |\")[0]\n",
        "\n",
        "    return image_name, image_page_url, image_path_url, next_image_page_url\n",
        "\n",
        "\n",
        "def download_labelset(labelset_url):\n",
        "    \"\"\"\n",
        "    Downloads a .csv file holding the label set from the given URL.\n",
        "    Args:\n",
        "        labelset_url (str): The URL of the website with the download button.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading labelset...\")\n",
        "\n",
        "    # Make an HTTP GET request to download the .csv file\n",
        "    response = requests.get(labelset_url,\n",
        "                            params={\"submit\": \"Export label entries to CSV\"})\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 200:\n",
        "        # Save the .csv file to a local directory\n",
        "        with open(SOURCE_DIR + \"label_set.csv\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "    if os.path.exists(SOURCE_DIR + \"label_set.csv\"):\n",
        "        print(\"Label set saved to: \", SOURCE_DIR + \"label_set.csv\")\n",
        "    else:\n",
        "        print(\"Could not download label set.\")\n",
        "\n",
        "\n",
        "def download_annotations(image_url):\n",
        "    \"\"\"\n",
        "    This function downloads the annotations from a CoralNet source using the\n",
        "    provided image URL. It logs into CoralNet using the provided username\n",
        "    and password, and exports the annotations for the images in the source\n",
        "    as a CSV file, which is saved in the local source directory.\n",
        "\n",
        "    :param image_url: A string containing the URL of an image in the source\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading annotations...\")\n",
        "\n",
        "    # Send a GET request to the login page to retrieve the login form\n",
        "    response = requests.get(LOGIN_URL)\n",
        "\n",
        "    # Pass along the cookies\n",
        "    cookies = response.cookies\n",
        "\n",
        "    # Parse the HTML of the response using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract the CSRF token from the HTML of the login page\n",
        "    csrf_token = soup.find(\"input\", attrs={\"name\": \"csrfmiddlewaretoken\"})\n",
        "\n",
        "    # Create a dictionary with the login form fields and their values\n",
        "    # (replace \"username\" and \"password\" with your actual username and\n",
        "    # password)\n",
        "    data = {\n",
        "        \"username\": USERNAME,\n",
        "        \"password\": PASSWORD,\n",
        "        \"csrfmiddlewaretoken\": csrf_token[\"value\"],\n",
        "    }\n",
        "\n",
        "    # Include the \"Referer\" header in the request\n",
        "    headers = {\n",
        "        \"Referer\": LOGIN_URL,\n",
        "    }\n",
        "\n",
        "    # Use requests.Session to create a session that will maintain your login\n",
        "    # state\n",
        "    with requests.Session() as session:\n",
        "\n",
        "        # Use session.post() to submit the login form, including the\n",
        "        # \"Referer\" header\n",
        "        response = session.post(LOGIN_URL,\n",
        "                                data=data,\n",
        "                                headers=headers,\n",
        "                                cookies=cookies)\n",
        "\n",
        "        # Use session.get() to make a GET request to the source URL\n",
        "        response = session.get(image_url)\n",
        "\n",
        "        # Pass along the cookies\n",
        "        cookies = response.cookies\n",
        "\n",
        "        # Parse the HTML response using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find the form in the HTML\n",
        "        form = soup.find(\"form\", {\"id\": \"export-annotations-form\"})\n",
        "\n",
        "        # Extract the form fields (input elements)\n",
        "        inputs = form.find_all(\"input\")\n",
        "\n",
        "        # Create a dictionary with the form fields and their values\n",
        "        data = {'optional_columns': []}\n",
        "        for i, input in enumerate(inputs):\n",
        "            if i == 0:\n",
        "                data[input[\"name\"]] = input[\"value\"]\n",
        "            else:\n",
        "                data['optional_columns'].append(input['value'])\n",
        "\n",
        "        # Use session.post() to submit the form\n",
        "        response = session.post(CORALNET_URL + form[\"action\"],\n",
        "                                data=data,\n",
        "                                headers=headers,\n",
        "                                cookies=cookies)\n",
        "\n",
        "        # Check the response status code\n",
        "        if response.status_code == 200:\n",
        "            # Convert the text in response to a dataframe\n",
        "            df = pd.read_csv(io.StringIO(response.text), sep=\",\")\n",
        "            # Save the dataframe locally\n",
        "            df.to_csv(SOURCE_DIR + \"annotations.csv\")\n",
        "\n",
        "        else:\n",
        "            print(\"Could not connect with CoralNet; please ensure that you \"\n",
        "                  \"entered your username, password, and source ID correctly.\")\n",
        "\n",
        "    if os.path.exists(SOURCE_DIR + \"label_set.csv\"):\n",
        "        print(\"Annotations saved to: \", SOURCE_DIR + \"annotations.csv\")\n",
        "    else:\n",
        "        print(\"Could not download annotations.\")\n",
        "\n",
        "\n",
        "def download_image(row):\n",
        "    \"\"\"\n",
        "    Downloads a single image from the given URL, using the provided file name\n",
        "    to save the image to a local directory.\n",
        "\n",
        "    Args:\n",
        "        row (list): A list containing the file name and URL of the image to\n",
        "            download. The list should have at least two elements: the file\n",
        "            name and the URL.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # the output path of the image being downloaded\n",
        "    image_path = IMAGE_DIR + row[0]\n",
        "\n",
        "    # Make an HTTP GET request to download the image\n",
        "    response = requests.get(row[2])\n",
        "\n",
        "    # Check the response status code\n",
        "    if response.status_code == 200:\n",
        "        # Save the image to a file in the output directory\n",
        "        with open(image_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            print(\"File downloaded: \", image_path)\n",
        "        else:\n",
        "            print(\"File could not be downloaded: \", image_path)\n",
        "\n",
        "\n",
        "def download_images(image_url):\n",
        "    \"\"\"\n",
        "    Downloads images from a list of URLs, using file names from a pandas\n",
        "    dataframe to save the images to a local directory.\n",
        "\n",
        "    Args: df (pandas.DataFrame): A dataframe containing the file names and\n",
        "    URLs of the images to download. The dataframe should have at least two\n",
        "    columns: 'file_name' and 'image_url'.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading images...\")\n",
        "\n",
        "    # A list containing the urls to all the image pages and a list containing\n",
        "    # the urls to all the images hosted on amazon\n",
        "    image_names = []\n",
        "    image_page_urls = []\n",
        "    image_path_urls = []\n",
        "\n",
        "    # Convert the webpage to soup\n",
        "    soup = url_to_soup(image_url)\n",
        "\n",
        "    # Grab the first image page url\n",
        "    images_divs = soup.find('span', class_='thumb_wrapper')\n",
        "    image_href = images_divs.find_all('a')[0].get(\"href\")\n",
        "    next_page_url = CORALNET_URL + image_href\n",
        "\n",
        "    # Crawl across all image page urls, grabbing the image path url as well\n",
        "    # as the next page url continue doing this until the end of the source\n",
        "    # project image pages, where there is no next.\n",
        "    while next_page_url is not None:\n",
        "\n",
        "        name, page_url, path_url, next_page_url = crawl(next_page_url)\n",
        "        \n",
        "        image_names.append(name)\n",
        "        image_page_urls.append(page_url)\n",
        "        image_path_urls.append(path_url)\n",
        "\n",
        "        print(name, page_url, path_url)\n",
        "\n",
        "    # Storing the results in dataframe\n",
        "    df = pd.DataFrame(list(zip(image_names, image_page_urls, image_path_urls)),\n",
        "                      columns=['image_name', 'image_page', 'image_url'])\n",
        "    # Saving locally\n",
        "    df.to_csv(SOURCE_DIR + str(SOURCE_ID) + \"_images.csv\")\n",
        "\n",
        "    # Use the multiprocessing library's Pool.map() method to download images\n",
        "    # in parallel\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        # Apply the download_image function to each row in the dataframe\n",
        "        pool.map(download_image, df.values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19caa26",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"This is the main function of the script. It calls the functions\n",
        "    download_labelset, download_annotations, and download_images to download\n",
        "    the label set, annotations, and images, respectively. \"\"\"\n",
        "\n",
        "    # Download the label set as a csv\n",
        "    download_labelset(LABELSET_URL)\n",
        "\n",
        "    # Check to see if user credentials have been set,\n",
        "    # if not, annotations cannot be downloaded; skip.\n",
        "    if not None in [USERNAME, PASSWORD, None]:\n",
        "        # Download the annotations as a csv\n",
        "        download_annotations(IMAGE_URL)\n",
        "\n",
        "    # Download all the images\n",
        "    download_images(IMAGE_URL)\n",
        "\n",
        "    print(\"Done.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "coralnet"
    },
    "kernelspec": {
      "display_name": "CoralNet",
      "language": "python",
      "name": "coralnet"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
