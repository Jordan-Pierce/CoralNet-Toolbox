{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configurations\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Constant for the coralnet url\n",
    "CORALNET_URL = \"https://coralnet.ucsd.edu\"\n",
    "\n",
    "# The source id of the source you want to download all the images from\n",
    "SOURCE_ID = 2687\n",
    "\n",
    "# The directory to store the output\n",
    "SOURCE_DIR = \"./\" + str(SOURCE_ID) + \"/\"\n",
    "IMAGE_DIR = SOURCE_DIR + \"images/\"\n",
    "\n",
    "os.makedirs(SOURCE_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def url_to_soup(url):\n",
    "    \"\"\"\n",
    "    Takes a URL and returns a BeautifulSoup object for the HTML at the URL.\n",
    "\n",
    "    Args: url (str): The URL of the webpage to be converted to a\n",
    "    BeautifulSoup object.\n",
    "\n",
    "    Returns:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object for the HTML at the URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML, store in soup\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    \"\"\"\n",
    "    Crawl a coralnet image page and get the associated image path url and\n",
    "    the url of the next image page.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the coralnet image page to crawl.\n",
    "\n",
    "    Returns: tuple: A tuple containing the image page URL, image path URL,\n",
    "    and the URL of the next image page.\n",
    "    \"\"\"\n",
    "\n",
    "    image_name = None\n",
    "    image_page_url = url\n",
    "    image_path_url = None\n",
    "    next_image_page_url = None\n",
    "\n",
    "    # From the provided image page url, get the associated image path url\n",
    "    soup = url_to_soup(url)\n",
    "\n",
    "    # Find the div element with id=\"original_image_container\" and\n",
    "    # style=\"display:none;\"\n",
    "    orginal_image_container = soup.find('div', id='original_image_container',\n",
    "                                        style='display:none;')\n",
    "\n",
    "    # Find the img element within the div and get the value of the src\n",
    "    # attribute\n",
    "    image_path_url = orginal_image_container.find('img').get('src')\n",
    "\n",
    "    # Now, get the next page's url\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        # check if the text of the <a> tag contains \"Next\"\n",
    "        if \"Next\" in a_tag.text:\n",
    "            # Get the value of the href attribute and prepend the CORALNET_URL\n",
    "            next_image_page_url = CORALNET_URL + a_tag.get('href')\n",
    "\n",
    "        # Else, it return None and we know we're at the end\n",
    "\n",
    "    # Finally, get the name of the image, because when downloaded it might\n",
    "    # not match\n",
    "    image_name = soup.find('title').text.split(\" |\")[0]\n",
    "\n",
    "    return image_name, image_page_url, image_path_url, next_image_page_url\n",
    "\n",
    "\n",
    "def download_labelset(url):\n",
    "    \"\"\"\n",
    "    Downloads a .csv file holding the labelset from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website with the download button.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Make an HTTP GET request to download the .csv file\n",
    "    response = requests.get(url, params={\"submit\": \"Export label entries to \"\n",
    "                                                   \"CSV\"})\n",
    "\n",
    "    # Check the response status code\n",
    "    if response.status_code == 200:\n",
    "        # Save the .csv file to a local directory\n",
    "        with open(SOURCE_DIR + \"label_set.csv\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "\n",
    "def download_image(row):\n",
    "    \"\"\"\n",
    "    Downloads a single image from the given URL, using the provided file name\n",
    "    to save the image to a local directory.\n",
    "\n",
    "    Args:\n",
    "        row (list): A list containing the file name and URL of the image to\n",
    "            download. The list should have at least two elements: the file\n",
    "            name and the URL.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # the output path of the image being downloaded\n",
    "    image_path = IMAGE_DIR + row[0]\n",
    "\n",
    "    # Make an HTTP GET request to download the image\n",
    "    response = requests.get(row[2])\n",
    "\n",
    "    # Check the response status code\n",
    "    if response.status_code == 200:\n",
    "        # Save the image to a file in the output directory\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            print(\"File downloaded: \", image_path)\n",
    "        else:\n",
    "            print(\"File could not be downloaded: \", image_path)\n",
    "\n",
    "\n",
    "def download_images(df):\n",
    "    \"\"\"\n",
    "    Downloads images from a list of URLs, using file names from a pandas\n",
    "    dataframe to save the images to a local directory.\n",
    "\n",
    "    Args: df (pandas.DataFrame): A dataframe containing the file names and\n",
    "    URLs of the images to download. The dataframe should have at least two\n",
    "    columns: 'file_name' and 'image_url'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Use the multiprocessing library's Pool.map() method to download images\n",
    "    # in parallel\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        # Apply the download_image function to each row in the dataframe\n",
    "        pool.map(download_image, df.values.tolist())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"This is the main function of the script. It initializes the necessary \n",
    "    constants and lists, converts the first image page URL to soup, and then \n",
    "    crawls through all the image pages to get the image page URLs and image \n",
    "    path URLs. \"\"\"\n",
    "\n",
    "    # A list containing the urls to all the image pages and a list containing\n",
    "    # the the urls to all the images hosted on amazon\n",
    "    image_names = []\n",
    "    image_page_urls = []\n",
    "    image_path_urls = []\n",
    "\n",
    "    # The source id of the source you want to download all the images from\n",
    "    source_url = CORALNET_URL + \"/source/\" + str(SOURCE_ID)\n",
    "    image_url = source_url + \"/browse/images/\"\n",
    "    labelset_url = source_url + \"/export/labelset/\"\n",
    "    annotation_url = source_url + \"/export/annotations/\"\n",
    "\n",
    "    # First download the labelset\n",
    "    download_labelset(labelset_url)\n",
    "\n",
    "    # Convert the webpage to soup\n",
    "    soup = url_to_soup(image_url)\n",
    "\n",
    "    # Grab the first image page url\n",
    "    images_divs = soup.find('span', class_='thumb_wrapper')\n",
    "    next_image_page_url = CORALNET_URL + images_divs.find_all('a')[0].get(\n",
    "        \"href\")\n",
    "\n",
    "    # Crawl across all image page urls, grabbing the image path url as well\n",
    "    # as the next page url continue doing this until the end of the source\n",
    "    # project image pages, where there is no next.\n",
    "    while next_image_page_url is not None:\n",
    "        image_name, image_page_url, image_path_url, next_image_page_url = crawl(\n",
    "            next_image_page_url)\n",
    "\n",
    "        image_names.append(image_name)\n",
    "        image_page_urls.append(image_page_url)\n",
    "        image_path_urls.append(image_path_url)\n",
    "\n",
    "        print(image_name, image_page_url, image_path_url)\n",
    "\n",
    "    # Storing the results in dataframe, saving locally\n",
    "    df = pd.DataFrame(list(zip(image_names, image_page_urls, image_path_urls)),\n",
    "                      columns=['image_name', 'image_page', 'image_url'])\n",
    "\n",
    "    df.to_csv(SOURCE_DIR + str(SOURCE_ID) + \"_images.csv\")\n",
    "\n",
    "    # Printing out the results\n",
    "    print(\"\\n\", \"Data scraped from CoralNet: \")\n",
    "    print(df, \"\\n\")\n",
    "\n",
    "    # Downloading the images in parallel using multiprocessing\n",
    "    print(\"Downloading images to: \", IMAGE_DIR)\n",
    "    download_images(df)\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdd218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coralnet] *",
   "language": "python",
   "name": "conda-env-coralnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
