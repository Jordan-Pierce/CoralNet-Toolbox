{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Upload CoralNet (Notebook)\n",
    "\n",
    "This notebook can be used to pass images and specific points to a CoralNet\n",
    "model for prediction. The images and points are passed to CoralNet in batches\n",
    "of 5, and the status of each job is checked every 75 seconds. If a job fails\n",
    "to upload, it will be added to a list of expired images and will be attempted\n",
    "again later. Once all images have been processed, the notebook will stop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from CoralNet_API import *\n",
    "from Download_CoralNet import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set up authentication\n",
    "\n",
    "The first step is to authenticate with CoralNet. You need to provide your\n",
    "username and password. If you don't have an account, you can create one at\n",
    "https://coralnet.ucsd.edu/. If you don't want to provide your credentials\n",
    "every time you run the script, you can store them in a separate file, or make\n",
    "them user/environmental variables. If you don't want to store your credentials\n",
    "in a file, you can also provide them as arguments when you run the script."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Successfully logged in for jordan.pierce@noaa.gov\n",
      "NOTE: Successful authentication\n"
     ]
    }
   ],
   "source": [
    "# Username\n",
    "CORALNET_USERNAME = os.getenv(\"CORALNET_USERNAME\")\n",
    "USERNAME = input(\"Username: \") if not CORALNET_USERNAME else CORALNET_USERNAME\n",
    "\n",
    "# Password\n",
    "CORALNET_PASSWORD = os.getenv(\"CORALNET_PASSWORD\")\n",
    "PASSWORD = input(\"Password: \") if not CORALNET_PASSWORD else CORALNET_PASSWORD\n",
    "\n",
    "try:\n",
    "    # Authenticate\n",
    "    authenticate(USERNAME, PASSWORD)\n",
    "    CORALNET_TOKEN, HEADERS = get_token(USERNAME, PASSWORD)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare the data\n",
    "\n",
    "The first step is to set the `SOURCE_ID` variable to represent the source\n",
    "that contains the model we want to use. We can then use the `get_model_meta`\n",
    "function to get the metadata for the model. This metadata includes the\n",
    "`MODEL_ID`. We can then use the `get_images` function to get the images\n",
    "associated with the source. The `get_images` function returns a dataframe\n",
    "that contains the `image_name` and `image_url` for each image. If there are\n",
    "many images in the source, it may take a few minutes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Metadata...\n",
      "Crawling for Images...\n"
     ]
    }
   ],
   "source": [
    "# Desired source provided by user\n",
    "SOURCE_ID = str(3420)\n",
    "\n",
    "# Variables for the model\n",
    "metadata = get_model_meta(SOURCE_ID, USERNAME, PASSWORD)\n",
    "MODEL_ID = metadata['Model_ID'][0]\n",
    "MODEL_URL = CORALNET_URL + f\"/api/classifier/{MODEL_ID}/deploy/\"\n",
    "\n",
    "# All images associated with the source\n",
    "SOURCE_IMAGES = get_images(SOURCE_ID, USERNAME, PASSWORD)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          image_name  \\\n666        mcr_lter6_out17m_pole5-6_qu4_20080406.jpg   \n244  mcr_lter3_fringingreef_pole2-3_qu6_20080503.jpg   \n93         mcr_lter1_out17m_pole3-4_qu1_20080411.jpg   \n\n                                        image_page  \\\n666  https://coralnet.ucsd.edu/image/2852711/view/   \n244  https://coralnet.ucsd.edu/image/2855209/view/   \n93   https://coralnet.ucsd.edu/image/2855834/view/   \n\n                                             image_url  \n666  https://coralnet-production.s3.amazonaws.com:4...  \n244  https://coralnet-production.s3.amazonaws.com:4...  \n93   https://coralnet-production.s3.amazonaws.com:4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>image_page</th>\n      <th>image_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>666</th>\n      <td>mcr_lter6_out17m_pole5-6_qu4_20080406.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2852711/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>244</th>\n      <td>mcr_lter3_fringingreef_pole2-3_qu6_20080503.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855209/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>mcr_lter1_out17m_pole3-4_qu1_20080411.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855834/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE_IMAGES.sample(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we set the `DATA_ROOT` variable to represent the root directory where\n",
    "subdirectories for all sources will be created. We also set the `SOURCE_DIR`\n",
    " variable to represent the directory where the current source's data will be\n",
    "  saved. We  create a folder to hold the points we want to sample from each\n",
    "  image, and the predictions for those points we'll get from the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Set the path to the root directory where you want to save the data for\n",
    "# each source. The data will be saved in a subdirectory named after the source.\n",
    "DATA_ROOT = \"../CoralNet_Data/\"\n",
    "\n",
    "# Where the output predictions will be stored\n",
    "SOURCE_DIR = DATA_ROOT + SOURCE_ID + \"/\"\n",
    "SOURCE_POINTS = SOURCE_DIR + \"points/\"\n",
    "SOURCE_PREDICTIONS = SOURCE_DIR + \"predictions/\"\n",
    "\n",
    "# Create a folder to contain predictions and points\n",
    "os.makedirs(SOURCE_DIR, exist_ok=True)\n",
    "os.makedirs(SOURCE_POINTS, exist_ok=True)\n",
    "os.makedirs(SOURCE_PREDICTIONS, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CoralNet's API requires that the images be passed as a URL that is\n",
    "publicly accessible. You can upload images to a cloud-based storage and get\n",
    "the URLs for each image, or you can upload the images to CoralNet (which\n",
    "stores them in AWS), and then download the URLs for each image using the\n",
    "`Download_CoralNet.py` script. The latter is the recommended approach.\n",
    "\n",
    "With each image, you also need to provide the points that you want to\n",
    "predict. These points should be in CSV file that has the following columns:\n",
    "- `image_name`: The name of the image that the points are associated with\n",
    "- `Row`: The row of the point\n",
    "- `Column`: The column of the point\n",
    "\n",
    "You can either provide a single CSV for all images, or a CSV for each image\n",
    "(they will be concatenated together). The cell below shows an example of a\n",
    "few images we want predictions for, and the points we want to predict."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['mcr_lter3_fringingreef_pole4-5_qu3_20080503.jpg',\n 'mcr_lter1_out17m_pole2-3_qu3_20080411.jpg',\n 'mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg',\n 'mcr_lter4_out10m_pole5-6_qu8_20080407.jpg',\n 'mcr_lter6_out10m_pole4-5_qu1_20080406.jpg',\n 'mcr_lter5_fringingreef_pole3-4_qu6_20080502.jpg',\n 'mcr_lter3_out17m_pole1-2_qu3_20080404.jpg',\n 'mcr_lter2_out10m_pole3-4_qu4_20080410.jpg']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list of image names we want predictions for. For this example,\n",
    "# we'll pretend that we have already uploaded the images to CoralNet.\n",
    "SOURCE_IMAGES['image_name'].sample(8).tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Enter the image names you want predictions for here (as a list)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "desired_images = SOURCE_IMAGES['image_name'].sample(8).tolist()\n",
    "\n",
    "# We will get the information needed from the source images dataframe\n",
    "IMAGES = SOURCE_IMAGES[SOURCE_IMAGES['image_name'].isin(desired_images)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          image_name  \\\n60         mcr_lter1_out10m_pole3-4_qu5_20080403.jpg   \n87         mcr_lter1_out17m_pole2-3_qu2_20080411.jpg   \n186        mcr_lter2_out10m_pole5-6_qu2_20080410.jpg   \n189        mcr_lter2_out10m_pole5-6_qu5_20080410.jpg   \n226        mcr_lter2_out17m_pole5-6_qu4_20080410.jpg   \n317        mcr_lter3_out17m_pole1-2_qu7_20080404.jpg   \n378  mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg   \n486  mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg   \n\n                                        image_page  \\\n60   https://coralnet.ucsd.edu/image/2855996/view/   \n87   https://coralnet.ucsd.edu/image/2855864/view/   \n186  https://coralnet.ucsd.edu/image/2855424/view/   \n189  https://coralnet.ucsd.edu/image/2855399/view/   \n226  https://coralnet.ucsd.edu/image/2855266/view/   \n317  https://coralnet.ucsd.edu/image/2854816/view/   \n378  https://coralnet.ucsd.edu/image/2854436/view/   \n486  https://coralnet.ucsd.edu/image/2853929/view/   \n\n                                             image_url  \n60   https://coralnet-production.s3.amazonaws.com:4...  \n87   https://coralnet-production.s3.amazonaws.com:4...  \n186  https://coralnet-production.s3.amazonaws.com:4...  \n189  https://coralnet-production.s3.amazonaws.com:4...  \n226  https://coralnet-production.s3.amazonaws.com:4...  \n317  https://coralnet-production.s3.amazonaws.com:4...  \n378  https://coralnet-production.s3.amazonaws.com:4...  \n486  https://coralnet-production.s3.amazonaws.com:4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>image_page</th>\n      <th>image_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>60</th>\n      <td>mcr_lter1_out10m_pole3-4_qu5_20080403.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855996/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>mcr_lter1_out17m_pole2-3_qu2_20080411.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855864/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>mcr_lter2_out10m_pole5-6_qu2_20080410.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855424/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>mcr_lter2_out10m_pole5-6_qu5_20080410.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855399/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>mcr_lter2_out17m_pole5-6_qu4_20080410.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2855266/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>mcr_lter3_out17m_pole1-2_qu7_20080404.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2854816/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>378</th>\n      <td>mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2854436/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>486</th>\n      <td>mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/2853929/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each of these images, we need to specify the points on the image we\n",
    "want the model to make predictions for. Here we use a function that will\n",
    "sample 200 points from each image. For demonstration purposes, we save\n",
    "these points as a CSV file in the `SOURCE_POINTS` folder.\n",
    "\n",
    "If you have your own CSV file(s), simply add the file paths to the\n",
    "`POINT_PATHS` list (see next cell)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating points for each of the desired images\n",
    "for image in desired_images:\n",
    "    # We use the SOURCE_IMAGES dataframe to get the URL of the image\n",
    "    image_url = SOURCE_IMAGES[SOURCE_IMAGES['image_name'] == image]['image_url'].values[0]\n",
    "    # Then we sample points from the image\n",
    "    x, y, samples = sample_points_for_url(image_url, num_samples=200, method='stratified')\n",
    "    # Finally we save the points to a csv file in the SOURCE_POINTS folder\n",
    "    pd.DataFrame(samples).to_csv(SOURCE_POINTS + image + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we get all the points for each of the desired images. If you already\n",
    "have a CSV file with all the points, you can simply add the file path to the\n",
    " list.\n",
    "\n",
    "**Enter the file paths to the CSV files here (as a list)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Get all the points for all the images\n",
    "POINT_PATHS = glob.glob(SOURCE_POINTS + \"*.csv\")\n",
    "\n",
    "# This dataframe will contain all of the points for all of the images\n",
    "# The columns are `image_name`, `Row`, and `Column`.\n",
    "POINTS = pd.DataFrame()\n",
    "# We then concatenate all the points into a single dataframe\n",
    "for path in POINT_PATHS:\n",
    "    points = pd.read_csv(path)\n",
    "    points['image_name'] = os.path.basename(path)\n",
    "    POINTS = pd.concat([POINTS, points])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that all CSV files for all images have been concatenated\n",
    "into a single dataframe. The `image_name` column represents the image that\n",
    "the points are associated with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "      row  column                                     image_name\n124  1707    1236  mcr_lter1_out10m_pole3-4_qu5_20080403.jpg.csv\n77   1000     776  mcr_lter3_out17m_pole1-2_qu7_20080404.jpg.csv\n135  1119    1212  mcr_lter2_out17m_pole5-6_qu4_20080410.jpg.csv\n160   943    1611  mcr_lter1_out10m_pole3-4_qu5_20080403.jpg.csv\n8    1183      14  mcr_lter1_out17m_pole2-3_qu2_20080411.jpg.csv",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row</th>\n      <th>column</th>\n      <th>image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>124</th>\n      <td>1707</td>\n      <td>1236</td>\n      <td>mcr_lter1_out10m_pole3-4_qu5_20080403.jpg.csv</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1000</td>\n      <td>776</td>\n      <td>mcr_lter3_out17m_pole1-2_qu7_20080404.jpg.csv</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>1119</td>\n      <td>1212</td>\n      <td>mcr_lter2_out17m_pole5-6_qu4_20080410.jpg.csv</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>943</td>\n      <td>1611</td>\n      <td>mcr_lter1_out10m_pole3-4_qu5_20080403.jpg.csv</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1183</td>\n      <td>14</td>\n      <td>mcr_lter1_out17m_pole2-3_qu2_20080411.jpg.csv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POINTS.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make predictions\n",
    "\n",
    "This is the main part of the script. We loop through each image, get the\n",
    "points for that image, and then make predictions for those points. We then\n",
    "save the predictions to a CSV file in the `SOURCE_PREDICTIONS` folder.\n",
    "\n",
    "There are multiple loops in this section. The first loop continues until all\n",
    "images have been processed. The first inner for loop prepares the data for the\n",
    "model, by creating a JSON object that contains the image URL and the points.\n",
    "These are stored in a queued list, representing the images that are waiting\n",
    "to be processed. The second inner while loop checks to see if there are any\n",
    "open positions (only 5 are allowed at a time). If there are, it will submit\n",
    "a queued job to the model until all the positions are filled. The third\n",
    "inner while loop checks the status of each job. If the job is complete, it\n",
    "will save the predictions to a CSV file, and remove the job from the active\n",
    "list. If the job is still running, it will wait 75 seconds before checking\n",
    "the status again. Once all the jobs are complete, the outer while loop will\n",
    "end, and the script will finish.\n",
    "\n",
    "Because the images are hosted on AWS, there is a chance that the URL will\n",
    "expire before the model can make a prediction. If this happens, the script\n",
    "will catch the error, and add the image to a list of expired images. Once\n",
    "all the images have been processed, the script will loop through the expired\n",
    "images, and update the URLs. It will then re-run the predictions for those\n",
    "images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBS: Queued: 0 \tActive: 0 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Getting sample points for mcr_lter1_out10m_pole3-4_qu5_20080403.jpg\n",
      "NOTE: Added mcr_lter1_out10m_pole3-4_qu5_20080403.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter1_out17m_pole2-3_qu2_20080411.jpg\n",
      "NOTE: Added mcr_lter1_out17m_pole2-3_qu2_20080411.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter2_out10m_pole5-6_qu2_20080410.jpg\n",
      "NOTE: Added mcr_lter2_out10m_pole5-6_qu2_20080410.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter2_out10m_pole5-6_qu5_20080410.jpg\n",
      "NOTE: Added mcr_lter2_out10m_pole5-6_qu5_20080410.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter2_out17m_pole5-6_qu4_20080410.jpg\n",
      "NOTE: Added mcr_lter2_out17m_pole5-6_qu4_20080410.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter3_out17m_pole1-2_qu7_20080404.jpg\n",
      "NOTE: Added mcr_lter3_out17m_pole1-2_qu7_20080404.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg\n",
      "NOTE: Added mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg to queue\n",
      "NOTE: Getting sample points for mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg\n",
      "NOTE: Added mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg to queue\n",
      "JOBS: Queued: 8 \tActive: 0 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Attempting to upload mcr_lter1_out10m_pole3-4_qu5_20080403.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_out10m_pole3-4_qu5_20080403.jpg\n",
      "NOTE: Removed mcr_lter1_out10m_pole3-4_qu5_20080403.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter2_out10m_pole5-6_qu2_20080410.jpg\n",
      "NOTE: Successfully uploaded mcr_lter2_out10m_pole5-6_qu2_20080410.jpg\n",
      "NOTE: Removed mcr_lter2_out10m_pole5-6_qu2_20080410.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter2_out17m_pole5-6_qu4_20080410.jpg\n",
      "NOTE: Successfully uploaded mcr_lter2_out17m_pole5-6_qu4_20080410.jpg\n",
      "NOTE: Removed mcr_lter2_out17m_pole5-6_qu4_20080410.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg\n",
      "NOTE: Successfully uploaded mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg\n",
      "NOTE: Removed mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_out17m_pole2-3_qu2_20080411.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_out17m_pole2-3_qu2_20080411.jpg\n",
      "NOTE: Removed mcr_lter1_out17m_pole2-3_qu2_20080411.jpg from queue\n",
      "NOTE: Maximum number of active jobs reached; checking status\n",
      "JOBS: Queued: 3 \tActive: 5 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Checking status again at 15:10:02\n",
      "Status: In Progress \tID: 21818 \tTime: 15:10:03\n",
      "Status: In Progress \tID: 21819 \tTime: 15:10:05\n",
      "Status: In Progress \tID: 21820 \tTime: 15:10:06\n",
      "Status: In Progress \tID: 21821 \tTime: 15:10:08\n",
      "Status: In Progress \tID: 21822 \tTime: 15:10:10\n",
      "NOTE: Checking status again at 15:11:26\n",
      "Status: In Progress \tID: 21818 \tTime: 15:11:27\n",
      "Status: In Progress \tID: 21819 \tTime: 15:11:28\n",
      "Status: In Progress \tID: 21820 \tTime: 15:11:30\n",
      "Status: In Progress \tID: 21821 \tTime: 15:11:32\n",
      "Status: In Progress \tID: 21822 \tTime: 15:11:33\n",
      "NOTE: Checking status again at 15:12:49\n",
      "Status: In Progress \tID: 21818 \tTime: 15:12:50\n",
      "Status: In Progress \tID: 21819 \tTime: 15:12:52\n",
      "Status: In Progress \tID: 21820 \tTime: 15:12:53\n",
      "Status: In Progress \tID: 21821 \tTime: 15:12:55\n",
      "Status: In Progress \tID: 21822 \tTime: 15:12:57\n",
      "NOTE: Checking status again at 15:14:13\n",
      "Status: In Progress \tID: 21818 \tTime: 15:14:13\n",
      "Status: In Progress \tID: 21819 \tTime: 15:14:15\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter2_out17m_pole5-6_qu4_20080410.jpg\n",
      "NOTE: Predictions for mcr_lter2_out17m_pole5-6_qu4_20080410 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg\n",
      "NOTE: Predictions for mcr_lter4_fringingreef_pole4-5_qu7_20080504 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_out17m_pole2-3_qu2_20080411.jpg\n",
      "NOTE: Predictions for mcr_lter1_out17m_pole2-3_qu2_20080411 saved successfully\n",
      "NOTE: Active jobs is 2; adding another.\n",
      "JOBS: Queued: 3 \tActive: 2 \tCompleted: 3 \tExpired: 0\n",
      "Image mcr_lter1_out10m_pole3-4_qu5_20080403.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_out17m_pole2-3_qu2_20080411.jpg has already been sampled; skipping\n",
      "Image mcr_lter2_out10m_pole5-6_qu2_20080410.jpg has already been sampled; skipping\n",
      "Image mcr_lter2_out10m_pole5-6_qu5_20080410.jpg has already been sampled; skipping\n",
      "Image mcr_lter2_out17m_pole5-6_qu4_20080410.jpg has already been sampled; skipping\n",
      "Image mcr_lter3_out17m_pole1-2_qu7_20080404.jpg has already been sampled; skipping\n",
      "Image mcr_lter4_fringingreef_pole4-5_qu7_20080504.jpg has already been sampled; skipping\n",
      "Image mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg has already been sampled; skipping\n",
      "JOBS: Queued: 3 \tActive: 2 \tCompleted: 3 \tExpired: 0\n",
      "NOTE: Attempting to upload mcr_lter2_out10m_pole5-6_qu5_20080410.jpg\n",
      "NOTE: Successfully uploaded mcr_lter2_out10m_pole5-6_qu5_20080410.jpg\n",
      "NOTE: Removed mcr_lter2_out10m_pole5-6_qu5_20080410.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg\n",
      "NOTE: Successfully uploaded mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg\n",
      "NOTE: Removed mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter3_out17m_pole1-2_qu7_20080404.jpg\n",
      "NOTE: Successfully uploaded mcr_lter3_out17m_pole1-2_qu7_20080404.jpg\n",
      "NOTE: Removed mcr_lter3_out17m_pole1-2_qu7_20080404.jpg from queue\n",
      "JOBS: Queued: 0 \tActive: 5 \tCompleted: 3 \tExpired: 0\n",
      "NOTE: Checking status again at 15:15:44\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_out10m_pole3-4_qu5_20080403.jpg\n",
      "NOTE: Predictions for mcr_lter1_out10m_pole3-4_qu5_20080403 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter2_out10m_pole5-6_qu2_20080410.jpg\n",
      "NOTE: Predictions for mcr_lter2_out10m_pole5-6_qu2_20080410 saved successfully\n",
      "Status: In Progress \tID: 21823 \tTime: 15:15:51\n",
      "Status: In Progress \tID: 21824 \tTime: 15:15:52\n",
      "Status: In Progress \tID: 21825 \tTime: 15:15:54\n",
      "NOTE: Checking status again at 15:17:10\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter2_out10m_pole5-6_qu5_20080410.jpg\n",
      "NOTE: Predictions for mcr_lter2_out10m_pole5-6_qu5_20080410 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter5_fringingreef_pole3-4_qu7_20080502.jpg\n",
      "NOTE: Predictions for mcr_lter5_fringingreef_pole3-4_qu7_20080502 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter3_out17m_pole1-2_qu7_20080404.jpg\n",
      "NOTE: Predictions for mcr_lter3_out17m_pole1-2_qu7_20080404 saved successfully\n",
      "NOTE: All images have been processed; exiting loop.\n"
     ]
    }
   ],
   "source": [
    "# Jobs that are currently queued\n",
    "queued_jobs = []\n",
    "queued_images = []\n",
    "# Jobs that are currently active\n",
    "active_jobs = []\n",
    "active_images = []\n",
    "# Jobs that are completed\n",
    "completed_jobs = []\n",
    "completed_images = []\n",
    "# A list that contains just the images that need updated urls\n",
    "expired_images = []\n",
    "# Flag to indicate if all images have been processed\n",
    "finished = False\n",
    "# The amount of time to wait before checking the status of a job\n",
    "patience = 75\n",
    "\n",
    "# This will continue looping until all images have been processed\n",
    "while not finished:\n",
    "\n",
    "    # Print the status of the jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Looping through each image requested, sample points, add to queue\n",
    "    for index, row in IMAGES.iterrows():\n",
    "        # If this image has already been sampled, skip it.\n",
    "        if row['image_name'] in queued_images + active_images + completed_images:\n",
    "            print(f\"Image {row['image_name']} has already been sampled; skipping\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        if not is_expired(row['image_url']):\n",
    "            # The image url has not expired, so we can queue the image\n",
    "            print(f\"NOTE: Getting sample points for {row['image_name']}\")\n",
    "            points = POINTS[POINTS['image_name'] == row['image_name'] + \".csv\"]\n",
    "            points = points.to_dict(orient=\"records\")\n",
    "        else:\n",
    "            # The image url expired, so we need to update it later.\n",
    "            expired_images.append(row['image_name'])\n",
    "            print(f\"WARNING: {row['image_name']} expired; adding to expired list\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        # Create a payload for the current image\n",
    "        payload = {}\n",
    "        payload['data'] = [{\"type\": \"image\",\n",
    "                            \"attributes\":\n",
    "                             {\n",
    "                                \"name\": row['image_name'],\n",
    "                                \"url\" : row['image_url'],\n",
    "                                \"points\": points\n",
    "                              },\n",
    "                          }]\n",
    "\n",
    "        job = {\n",
    "                \"image_name\": row['image_name'],\n",
    "                \"model_url\": MODEL_URL,\n",
    "                \"data\": json.dumps(payload, indent=4),\n",
    "                \"headers\": HEADERS\n",
    "              }\n",
    "\n",
    "        queued_jobs.append(job)\n",
    "        queued_images.append(row['image_name'])\n",
    "        print(f\"NOTE: Added {row['image_name']} to queue\")\n",
    "\n",
    "    # Print the status of the jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Start uploading the queued jobs to CoralNet if there are\n",
    "    # less than 5 active jobs, and there are more in the queue.\n",
    "    # If there are no queued jobs, this won't need to be entered.\n",
    "    while len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "\n",
    "        for job in queued_jobs:\n",
    "            # Flag to determine if a job needs to be removed from the queue\n",
    "            remove_from_queue = False\n",
    "\n",
    "            # Break when active gets to 5\n",
    "            if len(active_jobs) >= 5:\n",
    "                print(\"NOTE: Maximum number of active jobs reached; checking status\")\n",
    "                break # Breaks from both loops, since the while loop condition is met\n",
    "\n",
    "            # Upload the image and the sampled points to CoralNet\n",
    "            print(f\"NOTE: Attempting to upload {job['image_name']}\")\n",
    "            # Sends the requests to the `source` and in exchange, receive\n",
    "            # a message telling if it was received correctly.\n",
    "            response = requests.post(url=job[\"model_url\"],\n",
    "                                     data=job[\"data\"],\n",
    "                                     headers=job[\"headers\"])\n",
    "            if response.ok:\n",
    "                # If it was received, add to the current active jobs queue\n",
    "                print(f\"NOTE: Successfully uploaded {job['image_name']}\")\n",
    "                active_jobs.append(response)\n",
    "                active_images.append(job['image_name'])\n",
    "\n",
    "                # If the image was previously in expired, remove.\n",
    "                if job['image_name'] in expired_images:\n",
    "                    expired_images.remove(job['image_name'])\n",
    "                    print(f\"Removed {job['image_name']} from expired\")\n",
    "\n",
    "                # Marked to be removed from the queued jobs list\n",
    "                remove_from_queue = True\n",
    "            else:\n",
    "                # There was an error uploading to CoralNet; get the message\n",
    "                message = json.loads(response.text)['errors'][0]['detail']\n",
    "                print(f\"CoralNet: {message}\")\n",
    "                if \"5 jobs active\" in message:\n",
    "                    print(f\"NOTE: Will attempt again at {in_N_seconds(patience)}\")\n",
    "                    time.sleep(patience)\n",
    "\n",
    "                else:\n",
    "                    # Assumed that the image has expired; add to expired list.\n",
    "                    print(f\"ERROR: Failed to upload {job['image_name']}; added to expired\")\n",
    "                    expired_images.append(job['image_name'])\n",
    "\n",
    "                    # Marked to be removed from the queued jobs list\n",
    "                    remove_from_queue = True\n",
    "\n",
    "            # Only if the job was successfully uploaded, or expired, remove\n",
    "            # from the queued jobs list. This won't be reached if there were\n",
    "            # any active jobs from before.\n",
    "            if remove_from_queue:\n",
    "                queued_jobs.remove(job)\n",
    "                queued_images.remove(job['image_name'])\n",
    "                print(f\"NOTE: Removed {job['image_name']} from queue\")\n",
    "\n",
    "    # Check the status of the active jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Check the status of the active jobs, break when another can be added\n",
    "    while len(active_jobs) <= 5 and len(active_jobs) != 0:\n",
    "\n",
    "        # Sleep before checking status again\n",
    "        print(f\"NOTE: Checking status again at {in_N_seconds(patience)}\")\n",
    "        time.sleep(patience)\n",
    "\n",
    "        # Loop through the active jobs\n",
    "        for (job, image_name) in list(zip(active_jobs, active_images)):\n",
    "            # Check the status of the current job\n",
    "            current_status, message, wait = check_job_status(job, CORALNET_TOKEN)\n",
    "            print(message); time.sleep(wait)\n",
    "            # Current job has finished, output the results, remove from queue\n",
    "            if message == \"Completed Job\":\n",
    "                print(f\"NOTE: {message} for {image_name}\")\n",
    "                # Convert to csv, and save locally\n",
    "                convert_to_csv(current_status, image_name, SOURCE_PREDICTIONS)\n",
    "                # Add to completed jobs list\n",
    "                completed_jobs.append(current_status)\n",
    "                completed_images.append(image_name)\n",
    "                # Remove from active jobs list\n",
    "                active_jobs.remove(job)\n",
    "                active_images.remove(image_name)\n",
    "\n",
    "        # After checking the current status, break if another can be added\n",
    "        # Else wait and check the status of the active jobs again.\n",
    "        if len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "            print(f\"NOTE: Active jobs is {len(active_jobs)}; adding another.\")\n",
    "            break\n",
    "\n",
    "    # If there are no queued jobs, and no active jobs, but there are images in\n",
    "    # expired, resample points, and add to the queue. This gets just the AWS\n",
    "    # URL for the expired images and updates the image dataframe.\n",
    "    if not queued_jobs and not active_jobs and expired_images:\n",
    "        print(\"NOTE: Updating expired images' URL\")\n",
    "        # Get the subset of images dataframe containing only the expired images\n",
    "        IMAGES = IMAGES[IMAGES['image_name'].isin(expired_images)]\n",
    "        new_urls = []\n",
    "        for i, r in IMAGES.iterrows():\n",
    "            new_urls.append(get_image_url(r['image_page'], USERNAME, PASSWORD))\n",
    "        # Store the new urls in the subset of images dataframe\n",
    "        IMAGES['image_url'] = new_urls\n",
    "\n",
    "    # Check to see everything has been completed, breaking the loop\n",
    "    if not queued_jobs and not active_jobs and not expired_images:\n",
    "        print(\"NOTE: All images have been processed; exiting loop.\")\n",
    "        finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
