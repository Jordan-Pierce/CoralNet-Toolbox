{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1682713350068
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from Download_CoralNet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(username, password):\n",
    "    \"\"\"\n",
    "    Retrieves a CoralNet authentication token for API requests.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the CoralNet token and request headers for authenticated requests.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If authentication fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Requirements for authentication\n",
    "    CORALNET_AUTH = CORALNET_URL + \"/api/token_auth/\"\n",
    "    HEADERS = {\"Content-type\" : \"application/vnd.api+json\"}\n",
    "    PAYLOAD =  {\"username\": username, \"password\": password}\n",
    "\n",
    "    # Response from CoralNet when provided credentials\n",
    "    response = requests.post(CORALNET_AUTH,\n",
    "                             data=json.dumps(PAYLOAD),\n",
    "                             headers=HEADERS)\n",
    "\n",
    "    if response.ok:\n",
    "\n",
    "        print(\"NOTE: Successful authentication\")\n",
    "\n",
    "        # Get the coralnet token returned to the user\n",
    "        CORALNET_TOKEN = json.loads(response.content.decode())['token']\n",
    "\n",
    "        # Update the header to contain the user's coralnet token\n",
    "        HEADERS = {\"Authorization\": f\"Token {CORALNET_TOKEN}\",\n",
    "                   \"Content-type\": \"application/vnd.api+json\"}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"ERROR: Could not authenticate\\n{response.content}\")\n",
    "\n",
    "    return CORALNET_TOKEN, HEADERS\n",
    "\n",
    "\n",
    "def in_N_seconds(wait):\n",
    "    \"\"\"\n",
    "    Calculate the time in N seconds from the current time.\n",
    "\n",
    "    Args:\n",
    "    - wait: an integer representing the number of seconds to wait\n",
    "\n",
    "    Returns:\n",
    "    - A string representing the time in `wait` seconds from the current time, in the format \"HH:MM:SS\"\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    then = now + datetime.timedelta(seconds=wait)\n",
    "    return then.strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def check_expiration(url):\n",
    "    \"\"\"\n",
    "    Calculates the time remaining before a URL expires, based on its \"Expires\" timestamp.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to check.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    valid = False\n",
    "    time_remaining = 0\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Extract expiration timestamp from URL\n",
    "        match = re.search(r\"Expires=(\\d+)\", url)\n",
    "\n",
    "        if match:\n",
    "            expiration = int(match.group(1))\n",
    "\n",
    "            # Calculate time remaining before expiration\n",
    "            now = int(time.time())\n",
    "            time_remaining = expiration - now\n",
    "        else:\n",
    "            raise ValueError(f\"ERROR: Could not find expiration timestamp in URL\\n{url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "\n",
    "    # Check the amount of time remaining\n",
    "    if time_remaining >= 60:\n",
    "        valid = True\n",
    "\n",
    "    return valid\n",
    "\n",
    "\n",
    "def sample_points_for_url(url, num_samples=200, method='stratified'):\n",
    "    \"\"\"\n",
    "    Generates a set of sample coordinates within a given image size.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    width : int\n",
    "        The width of the image.\n",
    "    height : int\n",
    "        The height of the image.\n",
    "    num_samples : int, optional\n",
    "        The number of samples to generate. Default is 200.\n",
    "    method : str, optional\n",
    "        The method to use for generating samples. Valid values are:\n",
    "        - 'uniform': generates samples using uniform sampling\n",
    "        - 'random': generates samples using random sampling\n",
    "        - 'stratified': generates samples using stratified sampling (default)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing three elements:\n",
    "        - A numpy array of x-coordinates of the generated samples.\n",
    "        - A numpy array of y-coordinates of the generated samples.\n",
    "        - A list of dictionaries containing row and column coordinates of the generated samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not check_expiration(url):\n",
    "        raise Exception(f\"ERROR: URL is expiring soon; skipping.\\n{url}\")\n",
    "        \n",
    "    else:\n",
    "        # Request the image from AWS\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Read it to get the size\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        width, height = img.size\n",
    "        \n",
    "        x_coordinates = []\n",
    "        y_coordinates = []\n",
    "        samples = []\n",
    "\n",
    "        if method == 'uniform':\n",
    "            x_coords = np.linspace(0, width-1, int(np.sqrt(num_samples)))\n",
    "            y_coords = np.linspace(0, height-1, int(np.sqrt(num_samples)))\n",
    "            for x in x_coords:\n",
    "                for y in y_coords:\n",
    "                    x_coordinates.append(int(x))\n",
    "                    y_coordinates.append(int(y))\n",
    "                    samples.append({'row': int(y), 'column': int(x)})\n",
    "\n",
    "        elif method == 'random':\n",
    "            for i in range(num_samples):\n",
    "                x = random.randint(0, width-1)\n",
    "                y = random.randint(0, height-1)\n",
    "                x_coordinates.append(x)\n",
    "                y_coordinates.append(y)\n",
    "                samples.append({'row': y, 'column': x})\n",
    "\n",
    "        elif method == 'stratified':\n",
    "            n = int(np.sqrt(num_samples))\n",
    "            x_range = np.linspace(0, width-1, n+1)\n",
    "            y_range = np.linspace(0, height-1, n+1)\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    x = np.random.uniform(x_range[i], x_range[i+1])\n",
    "                    y = np.random.uniform(y_range[j], y_range[j+1])\n",
    "                    x_coordinates.append(int(x))\n",
    "                    y_coordinates.append(int(y))\n",
    "                    samples.append({'row': int(y), 'column': int(x)})\n",
    "\n",
    "        x = np.array(x_coordinates).astype(int)\n",
    "        y = np.array(y_coordinates).astype(int)\n",
    "\n",
    "    return x, y, samples\n",
    "\n",
    "\n",
    "def check_job_status(response):\n",
    "    \"\"\"\n",
    "    Sends a request to retrieve the completed annotations and returns the status update.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    response : requests.Response\n",
    "        A Response object returned from a previous request to CoralNet API.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing status information, which includes the following keys:\n",
    "        - 'status': a string indicating the current status of the job, such as \"in progress\" or \"completed\"\n",
    "        - 'message': a string providing additional details about the job status, if available\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sends a request to retrieve the completed annotations, obtains status update\n",
    "    status = requests.get(url=f\"https://coralnet.ucsd.edu{response.headers['Location']}\", \n",
    "                      headers={\"Authorization\": f\"Token {CORALNET_TOKEN}\"})\n",
    "    \n",
    "    current_status = json.loads(status.content) \n",
    "    wait = 1\n",
    "\n",
    "    if status.ok:\n",
    "        \n",
    "        # Still in progress\n",
    "        if 'status' in current_status['data'][0]['attributes'].keys(): \n",
    "\n",
    "            s = current_status['data'][0]['attributes']['successes'] \n",
    "            f = current_status['data'][0]['attributes']['failures'] \n",
    "            t = current_status['data'][0]['attributes']['total']\n",
    "            status_str = current_status['data'][0]['attributes']['status'] \n",
    "            ids = current_status['data'][0]['id'].split(\",\")\n",
    "            ids = ''.join(str(_) for _ in ids)\n",
    "\n",
    "            now = time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            message = f\"Status: {status_str} \\tID: {ids} \\tTime: {now}\"\n",
    "\n",
    "        else:\n",
    "            # Completed\n",
    "            message = \"Completed Job\"\n",
    "            \n",
    "    else:\n",
    "        # CoralNet is getting too many requests, sleep for a second.\n",
    "        message = f\"CoralNet: {current_status['errors'][0]['detail']}\"\n",
    "        try:\n",
    "            # Try to wait the amount of time requested by CoralNet\n",
    "            match = re.search(r'\\d+', message)\n",
    "            wait = int(match.group())\n",
    "        except:\n",
    "            wait = 30\n",
    "\n",
    "    return current_status, message, wait\n",
    "\n",
    "\n",
    "def print_job_status(queue, active, completed, expired):\n",
    "    \"\"\"\n",
    "    Print the current status of jobs and images being processed.\n",
    "\n",
    "    Args:\n",
    "    - queued_jobs (list): A list of jobs that are currently queued.\n",
    "    - active_jobs (list): A list of jobs that are currently active.\n",
    "    - completed_jobs (list): A list of jobs that have been completed.\n",
    "    - expired_images (list): A list of images that need updated URLs.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(f\"JOBS: Queued: {len(queue)} \\t\"\n",
    "          f\"Active: {len(active)} \\t\"\n",
    "          f\"Completed: {len(completed)} \\t\"\n",
    "          f\"Expired: {len(expired)}\")\n",
    "\n",
    "\n",
    "def convert_to_csv(response, image_name, output_dir):\n",
    "    \"\"\"\n",
    "    Converts response data into a Pandas DataFrame and concatenates each row into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    response : dict\n",
    "        A dictionary object containing response data from a server.\n",
    "    image_file : str\n",
    "        The name of the image file corresponding to the response data.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    model_predictions : pandas.DataFrame\n",
    "        A Pandas DataFrame containing prediction data, where each row represents a single point in the image.\n",
    "        The columns of the DataFrame include 'image', 'X', 'Y', 'score_*', 'label_id_*', 'label_code_*', and 'label_name_*'.\n",
    "        The asterisk (*) in the column names represents the index of the classification for each point, starting at 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_predictions = pd.DataFrame()\n",
    "\n",
    "    for point in response['data'][0]['attributes']['points']:\n",
    "\n",
    "        per_point = dict()\n",
    "        per_point['image'] = image_name\n",
    "        per_point['X'] = point['column']\n",
    "        per_point['Y'] = point['row']\n",
    "\n",
    "        for index, classification in enumerate(point['classifications']):\n",
    "\n",
    "            per_point['score_' + str(index + 1)] = classification['score']\n",
    "            per_point['label_id_' + str(index + 1)] = classification['label_id']\n",
    "            per_point['label_code_' + str(index + 1)] = classification['label_code']\n",
    "            per_point['label_name_' + str(index + 1)] = classification['label_name']\n",
    "\n",
    "        model_predictions = pd.concat([model_predictions, pd.DataFrame.from_dict([per_point])])\n",
    "        \n",
    "    basename = os.path.basename(image_name).split(\".\")[0]\n",
    "    output_file = output_dir + basename + \".csv\"\n",
    "    model_predictions.reset_index(drop=True, inplace=True)\n",
    "    model_predictions.to_csv(output_file, index=True)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"NOTE: Predictions for {basename} saved successfully\")\n",
    "    else:\n",
    "        print(f\"ERROR: Could not save predictions for {basename}\")\n",
    "    \n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Successful authentication\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Username and password provided by user\n",
    "    USERNAME = os.getenv(\"CORALNET_USERNAME\")\n",
    "    PASSWORD = os.getenv(\"CORALNET_PASSWORD\")\n",
    "\n",
    "    # Verify that the username and password are valid\n",
    "    CORALNET_TOKEN, HEADERS = get_token(USERNAME, PASSWORD)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sys.exit()\n",
    "\n",
    "# Desired source provided by user\n",
    "SOURCE_ID = str(4006)\n",
    "\n",
    "# Set the data root\n",
    "DATA_ROOT = \"C://Users/jordan.pierce/Documents/GitHub/CoralNet_Tools/CoralNet_Data/4006/\"\n",
    "OUTPUT_PREDICTIONS = DATA_ROOT + \"predictions/\"\n",
    "\n",
    "# Create a folder to contain predictions and points\n",
    "os.makedirs(OUTPUT_PREDICTIONS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Metadata...\n",
      "Crawling for Images...\n"
     ]
    }
   ],
   "source": [
    "# Variables for the model\n",
    "metadata = get_model_meta(SOURCE_ID, USERNAME, PASSWORD)\n",
    "MODEL_ID = metadata['Model_ID'][0]\n",
    "MODEL_URL = CORALNET_URL + f\"/api/classifier/{MODEL_ID}/deploy/\"\n",
    "\n",
    "# Images associated with the source\n",
    "images = get_images(SOURCE_ID, USERNAME, PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                         image_name  \\\n7   mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg   \n3   mcr_lter1_fringingreef_pole1-2_qu4_20080415.jpg   \n17  mcr_lter1_fringingreef_pole3-4_qu2_20080415.jpg   \n\n                                       image_page  \\\n7   https://coralnet.ucsd.edu/image/3370050/view/   \n3   https://coralnet.ucsd.edu/image/3370046/view/   \n17  https://coralnet.ucsd.edu/image/3370060/view/   \n\n                                            image_url  \n7   https://coralnet-production.s3.amazonaws.com:4...  \n3   https://coralnet-production.s3.amazonaws.com:4...  \n17  https://coralnet-production.s3.amazonaws.com:4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>image_page</th>\n      <th>image_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/3370050/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mcr_lter1_fringingreef_pole1-2_qu4_20080415.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/3370046/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>mcr_lter1_fringingreef_pole3-4_qu2_20080415.jpg</td>\n      <td>https://coralnet.ucsd.edu/image/3370060/view/</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desired images\n",
    "images = images.sample(3, replace=False)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBS: Queued: 0 \tActive: 0 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg to queue\n",
      "NOTE: Sampled 196 points for mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg\n",
      "NOTE: Added mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg to queue\n",
      "JOBS: Queued: 10 \tActive: 0 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg from queue\n",
      "JOBS: Queued: 5 \tActive: 5 \tCompleted: 0 \tExpired: 0\n",
      "NOTE: Checking status again at 14:14:55\n",
      "Status: In Progress \tID: 21783 \tTime: 14:14:55\n",
      "Status: In Progress \tID: 21784 \tTime: 14:14:57\n",
      "Status: In Progress \tID: 21785 \tTime: 14:14:59\n",
      "Status: In Progress \tID: 21786 \tTime: 14:15:00\n",
      "Status: In Progress \tID: 21787 \tTime: 14:15:02\n",
      "NOTE: Checking status again at 14:16:18\n",
      "Status: In Progress \tID: 21783 \tTime: 14:16:19\n",
      "Status: In Progress \tID: 21784 \tTime: 14:16:20\n",
      "Status: In Progress \tID: 21785 \tTime: 14:16:22\n",
      "Status: In Progress \tID: 21786 \tTime: 14:16:24\n",
      "Status: In Progress \tID: 21787 \tTime: 14:16:25\n",
      "NOTE: Checking status again at 14:17:41\n",
      "Status: In Progress \tID: 21783 \tTime: 14:17:42\n",
      "Status: In Progress \tID: 21784 \tTime: 14:17:44\n",
      "Status: In Progress \tID: 21785 \tTime: 14:17:45\n",
      "Status: In Progress \tID: 21786 \tTime: 14:17:47\n",
      "Status: In Progress \tID: 21787 \tTime: 14:17:49\n",
      "NOTE: Checking status again at 14:19:05\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole1-2_qu2_20080415 saved successfully\n",
      "Status: In Progress \tID: 21784 \tTime: 14:19:08\n",
      "Status: In Progress \tID: 21785 \tTime: 14:19:10\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole1-2_qu8_20080415 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole3-4_qu4_20080415 saved successfully\n",
      "NOTE: Active jobs is 2; adding another.\n",
      "JOBS: Queued: 5 \tActive: 2 \tCompleted: 3 \tExpired: 0\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg has already been sampled; skipping\n",
      "JOBS: Queued: 5 \tActive: 2 \tCompleted: 3 \tExpired: 0\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg from queue\n",
      "JOBS: Queued: 2 \tActive: 5 \tCompleted: 3 \tExpired: 0\n",
      "NOTE: Checking status again at 14:20:34\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole3-4_qu1_20080415 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole4-5_qu2_20080415 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole2-3_qu7_20080415 saved successfully\n",
      "Status: In Progress \tID: 21789 \tTime: 14:20:43\n",
      "Status: In Progress \tID: 21790 \tTime: 14:20:45\n",
      "NOTE: Active jobs is 2; adding another.\n",
      "JOBS: Queued: 2 \tActive: 2 \tCompleted: 6 \tExpired: 0\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu7_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole3-4_qu1_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole4-5_qu2_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg has already been sampled; skipping\n",
      "Image mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg has already been sampled; skipping\n",
      "JOBS: Queued: 2 \tActive: 2 \tCompleted: 6 \tExpired: 0\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg from queue\n",
      "NOTE: Attempting to upload mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg\n",
      "NOTE: Successfully uploaded mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg\n",
      "NOTE: Removed mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg from queue\n",
      "JOBS: Queued: 0 \tActive: 4 \tCompleted: 6 \tExpired: 0\n",
      "NOTE: Checking status again at 14:22:03\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole1-2_qu1_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole1-2_qu1_20080415 saved successfully\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole2-3_qu5_20080415 saved successfully\n",
      "Status: In Progress \tID: 21791 \tTime: 14:22:09\n",
      "Status: In Progress \tID: 21792 \tTime: 14:22:11\n",
      "NOTE: Checking status again at 14:23:27\n",
      "Status: In Progress \tID: 21791 \tTime: 14:23:28\n",
      "Status: In Progress \tID: 21792 \tTime: 14:23:29\n",
      "NOTE: Checking status again at 14:24:45\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole1-2_qu7_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole1-2_qu7_20080415 saved successfully\n",
      "Status: In Progress \tID: 21792 \tTime: 14:24:49\n",
      "NOTE: Checking status again at 14:26:05\n",
      "Status: In Progress \tID: 21792 \tTime: 14:26:05\n",
      "NOTE: Checking status again at 14:27:21\n",
      "Completed Job\n",
      "NOTE: Completed Job for mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg\n",
      "NOTE: Predictions for mcr_lter1_fringingreef_pole2-3_qu8_20080415 saved successfully\n",
      "NOTE: All images have been processed; exiting loop.\n"
     ]
    }
   ],
   "source": [
    "# Jobs that are currently queued\n",
    "queued_jobs = []\n",
    "queued_images = []\n",
    "# Jobs that are currently active\n",
    "active_jobs = []\n",
    "active_images = []\n",
    "# Jobs that are completed\n",
    "completed_jobs = []\n",
    "completed_images = []\n",
    "# A list that contains just the images that need updated urls\n",
    "expired_images = []\n",
    "# Flag to indicate if all images have been processed\n",
    "finished = False\n",
    "# The amount of time to wait before checking the status of a job\n",
    "patience = 75\n",
    "\n",
    "# This will continue looping until all images have been processed\n",
    "while not finished:\n",
    "\n",
    "    # Print the status of the jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Looping through each image requested, sample points, add to queue\n",
    "    for index, row in images.iterrows():\n",
    "        # If this image has already been sampled, skip it.\n",
    "        if row['image_name'] in queued_images + active_images + completed_images:\n",
    "            print(f\"Image {row['image_name']} has already been sampled; skipping\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        try:\n",
    "            # Sample points from image\n",
    "            x, y, points = sample_points_for_url(row['image_url'], 200)\n",
    "            print(f\"NOTE: Sampled {len(points)} points for {row['image_name']}\")\n",
    "        except:\n",
    "            # The image url expired, so we need to resample points again later\n",
    "            expired_images.append(row['image_name'])\n",
    "            print(f\"WARNING: Could not sample points; {row['image_name']} expired.\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        # Create a payload for the current image\n",
    "        payload = {}\n",
    "        payload['data'] = [{\"type\": \"image\",\n",
    "                            \"attributes\":\n",
    "                             {\n",
    "                                \"name\": row['image_name'],\n",
    "                                \"url\" : row['image_url'],\n",
    "                                \"points\": points\n",
    "                              },\n",
    "                          }]\n",
    "\n",
    "        job = {\n",
    "                \"image_name\": row['image_name'],\n",
    "                \"model_url\": MODEL_URL,\n",
    "                \"data\": json.dumps(payload, indent=4),\n",
    "                \"headers\": HEADERS\n",
    "              }\n",
    "\n",
    "        queued_jobs.append(job)\n",
    "        queued_images.append(row['image_name'])\n",
    "        print(f\"NOTE: Added {row['image_name']} to queue\")\n",
    "\n",
    "    # Print the status of the jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Start uploading the queued jobs to CoralNet if there are\n",
    "    # less than 5 active jobs, and there are more in the queue.\n",
    "    # If there are no queued jobs, this won't need to be entered.\n",
    "    while len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "\n",
    "        for job in queued_jobs:\n",
    "            # Flag to determine if a job needs to be removed from the queue\n",
    "            remove_from_queue = False\n",
    "\n",
    "            # Break when active gets to 5\n",
    "            if len(active_jobs) >= 5:\n",
    "                print(\"NOTE: Maximum number of active jobs reached; checking status\")\n",
    "                break # Breaks from both loops, since the while loop condition is met\n",
    "\n",
    "            # Upload the image and the sampled points to CoralNet\n",
    "            print(f\"NOTE: Attempting to upload {job['image_name']}\")\n",
    "            # Sends the requests to the `source` and in exchange, receive\n",
    "            # a message telling if it was received correctly.\n",
    "            response = requests.post(url=job[\"model_url\"],\n",
    "                                     data=job[\"data\"],\n",
    "                                     headers=job[\"headers\"])\n",
    "            if response.ok:\n",
    "                # If it was received, add to the current active jobs queue\n",
    "                print(f\"NOTE: Successfully uploaded {job['image_name']}\")\n",
    "                active_jobs.append(response)\n",
    "                active_images.append(job['image_name'])\n",
    "\n",
    "                # If the image was previously in expired, remove.\n",
    "                if job['image_name'] in expired_images:\n",
    "                    expired_images.remove(job['image_name'])\n",
    "                    print(f\"Removed {job['image_name']} from expired\")\n",
    "\n",
    "                # Marked to be removed from the queued jobs list\n",
    "                remove_from_queue = True\n",
    "            else:\n",
    "                # There was an error uploading to CoralNet; get the message\n",
    "                message = json.loads(response.text)['errors'][0]['detail']\n",
    "                print(f\"CoralNet: {message}\")\n",
    "                if \"5 jobs active\" in message:\n",
    "                    print(f\"NOTE: Will attempt again at {in_N_seconds(patience)}\")\n",
    "                    time.sleep(patience)\n",
    "\n",
    "                else:\n",
    "                    # Assumed that the image has expired; add to expired list.\n",
    "                    print(f\"ERROR: Failed to upload {job['image_name']}; added to expired\")\n",
    "                    expired_images.append(job['image_name'])\n",
    "\n",
    "                    # Marked to be removed from the queued jobs list\n",
    "                    remove_from_queue = True\n",
    "\n",
    "            # Only if the job was successfully uploaded, or expired, remove\n",
    "            # from the queued jobs list. This won't be reached if there were\n",
    "            # any active jobs from before.\n",
    "            if remove_from_queue:\n",
    "                queued_jobs.remove(job)\n",
    "                queued_images.remove(job['image_name'])\n",
    "                print(f\"NOTE: Removed {job['image_name']} from queue\")\n",
    "\n",
    "    # Check the status of the active jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_images)\n",
    "\n",
    "    # Check the status of the active jobs, break when another can be added\n",
    "    while len(active_jobs) <= 5 and len(active_jobs) != 0:\n",
    "\n",
    "        # Sleep before checking status again\n",
    "        print(f\"NOTE: Checking status again at {in_N_seconds(patience)}\")\n",
    "        time.sleep(patience)\n",
    "\n",
    "        # Loop through the active jobs\n",
    "        for (job, image_name) in list(zip(active_jobs, active_images)):\n",
    "            # Check the status of the current job\n",
    "            current_status, message, wait = check_job_status(job)\n",
    "            print(message); time.sleep(wait)\n",
    "            # Current job has finished, output the results, remove from queue\n",
    "            if message == \"Completed Job\":\n",
    "                print(f\"NOTE: {message} for {image_name}\")\n",
    "                # Convert to csv, and save locally\n",
    "                convert_to_csv(current_status, image_name, OUTPUT_PREDICTIONS)\n",
    "                # Add to completed jobs list\n",
    "                completed_jobs.append(current_status)\n",
    "                completed_images.append(image_name)\n",
    "                # Remove from active jobs list\n",
    "                active_jobs.remove(job)\n",
    "                active_images.remove(image_name)\n",
    "\n",
    "        # After checking the current status, break if another can be added\n",
    "        # Else wait and check the status of the active jobs again.\n",
    "        if len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "            print(f\"NOTE: Active jobs is {len(active_jobs)}; adding another.\")\n",
    "            break\n",
    "\n",
    "    # If there are no queued jobs, and no active jobs, but there are images in\n",
    "    # expired, resample points, and add to the queue. This gets just the AWS\n",
    "    # URL for the expired images and updates the image dataframe.\n",
    "    if not queued_jobs and not active_jobs and expired_images:\n",
    "        print(\"NOTE: Updating expired images' URL\")\n",
    "        # Get the subset of images dataframe containing only the expired images\n",
    "        images = images[images['image_name'].isin(expired_images)]\n",
    "        new_urls = []\n",
    "        for i, r in images.iterrows():\n",
    "            new_urls.append(get_image_url(r['image_page'], USERNAME, PASSWORD))\n",
    "        # Store the new urls in the subset of images dataframe\n",
    "        images['image_url'] = new_urls\n",
    "\n",
    "    # Check to see everything has been completed, breaking the loop\n",
    "    if not queued_jobs and not active_jobs and not expired_images:\n",
    "        print(\"NOTE: All images have been processed; exiting loop.\")\n",
    "        finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
