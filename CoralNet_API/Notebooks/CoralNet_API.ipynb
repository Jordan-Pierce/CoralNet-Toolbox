{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### CoralNet API (Notebook)\n",
    "\n",
    "This notebook can be used to pass images and specific points to a CoralNet\n",
    "model for prediction. The images and points are passed to CoralNet in batches\n",
    "of 5, and the status of each job is checked every 75 seconds. If a job fails\n",
    "to upload, it will be added to a list of expired images and will be attempted\n",
    "again later. Once all images have been processed, the notebook will stop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1087878500.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn [1], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    ..from CoralNet_API import *\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from ..CoralNet_API import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set up authentication\n",
    "\n",
    "The first step is to authenticate with CoralNet. You need to provide your\n",
    "username and password. If you don't have an account, you can create one at\n",
    "https://coralnet.ucsd.edu/. If you don't want to provide your credentials\n",
    "every time you run the script, you can store them in a separate file, or make\n",
    "them user/environmental variables. If you don't want to store your credentials\n",
    "in a file, you can also provide them as arguments when you run the script."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Authentication successful for jordan.pierce@noaa.gov\n",
      "NOTE: Token retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Username\n",
    "CORALNET_USERNAME = os.getenv(\"CORALNET_USERNAME\")\n",
    "USERNAME = input(\"Username: \") if not CORALNET_USERNAME else CORALNET_USERNAME\n",
    "\n",
    "# Password\n",
    "CORALNET_PASSWORD = os.getenv(\"CORALNET_PASSWORD\")\n",
    "PASSWORD = input(\"Password: \") if not CORALNET_PASSWORD else CORALNET_PASSWORD\n",
    "\n",
    "try:\n",
    "    # Authenticate\n",
    "    authenticate(USERNAME, PASSWORD)\n",
    "    CORALNET_TOKEN, HEADERS = get_token(USERNAME, PASSWORD)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare the data\n",
    "\n",
    "The first step is to set the `SOURCE_ID` variable to represent the source\n",
    "that contains the model we want to use. We can then use the `download_metadata`\n",
    "function to get the metadata for the model. This metadata includes the\n",
    "`MODEL_ID`. We can then use the `get_images` function to get the images\n",
    "associated with the source. The `get_images` function returns a dataframe\n",
    "that contains the `image_name` and `image_page_url` for each image. We then use the\n",
    "`get_image_urls` function to get the most updated AWS `image_url` (they expire every hour).\n",
    "\n",
    "To use these functions, we create a driver (i.e., browser) that takes the username and password,\n",
    " logins into CoralNet, and then passes the driver to the functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Desired source provided by user\n",
    "SOURCE_ID = str(4060)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Downloading model metadata for 4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Crawling all pages for source 4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Finished crawling all pages\n",
      "NOTE: Retrieving image URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Retrieved 30 image URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the options for the driver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-images\")\n",
    "\n",
    "# Pass the options object while creating the driver\n",
    "driver = check_for_browsers(options=options)\n",
    "# Store the credentials in the driver\n",
    "driver.capabilities['credentials'] = {\n",
    "    'username': USERNAME,\n",
    "    'password': PASSWORD\n",
    "}\n",
    "# Login to CoralNet\n",
    "driver, _ = login(driver)\n",
    "\n",
    "# Variables for the model\n",
    "driver, meta = download_metadata(driver, SOURCE_ID)\n",
    "\n",
    "# Parse the model ID\n",
    "MODEL_ID = meta['Model_ID'][0]\n",
    "MODEL_URL = CORALNET_URL + f\"/api/classifier/{MODEL_ID}/deploy/\"\n",
    "\n",
    "# Get the images for the source\n",
    "driver, SOURCE_IMAGES = get_images(driver, SOURCE_ID)\n",
    "\n",
    "# Get the image page URLs\n",
    "image_pages = SOURCE_IMAGES['image_page'].tolist()\n",
    "driver, SOURCE_IMAGES['image_url'] = get_image_urls(driver, image_pages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       image_page  \\\n12  https://coralnet.ucsd.edu/image/3390414/view/   \n19  https://coralnet.ucsd.edu/image/3392453/view/   \n15  https://coralnet.ucsd.edu/image/3392449/view/   \n\n                                         image_name  \\\n12  mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg   \n19  mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg   \n15  mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg   \n\n                                            image_url  \n12  https://coralnet-production.s3.amazonaws.com:4...  \n19  https://coralnet-production.s3.amazonaws.com:4...  \n15  https://coralnet-production.s3.amazonaws.com:4...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_page</th>\n      <th>image_name</th>\n      <th>image_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>https://coralnet.ucsd.edu/image/3390414/view/</td>\n      <td>mcr_lter1_fringingreef_pole2-3_qu5_20080415.jpg</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>https://coralnet.ucsd.edu/image/3392453/view/</td>\n      <td>mcr_lter1_fringingreef_pole3-4_qu4_20080415.jpg</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>https://coralnet.ucsd.edu/image/3392449/view/</td>\n      <td>mcr_lter1_fringingreef_pole2-3_qu8_20080415.jpg</td>\n      <td>https://coralnet-production.s3.amazonaws.com:4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a few images from the source\n",
    "SOURCE_IMAGES.sample(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we set the `DATA_ROOT` variable to represent the root directory where\n",
    "subdirectories for all sources will be created. We also set the `SOURCE_DIR`\n",
    " variable to represent the directory where the current source's data will be\n",
    "  saved. We  create a folder to hold the points we want to sample from each\n",
    "  image, and the predictions for those points we'll get from the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Set the path to the root directory where you want to save the data for\n",
    "# each source. The data will be saved in a subdirectory named after the source.\n",
    "DATA_ROOT = \"../CoralNet_Data/\"\n",
    "\n",
    "# Where the output predictions will be stored\n",
    "SOURCE_DIR = DATA_ROOT + SOURCE_ID + \"/\"\n",
    "SOURCE_POINTS = SOURCE_DIR + \"points/\"\n",
    "SOURCE_PREDICTIONS = SOURCE_DIR + \"predictions/\"\n",
    "\n",
    "# Create a folder to contain predictions and points\n",
    "os.makedirs(SOURCE_DIR, exist_ok=True)\n",
    "os.makedirs(SOURCE_POINTS, exist_ok=True)\n",
    "os.makedirs(SOURCE_PREDICTIONS, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CoralNet's API requires that the images be passed as a URL that is\n",
    "publicly accessible. You can upload images to a cloud-based storage and get\n",
    "the URLs for each image, or you can upload the images to CoralNet (which\n",
    "stores them in AWS), and then download the URLs for each image using the\n",
    "`Download_CoralNet.py` script. The latter is the recommended approach.\n",
    "\n",
    "With each image, you also need to provide the points that you want to\n",
    "predict. These points should be in CSV file that has the following columns:\n",
    "- `image_name`: The name of the image that the points are associated with\n",
    "- `Row`: The row of the point\n",
    "- `Column`: The column of the point\n",
    "\n",
    "You can either provide a single CSV for all images, or a CSV for each image\n",
    "(they will be concatenated together). The cell below shows an example of a\n",
    "few images we want predictions for."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Enter the image names you want predictions for here (as a list)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Here we sample a few image names we want predictions for\n",
    "desired_images = SOURCE_IMAGES['image_name'].sample(10)\n",
    "\n",
    "# We will get the information needed from the source images dataframe\n",
    "IMAGES = SOURCE_IMAGES[SOURCE_IMAGES['image_name'].isin(desired_images)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each of these images, we need to specify the points on the image we\n",
    "want the model to make predictions for. Here we use a function that will\n",
    "sample 200 points from each image. For demonstration purposes, we save\n",
    "these points as a CSV file in the `SOURCE_POINTS` folder.\n",
    "\n",
    "If you have your own CSV file(s), simply add the file paths to the\n",
    "`POINT_PATHS` list (see next cell)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating points for each of the desired images\n",
    "for image in desired_images:\n",
    "    # We use the SOURCE_IMAGES dataframe to get the URL of the image\n",
    "    image_url = IMAGES[IMAGES['image_name'] == image]['image_url'].values[0]\n",
    "    # Then we sample points from the image\n",
    "    x, y, samples = sample_points_for_url(image_url, num_samples=200, method='stratified')\n",
    "    # If the url hasn't expired\n",
    "    if samples is not None:\n",
    "        # Create a points dataframe for the image\n",
    "        points_df = pd.DataFrame(samples)\n",
    "        points_df['image_name'] = image\n",
    "        # Finally we save the points to a csv file in the SOURCE_POINTS folder\n",
    "        points_df.to_csv(SOURCE_POINTS + image + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we get all the points for each of the desired images. If you already\n",
    "have one or multiple CSV files with the points, you can simply add the\n",
    "file paths to the list below.\n",
    "\n",
    "**Enter the file paths to the CSV files here (as a list)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Get all the points for all the images\n",
    "POINT_PATHS = glob.glob(SOURCE_POINTS + \"*.csv\")\n",
    "\n",
    "# This dataframe will contain all the points for all the images\n",
    "# The columns are `image_name`, `Row`, and `Column`.\n",
    "POINTS = pd.DataFrame()\n",
    "# We then concatenate all the points into a single dataframe\n",
    "for path in POINT_PATHS:\n",
    "    points = pd.read_csv(path)\n",
    "    points['image_name'] = os.path.basename(path)\n",
    "    POINTS = pd.concat([POINTS, points])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that all CSV files for all images have been concatenated\n",
    "into a single dataframe. The `image_name` column represents the image that\n",
    "the points are associated with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "      row  column                                         image_name\n97   1894     958  mcr_lter1_fringingreef_pole3-4_qu6_20080415.jp...\n41   1852     402  mcr_lter1_fringingreef_pole1-2_qu8_20080415.jp...\n67   1586     619  mcr_lter1_fringingreef_pole1-2_qu7_20080415.jp...\n111  1823    1042  mcr_lter1_fringingreef_pole1-2_qu8_20080415.jp...\n82   1742     810  mcr_lter1_fringingreef_pole3-4_qu7_20080415.jp...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row</th>\n      <th>column</th>\n      <th>image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97</th>\n      <td>1894</td>\n      <td>958</td>\n      <td>mcr_lter1_fringingreef_pole3-4_qu6_20080415.jp...</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1852</td>\n      <td>402</td>\n      <td>mcr_lter1_fringingreef_pole1-2_qu8_20080415.jp...</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>1586</td>\n      <td>619</td>\n      <td>mcr_lter1_fringingreef_pole1-2_qu7_20080415.jp...</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>1823</td>\n      <td>1042</td>\n      <td>mcr_lter1_fringingreef_pole1-2_qu8_20080415.jp...</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>1742</td>\n      <td>810</td>\n      <td>mcr_lter1_fringingreef_pole3-4_qu7_20080415.jp...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a few points\n",
    "POINTS.sample(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make predictions\n",
    "\n",
    "This is the main part of the script. We loop through each image, get the\n",
    "points for that image, and then make predictions for those points. We then\n",
    "save the predictions to a CSV file in the `SOURCE_PREDICTIONS` folder.\n",
    "\n",
    "There are multiple loops in this section. The first loop continues until all\n",
    "images have been processed. The first inner for loop prepares the data for the\n",
    "model, by creating a JSON object that contains the image URL and the points.\n",
    "These are stored in a queued list, representing the images that are waiting\n",
    "to be processed. The second inner while loop checks to see if there are any\n",
    "open positions (only 5 are allowed at a time). If there are, it will submit\n",
    "a queued job to the model until all the positions are filled. The third\n",
    "inner while loop checks the status of each job. If the job is complete, it\n",
    "will save the predictions to a CSV file, and remove the job from the active\n",
    "list. If the job is still running, it will wait 75 seconds before checking\n",
    "the status again. Once all the jobs are complete, the outer while loop will\n",
    "end, and the script will finish.\n",
    "\n",
    "Because the images are hosted on AWS, there is a chance that the URL will\n",
    "expire before the model can make a prediction. If this happens, the script\n",
    "will catch the error, and add the image to a list of expired images. Once\n",
    "all the images have been processed, the script will loop through the expired\n",
    "images, and update the URLs. It will then re-run the predictions for those\n",
    "images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Image mcr_lter1_fringingreef_pole1-2_qu2_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole1-2_qu6_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole1-2_qu8_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole2-3_qu1_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole2-3_qu2_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole2-3_qu4_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole3-4_qu6_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole3-4_qu7_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole3-4_qu8_20080415.jpg not in queue; sampling\n",
      "NOTE: Image mcr_lter1_fringingreef_pole4-5_qu4_20080415.jpg not in queue; sampling\n",
      "JOBS: Queued: 1 \tActive: 0 \tCompleted: 0 \tExpired Images: 0\n",
      "NOTE: Attempting to upload 10 images\n",
      "NOTE: Successfully uploaded: 10 images\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:00:28\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:00:28\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:01:44\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:01:45\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:03:01\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:03:02\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:04:18\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:04:18\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:05:34\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:05:35\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:06:51\n",
      "NOTE: Status: Pending\tID: 22003\tTime: 16:06:52\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:08:08\n",
      "NOTE: Status: In Progress\tID: 22003\tTime: 16:08:08\tTotal: 10\tSuccess: 0\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:09:24\n",
      "NOTE: Status: In Progress\tID: 22003\tTime: 16:09:25\tTotal: 10\tSuccess: 7\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:10:41\n",
      "NOTE: Status: In Progress\tID: 22003\tTime: 16:10:42\tTotal: 10\tSuccess: 7\tFailures: 0\n",
      "JOBS: Queued: 0 \tActive: 1 \tCompleted: 0 \tExpired Images: 0\n",
      "\n",
      "NOTE: Checking status again at 16:11:58\n",
      "NOTE: Completed Job\n",
      "NOTE: Saving annotations for 10 images\n",
      "NOTE: Saved 10 predictions to CSV files\n",
      "NOTE: 0 images had expired and will be re-uploaded\n",
      "NOTE: Adding 10 images to completed\n",
      "NOTE: Removing 10 images from active\n",
      "NOTE: All images have been processed; exiting loop.\n"
     ]
    }
   ],
   "source": [
    "# Jobs that are currently queued\n",
    "queued_jobs = []\n",
    "queued_imgs = []\n",
    "# Jobs that are currently active\n",
    "active_jobs = []\n",
    "active_imgs = []\n",
    "# Jobs that are completed\n",
    "completed_jobs = []\n",
    "completed_imgs = []\n",
    "# A list that contains just the images that need updated urls\n",
    "expired_imgs = []\n",
    "# Flag to indicate if all images have been passed to model\n",
    "finished = False\n",
    "# The amount of time to wait before checking the status of a job\n",
    "patience = 75\n",
    "# The number of images to include in each job\n",
    "data_per_payload = 100\n",
    "\n",
    "# This will continue looping until all images have been processed\n",
    "while not finished:\n",
    "\n",
    "    # A list for images and data that have been sampled this round\n",
    "    payload_data = []\n",
    "    payload_imgs = []\n",
    "\n",
    "    for index, row in IMAGES.iterrows():\n",
    "    # Loops through each image requested, gets points, adds to a queue\n",
    "\n",
    "        # Get the current image name and url\n",
    "        name = row['image_name']\n",
    "        url = row['image_url']\n",
    "\n",
    "        # If this image has already been completed, skip it.\n",
    "        if name in completed_imgs:\n",
    "            # print(f\"Image {name} already completed; skipping\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        # If this image is already in active, skip it.\n",
    "        elif any(name in n for n in active_imgs):\n",
    "            # print(f\"Image {name} already active; skipping\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        elif any(name in n for n in queued_imgs):\n",
    "            # print(f\"Image {name} already queued; skipping\")\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "        # The image url has not expired, so we can queue the image\n",
    "        elif not is_expired(url):\n",
    "            print(f\"NOTE: Image {name} not in queue; sampling\")\n",
    "            points = POINTS[POINTS['image_name'].str.contains(name)]\n",
    "            points = points[['row', 'column']].to_dict(orient=\"records\")\n",
    "\n",
    "            # Add the data to the list for payloads\n",
    "            payload_imgs.append(name)\n",
    "            payload_data.append(\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"attributes\": {\n",
    "                        \"name\": name,\n",
    "                        \"url\": url,\n",
    "                        \"points\": points\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            # The image url expired, so we need to update it later.\n",
    "            print(f\"WARNING: {name} expired; adding to expired list\")\n",
    "            expired_imgs.append(name)\n",
    "            continue # Skip to the next image within the current for loop\n",
    "\n",
    "    # Here we initialize the payload, which is a JSON object that\n",
    "    # contains the image URLs and their points; payloads will contain\n",
    "    # batches of data (N = data_per_payload).\n",
    "    for _ in np.arange(0, len(payload_imgs), data_per_payload):\n",
    "        # Get the image names and data for the payload\n",
    "        image_names = payload_imgs[_ : _ + data_per_payload]\n",
    "        payload = {'data': payload_data[_ : _ + data_per_payload]}\n",
    "        # Use the payload to construct the job\n",
    "        job = {\n",
    "                \"headers\": HEADERS,\n",
    "                \"model_url\": MODEL_URL,\n",
    "                \"image_names\": image_names,\n",
    "                \"data\": json.dumps(payload, indent=4),\n",
    "\n",
    "              }\n",
    "        # Add the job to the queue\n",
    "        queued_jobs.append(job)\n",
    "        queued_imgs.append(image_names)\n",
    "\n",
    "    # Print the status of the jobs\n",
    "    print_job_status(queued_jobs, active_jobs, completed_jobs, expired_imgs)\n",
    "\n",
    "    # Start uploading the queued jobs to CoralNet if there are\n",
    "    # less than 5 active jobs, and there are more in the queue.\n",
    "    # If there are no queued jobs, this won't need to be entered.\n",
    "    while len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "\n",
    "        # Loop through all the queued jobs\n",
    "        for job, names in list(zip(queued_jobs, queued_imgs)):\n",
    "\n",
    "            # Break when active gets to 5\n",
    "            if len(active_jobs) >= 5:\n",
    "                print(\"NOTE: Maximum number of active jobs reached; checking status\")\n",
    "                break # Breaks from both loops, since the while loop condition is met\n",
    "\n",
    "            # Upload the image and the sampled points to CoralNet\n",
    "            print(f\"NOTE: Attempting to upload {len(names)} images\")\n",
    "\n",
    "            # Sends the requests to the `source` and in exchange, receives\n",
    "            # a message telling if it was received correctly.\n",
    "            response = requests.post(url=job[\"model_url\"],\n",
    "                                     data=job[\"data\"],\n",
    "                                     headers=job[\"headers\"])\n",
    "            if response.ok:\n",
    "                # If it was received\n",
    "                print(f\"NOTE: Successfully uploaded: {len(names)} images\")\n",
    "\n",
    "                # Add to active jobs\n",
    "                active_jobs.append(response)\n",
    "                active_imgs.append(names)\n",
    "\n",
    "                # Remove from queued jobs\n",
    "                queued_jobs.remove(job)\n",
    "                queued_imgs.remove(names)\n",
    "\n",
    "            else:\n",
    "                # There was an error uploading to CoralNet; get the message\n",
    "                message = json.loads(response.text)['errors'][0]['detail']\n",
    "\n",
    "                # Print the message\n",
    "                print(f\"CoralNet: {message}\")\n",
    "\n",
    "                if \"5 jobs active\" in message:\n",
    "                    print(f\"NOTE: Will attempt again at {in_N_seconds(patience)}\")\n",
    "                    time.sleep(patience)\n",
    "\n",
    "                else:\n",
    "                    # Assumed that the images have expired\n",
    "                    print(f\"ERROR: Failed to upload: {len(names)} images\")\n",
    "\n",
    "                    # Add to expired images\n",
    "                    expired_imgs.extend(names)\n",
    "\n",
    "                    # Remove from queue\n",
    "                    queued_jobs.remove(job)\n",
    "                    queued_imgs.remove(names)\n",
    "\n",
    "        # If all images have expired, break from the loop\n",
    "        if IMAGES['image_name'].isin(expired_imgs).all():\n",
    "            print(\"NOTE: All images have expired\")\n",
    "            break\n",
    "\n",
    "    # Check the status of the active jobs, break when another can be added\n",
    "    while len(active_jobs) <= 5 and len(active_jobs) != 0:\n",
    "\n",
    "        # Check the status of the active jobs\n",
    "        print_job_status(queued_jobs, active_jobs, completed_jobs, expired_imgs)\n",
    "\n",
    "        # Sleep before checking status again\n",
    "        print(f\"\\nNOTE: Checking status again at {in_N_seconds(patience)}\")\n",
    "        time.sleep(patience)\n",
    "\n",
    "        # Loop through the active jobs\n",
    "        for i, (job, names) in enumerate(list(zip(active_jobs, active_imgs))):\n",
    "\n",
    "            # Check the status of the current job\n",
    "            current_status, message, wait = check_job_status(job, CORALNET_TOKEN)\n",
    "\n",
    "            # Print the message\n",
    "            print(f\"NOTE: {message}\")\n",
    "\n",
    "            # Current job has finished, output the results, remove from queue\n",
    "            if message == \"Completed Job\":\n",
    "\n",
    "                # Convert to csv, and save locally, check expired\n",
    "                predictions, expired = convert_to_csv(current_status,\n",
    "                                                      names,\n",
    "                                                      SOURCE_PREDICTIONS)\n",
    "\n",
    "                # Deal with images after the job has been completed\n",
    "                for name in names: #\n",
    "                    # If the image was in expired, add to expired\n",
    "                    if name in expired:\n",
    "                        expired_imgs.append(name)\n",
    "                    # Else, add to completed_imgs\n",
    "                    else:\n",
    "                        completed_imgs.append(name)\n",
    "\n",
    "                # Add to completed jobs list\n",
    "                print(f\"NOTE: Adding {len(names)} images to completed\")\n",
    "                completed_jobs.append(current_status)\n",
    "\n",
    "                # Remove from active jobs, images list\n",
    "                print(f\"NOTE: Removing {len(names)} images from active\")\n",
    "                active_imgs.remove(names)\n",
    "                active_jobs.remove(job)\n",
    "\n",
    "            # Wait for the specified time before checking the status again\n",
    "            time.sleep(wait)\n",
    "\n",
    "        # After checking the current status, break if another can be added\n",
    "        # Else wait and check the status of the active jobs again.\n",
    "        if len(active_jobs) < 5 and len(queued_jobs) > 0:\n",
    "            print(f\"NOTE: Active jobs {len(active_jobs)}; adding another.\")\n",
    "            break\n",
    "\n",
    "    # Check to see everything has been completed, breaking the loop\n",
    "    if not queued_jobs and not active_jobs and not expired_imgs:\n",
    "        print(\"NOTE: All images have been processed; exiting loop.\")\n",
    "        finished = True\n",
    "\n",
    "    # If there are no queued jobs, and no active jobs, but there are images in\n",
    "    # expired, get just the AWS URL for the expired images and update dataframe.\n",
    "    if not queued_jobs and not active_jobs and expired_imgs:\n",
    "        print(f\"NOTE: Updating {len(expired_imgs)} expired images' URL\")\n",
    "        # Get the subset of images dataframe containing only the expired images\n",
    "        IMAGES = IMAGES[IMAGES['image_name'].isin(expired_imgs)].copy()\n",
    "        old_urls = IMAGES['image_page'].tolist()\n",
    "        # Get the unexpired AWS  image URLs\n",
    "        driver, new_urls = get_image_urls(driver, old_urls)\n",
    "        IMAGES['image_url'] = new_urls\n",
    "        expired_imgs = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
