{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CoralNet-Toolbox","text":"[![python-version](https://img.shields.io/pypi/pyversions/CoralNet-Toolbox.svg)](https://pypi.org/project/CoralNet-Toolbox) [![version](https://img.shields.io/pypi/v/CoralNet-Toolbox.svg)](https://pypi.python.org/pypi/CoralNet-Toolbox) [![pypi-passing](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![windows](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/windows.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![macos](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/macos.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![ubuntu](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/ubuntu.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![downloads](https://static.pepy.tech/badge/coralnet-toolbox)](https://pepy.tech/projects/coralnet-toolbox)"},{"location":"#quick-start","title":"Quick StartWatch the Video Demos","text":"<p>Running the following command will install the <code>coralnet-toolbox</code>, which you can then run from the command line: <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n\n# Run\ncoralnet-toolbox\n</code></pre></p> <p>For further instructions please see the following: - Installation - Usage</p> <p> </p>"},{"location":"#tldr","title":"TL;Dr","text":"<p>The <code>CoralNet-Toolbox</code> is an unofficial codebase that can be used to augment processes associated with those on CoralNet.</p> <p>It uses\u2728<code>Ultralytics</code>\ud83d\ude80 as a  base, which is an open-source library for computer vision and deep learning built in <code>PyTorch</code>. For more information on their <code>AGPL-3.0</code> license, see here.</p> <p>The <code>toolbox</code> also uses the following to create rectangle and polygon annotations: - <code>Fast-SAM</code> - <code>RepViT-SAM</code> - <code>EdgeSAM</code> - <code>MobileSAM</code> - <code>CoralSCOP</code> - <code>SAM</code> - <code>AutoDistill</code>   - <code>GroundingDino</code></p>"},{"location":"#tools","title":"Tools","text":"<p>Enhance your CoralNet experience with these tools: - \u270f\ufe0f Annotate: Create annotations freely - \ud83d\udc41\ufe0f Visualize: See CoralNet and CPCe annotations superimposed on images - \ud83d\udd2c Sample: Sample patches using various methods (Uniform, Random, Stratified) - \ud83e\udde9 Patches: Create patches (points) - \ud83d\udd33 Rectangles: Create rectangles (bounding boxes) - \ud83d\udfe3 Polygons: Create polygons (instance masks) - \ud83e\uddbe SAM: Use <code>FastSAM</code>, <code>CoralSCOP</code>, <code>RepViT-SAM</code>, <code>EdgeSAM</code>, <code>MobileSAM</code>, and <code>SAM</code> to create polygons - \ud83e\uddea AutoDistill: Use <code>AutoDistill</code> to access <code>GroundingDINO</code> for creating rectangles - \ud83e\udde0 Train: Build local patch-based classifiers, object detection, and instance segmentation models - \ud83d\udd2e Deploy: Use trained models for predictions - \ud83d\udcca Evaluation: Evaluate model performance - \ud83d\ude80 Optimize: Productionize models for faster inferencing - \u2699\ufe0f Batch Inference: Perform predictions on multiple images, automatically - \u2194\ufe0f I/O: Import and Export annotations from / to CoralNet, Viscore, and TagLab - \ud83d\udcf8 YOLO: Import and Export YOLO datasets for machine learning</p>"},{"location":"#todo","title":"TODO","text":"<ul> <li>\ud83d\udd0d API: Get predictions from any CoralNet source model</li> <li>\ud83d\udce5 Download: Retrieve source data from CoralNet</li> <li>\ud83d\udce4 Upload: Add images and annotations to CoralNet</li> <li>\ud83d\udce6 Toolshed: Access tools from the old repository</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#how-to-install","title":"How to Install","text":""},{"location":"installation/#anaconda","title":"Anaconda","text":"<p>It's recommended to use <code>Anaconda</code> to create an environment for the <code>toolbox</code>: <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n</code></pre></p>"},{"location":"installation/#install","title":"Install","text":"<p>Once this has finished, install the <code>toolbox</code>: <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n</code></pre></p>"},{"location":"installation/#cuda","title":"CUDA","text":"<p>If you have <code>CUDA</code>, you should install the versions of <code>cuda-nvcc</code> and <code>cudatoolkit</code> that you need, and then install the corresponding versions of <code>torch</code> and <code>torchvision</code>. Below is an example of how that can be done using <code>CUDA</code> version 11.8: <pre><code># cmd\n\n# Example for CUDA 11.8\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n</code></pre></p> <p>If <code>CUDA</code> is installed on your computer, and <code>torch</code> was built with it properly, you should see a <code>\ud83d\udc07</code> icon in the <code>toolbox</code> instead of a <code>\ud83d\udc22</code>; if you have multiple <code>CUDA</code> devices available, you should see a <code>\ud83d\ude80</code> icon, and if you're using a Mac with <code>Metal</code>, you should see an <code>\ud83c\udf4e</code> icon (click on the icon to see the device information).</p> <p>See here for more details on versions for the following: - <code>cuda-nvcc</code> - <code>cudatoolkit</code> - <code>torch</code></p>"},{"location":"installation/#run","title":"Run","text":"<p>Finally, you can run the <code>toolbox</code> from the command line: <pre><code># cmd\n\n# Run\ncoralnet-toolbox\n</code></pre></p>"},{"location":"installation/#github-repository","title":"GitHub Repository","text":"<p>If you prefer to clone the repository and run the <code>toolbox</code> from the source code, you can do so with the following:</p> <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n\n# Clone and enter the repository\ngit clone https://github.com/Jordan-Pierce/CoralNet-Toolbox.git\ncd CoralNet-Toolbox\n\n# Install the latest\npip install -e .\n\n# Install CUDA requirements (if applicable)\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n\n# Run\ncoralnet-toolbox\n</code></pre> <p>Or, if you want to simply install from the <code>toolbox</code> from the GitHub repo directly you can do the following:</p> <pre><code># cmd\n\npip install git+https://github.com/Jordan-Pierce/CoralNet-Toolbox.git@main\n\n# replace @main with a different branch if you want to test out experimental code\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#overview","title":"Overview","text":"<p>The CoralNet Toolbox is a Python application built using PyQt5 for image annotation. This guide provides instructions on how to use the application, including key functionalities and hotkeys.</p>"},{"location":"usage/#annotations","title":"Annotations","text":"<ul> <li>PatchAnnotation: Represents a patch annotation.</li> <li>RectangleAnnotation: Represents a rectangular annotation.</li> <li>PolygonAnnotation: Represents a polygonal annotation.</li> </ul>"},{"location":"usage/#computer-vision-tasks","title":"Computer Vision Tasks","text":"<ul> <li>Classification: Assign a label to an image (Patch).</li> <li>Detection: Detect objects in an image (Rectangle).</li> <li>Segmentation: Segment objects in an image (Polygon).</li> </ul>"},{"location":"usage/#thresholds-for-computer-vision-tasks","title":"Thresholds for Computer Vision Tasks","text":"<ul> <li>Patch Size: Adjust the patch size in the status bar.</li> <li>Uncertainty Threshold: Adjust the uncertainty threshold in the status bar.</li> <li>IoU Threshold: Adjust the IoU threshold in the status bar.</li> <li>Area Threshold: Adjust the min and max area threshold in the status bar.</li> </ul>"},{"location":"usage/#main-window","title":"Main Window","text":"<p>The main window consists of several components: - Menu Bar: Contains import, export, and other actions. - Tool Bar: Contains tools for selection and annotation. - Status Bar: Displays the image size, cursor position, view extent, annotation transparency, and thresholds. - Annotation Window: Displays the image and annotations. - Label Window: Lists and manages labels. - Image Window: Displays imported images. - Confidence Window: Displays cropped images and confidence charts.</p>"},{"location":"usage/#menu-bar-actions","title":"Menu Bar Actions","text":"<ul> <li>Import:</li> <li>Import Images: Load image files.</li> <li>Import Frames: Load video frames (Currently not available).</li> <li>Import Orthomosaic: Load orthomosaic images (Currently not available).</li> <li>Import Labels (JSON): Load label data from a JSON file.</li> <li>Import Annotations (JSON): Load annotation data from a JSON file.</li> <li>Import Annotations (CoralNet): Load annotation data from a CoralNet CSV file.</li> <li>Import Annotations (Viscore): Load annotation data from a Viscore CSV file.</li> <li>Import Annotations (TagLab): Load annotation data from a TagLab JSON file.</li> <li> <p>Import Dataset: Import a YOLO dataset for machine learning (Detection, Segmentation).</p> </li> <li> <p>Export:</p> </li> <li>Export Labels (JSON): Save label data to a JSON file.</li> <li>Export Annotations (JSON): Save annotation data to a JSON file.</li> <li>Export Annotations (CoralNet): Save annotation data to a CoralNet CSV file.</li> <li>Export Annotations (Viscore): Save annotation data to a Viscore CSV file.</li> <li>Export Annotations (TagLab): Save annotation data to a TagLab JSON file.</li> <li> <p>Export Dataset: Create a YOLO dataset for machine learning (Classification, Detection, Segmentation).</p> </li> <li> <p>Sample:</p> </li> <li> <p>Sample Annotations: Automatically generate Patch annotations.</p> </li> <li> <p>CoralNet: (Currently not available)</p> </li> <li>Authenticate: Authenticate with CoralNet.</li> <li>Upload: Upload data to CoralNet.</li> <li>Download: Download data from CoralNet.</li> <li> <p>Model API: Access CoralNet model API.</p> </li> <li> <p>Machine Learning:</p> </li> <li>Merge Datasets: Merge multiple Classification datasets.</li> <li>Tile Dataset: Tile existing Detection / Segmention datasets using yolo-tiling.</li> <li>Train Model: Train a machine learning model.</li> <li>Evaluate Model: Evaluate a trained model.</li> <li>Optimize Model: Convert model format.</li> <li>Deploy Model: Make predictions using a trained model (Classification, Detection, Segmentation).</li> <li> <p>Batch Inference: Perform batch inferences.</p> </li> <li> <p>SAM:</p> </li> <li>Deploy Predictor: Deploy EdgeSAM, MobileSAM, or SAM to use interactively (points, box); Segment Anything</li> <li>Deploy Generator: Deploy FastSAM to automatically segment the image; Segment Everything<ul> <li>Recommendation: Use the \"Use Predictor to create Polygons\", as the results are significantly better</li> </ul> </li> <li> <p>Batch Inference: Perform batch inferences.</p> </li> <li> <p>AutoDistill:</p> </li> <li>Deploy Model: Deploy a foundational model<ul> <li>Models Available: GroundingDino,</li> </ul> </li> <li>Batch Inference: Perform batch inferences.</li> </ul>"},{"location":"usage/#tool-bar","title":"Tool Bar","text":"<ul> <li>Select Tool: Select multiple annotations; move and change the size of annotations.</li> <li>Patch Tool: Add new PatchAnnotations.</li> <li>Polygon Tool: Add new PolygonAnnotations.</li> <li>Rectangle Tool: Add new RectangleAnnotations.</li> <li>SAM Tool: Use SAM model for automatic segmentation (points, box).</li> </ul>"},{"location":"usage/#status-bar","title":"Status Bar","text":"<ul> <li>Image Size: Displays the image size.</li> <li>Cursor Position: Displays the cursor position.</li> <li>View Extent: Displays the view extent.</li> <li>Annotation Transparency: Adjust the annotation transparency.</li> <li>Select All Labels: Select all labels, adjusting transparency for all labels.</li> <li>Unselect All Labels: Unselect all labels, adjusting transparency for only selected labels.</li> </ul>"},{"location":"usage/#annotation-window","title":"Annotation Window","text":"<ul> <li>Zoom: Use the mouse wheel to zoom in and out.</li> <li>Pan: Hold Ctrl + Right-click the mouse button to pan the image.</li> <li>Add Annotation: Click with the Left mouse button while using one of the annotation tools.</li> <li>Select Annotations:</li> <li>Ctrl + Left-Click on multiple annotations while using the select tool.</li> <li>Ctrl + Left-Click and drag to select multiple annotations while using the select tool.<ul> <li>Move Annotation: Drag a selected annotation.</li> <li>Modify Annotation: Hold Shift and drag the vertices of the selected annotation (Rectangle, Polygon).</li> <li>Resize Annotation: Hold Ctrl and Zoom in / out to increase / decrease a selected annotation's size.</li> <li>Delete Annotations: Press Ctrl + Delete to delete the selected annotations.</li> </ul> </li> </ul>"},{"location":"usage/#label-window","title":"Label Window","text":"<ul> <li>Move Label: Right-click and drag to move labels.</li> <li>Add Label: Click the \"Add Label\" button to add a new label.</li> <li>Delete Label: Click the \"Delete Label\" button to delete the selected label.</li> <li>Edit Label: Click the \"Edit Label\" button to edit the selected label.</li> <li>Lock Label: Click the \"Lock Label\" button to lock the selected label.</li> </ul>"},{"location":"usage/#image-window","title":"Image Window","text":"<ul> <li>Load Image: Click on a row to load the image in the annotation window.</li> <li>Delete Image: Right-click on a row and select \"Delete Image\" to remove the image.</li> <li>Delete Annotations: Right-click on a row and select \"Delete Annotations\" to remove the image's annotations.</li> <li>Search / Filter:</li> <li>By Image: Filter for images by name or sub-string.</li> <li>By Label: Filter images by labels they contain.</li> <li>No Annotations: Filter images with no annotations.</li> <li>Has Annotations: Filter images with annotations.</li> <li>Has Predictions: Filter images with predictions.</li> </ul>"},{"location":"usage/#confidence-window","title":"Confidence Window","text":"<ul> <li>Display Cropped Image: Shows the cropped image of the selected annotation.</li> <li>Confidence Chart: Displays a bar chart with confidence scores.</li> <li>Prediction Selection: Select a prediction from the list to change the label.</li> </ul>"},{"location":"usage/#hotkeys","title":"Hotkeys","text":"<ul> <li>Ctrl + Delete: Delete the selected annotations.</li> <li>Ctrl + W/A/S/D: Navigate through labels.</li> <li>Ctrl + Mouse Wheel: Adjust annotation size.</li> <li>Ctrl + Left/Right: Cycle through annotations.</li> <li>Ctrl + Up/Down: Cycle through images.</li> <li>Ctrl + Shift + &lt;: Select all annotations.</li> <li>Ctrl + Shift + &gt;: Unselect all annotations.</li> <li> <p>Escape: Exit the program.</p> </li> <li> <p>Machine Learning, SAM, and AutoDistill: After a model is loaded</p> </li> <li>Ctrl + 1: Make prediction on selected Patch annotation, else all in the image with Review label.</li> <li>Ctrl + 2: Make predictions using Object Detection model.</li> <li>Ctrl + 3: Make predictions using Instance Segmentation model.</li> <li>Ctrl + 4: Make predictions using FastSAM model.</li> <li> <p>Ctrl + 5: Make predictions using AutoDistill model.</p> </li> <li> <p>SAM: After a model is loaded</p> </li> <li>Space Bar: Set working area; finalize prediction.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Ctrl + Left-Click: Add positive point.</li> <li>Ctrl + Right-Click: Add negative point.</li> </ul>"}]}