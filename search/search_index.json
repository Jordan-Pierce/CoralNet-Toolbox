{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CoralNet-Toolbox \ud83e\udeb8\ud83e\uddf0","text":"[![python-version](https://img.shields.io/pypi/pyversions/CoralNet-Toolbox.svg)](https://pypi.org/project/CoralNet-Toolbox) [![version](https://img.shields.io/pypi/v/CoralNet-Toolbox.svg)](https://pypi.python.org/pypi/CoralNet-Toolbox) [![pypi-passing](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![windows](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/windows.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![macos](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/macos.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![ubuntu](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/ubuntu.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox)"},{"location":"#quick-start","title":"Quick StartWatch the Video Demos","text":"<p>Running the following command will install the <code>coralnet-toolbox</code>, which you can then run from the command line: <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n\n# Run\ncoralnet-toolbox\n</code></pre></p> <p>For further instructions please see the following guides: - Installation - Usage - Patch-based Image Classifier</p> <p> </p>"},{"location":"#tldr","title":"TL;Dr","text":"<p>The <code>CoralNet-Toolbox</code> is an unofficial codebase that can be used to augment processes associated with those on CoralNet.</p> <p>It uses\u2728<code>Ultralytics</code>\ud83d\ude80 as a  base, which is an open-source library for computer vision and deep learning built in <code>PyTorch</code>. For more information on their <code>AGPL-3.0</code> license, see here.</p> <p>The <code>toolbox</code> also uses the following to create rectangle and polygon annotations: - <code>Fast-SAM</code> - <code>RepViT-SAM</code> - <code>EdgeSAM</code> - <code>MobileSAM</code> - <code>CoralSCOP</code> - <code>SAM</code> - <code>YOLOE</code> - <code>AutoDistill</code>   - <code>Grounding Dino</code>   - <code>OWLViT</code>   - <code>OmDetTurbo</code></p>"},{"location":"#tools","title":"Tools","text":"Patch Annotation Tool Rectangle Annotation Tool Polygon Annotation Tool Patch-based Image Classification Object Detection Instance Segmentation Segment Anything Model (SAM) Polygon Classification Patch-based LAI Classification See Anything (YOLOE) <p>Enhance your CoralNet experience with these tools: - \ud83d\udce5 Download: Retrieve Source data (images and annotations) from CoralNet - \ud83c\udfac Rasters: Import images, or extract frames directly from video files - \u270f\ufe0f Annotate: Create annotations freely - \ud83d\udc41\ufe0f Visualize: See CoralNet and CPCe annotations superimposed on images - \ud83d\udd2c Sample: Sample patches using various methods (Uniform, Random, Stratified) - \ud83e\udde9 Patches: Create patches (points) - \ud83d\udd33 Rectangles: Create rectangles (bounding boxes) - \ud83d\udfe3 Polygons: Create polygons (instance masks) - \ud83e\uddbe SAM: Use <code>FastSAM</code>, <code>CoralSCOP</code>, <code>RepViT-SAM</code>, <code>EdgeSAM</code>, <code>MobileSAM</code>, and <code>SAM</code> to create polygons   - Uses <code>xSAM</code> - \ud83d\udc40 YOLOE (See Anything): Detect similar appearing objects using visual prompts automatically - \ud83e\uddea AutoDistill: Use <code>AutoDistill</code> to access the following for creating rectangles and polygons:   - Uses <code>Grounding DINO</code>, <code>OWLViT</code>, <code>OmDetTurbo</code> - \ud83e\udde0 Train: Build local patch-based classifiers, object detection, and instance segmentation models - \ud83d\udd2e Deploy: Use trained models for predictions - \ud83d\udcca Evaluation: Evaluate model performance - \ud83d\ude80 Optimize: Productionize models for faster inferencing - \u2699\ufe0f Batch Inference: Perform predictions on multiple images, automatically - \u2194\ufe0f I/O: Import and Export annotations from / to CoralNet, Viscore, and TagLab   - Export annotations as segmentation masks - \ud83d\udcf8 YOLO: Import and Export YOLO datasets for machine learning - \ud83e\uddf1 Tile Dataset: Tile existing Detection / Segmentation datasets   - Uses <code>yolo-tiling</code></p>"},{"location":"#todo","title":"TODO","text":"<ul> <li>\ud83e\udd8a BioCLIP, MobileCLIP (AutoDistill): Automatically classify annotations</li> <li>\ud83d\udce6 Toolshed: Access tools from the old repository</li> </ul>"},{"location":"classify/","title":"Classify","text":"<p>A big thanks to the researchers at the research arm of the Seattle Aquarium for providing this guide.</p> <pre><code>@misc{williams2025,\n  author = {Williams, Megan},\n  title = {SOP to train a classification model in Toolbox},\n  institution = {Seattle Aquarium},\n  date = {2025-04-04}\n}\n</code></pre>"},{"location":"classify/#sop-to-train-a-classification-model-in-toolbox","title":"SOP to train a classification model in Toolbox","text":"<p>The following steps are required to create a training dataset, train a classification model using Ultralytics YOLO in Toolbox, and apply the model to make predictions.</p>"},{"location":"classify/#1-toolbox-installation-and-setup","title":"1. Toolbox installation and setup","text":"<ul> <li>Instructions for installing and running Toolbox can be found in the documentation linked here.</li> </ul>"},{"location":"classify/#2-prepare-training-and-testing-images","title":"2. Prepare training and testing images","text":"<ul> <li>Ensure images are color corrected and a quality desired for analysis.</li> <li>Split images into training and testing folder. Our technique is:</li> <li>Training folder: Move 2 out of every 3 images here.</li> <li>Testing folder: Move 1 out of every 3 images here.</li> </ul>"},{"location":"classify/#3-load-labelset","title":"3. Load labelset","text":"<ul> <li>Open Toolbox and import your classification labelset</li> <li>Go to Labelset \u2192 Import</li> <li>Our JSON labelset can be found here. You can also preview the labelset in Excel here.</li> </ul>"},{"location":"classify/#4-import-and-annotate-training-images","title":"4. Import and annotate training images","text":"<ul> <li>Import training images into Toolbox</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images</li> <li>Create image patches (classification annotations)</li> <li>Select a label in the lower label window.</li> <li>Choose the image patch tool (rectangle icon) from the toolbar on the left.</li> <li>In the annotation window (center window), left click the appropriate location in the image to add a patch for that label.</li> <li>Repeat for each label across your training images.</li> </ul>"},{"location":"classify/#5-export-classification-dataset","title":"5. Export classification dataset","text":"<ul> <li>After annotating, export the dataset:</li> <li>File \u2192 Export \u2192 Dataset \u2192 Classify</li> <li>Toolbox will generate a dataset directory containing train, validation, and test folders with labeled image patches.</li> </ul>"},{"location":"classify/#6-train-classification-model","title":"6. Train classification model","text":"<ul> <li>Start training a YOLO classification model</li> <li>Ultralytics \u2192 Train Model \u2192 Classify</li> <li>In the training window:</li> <li>Dataset: Click Browse and select the exported dataset folder.</li> <li>Model Selection: Choose the Ultralytics YOLO model that fits your needs (e.g., YOLOv8 or YOLOv11). A guide comparing model options is available here.</li> <li>Parameters:<ul> <li>Set the location where you want your trained model to be saved.</li> <li>You can use default training parameters or customize them. More information about the parameters can be found here.</li> </ul> </li> <li>Click OK to begin training. You can monitor training progress in the terminal.</li> </ul>"},{"location":"classify/#7-load-and-deploy-model","title":"7. Load and deploy model","text":"<ul> <li>After training completes:</li> <li>Go to Ultralytics \u2192 Deploy Model \u2192 Classify</li> <li>Under Actions, click Browse Model and select your trained weights file (best.pt).</li> <li>Click load model</li> </ul>"},{"location":"classify/#8-test-the-model-on-new-images","title":"8. Test the model on new images","text":"<ul> <li>Remove training images:</li> <li>In the image window, click \"Select All\", right-click and select delete all images to remove.</li> <li>Import test images:</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images and choose the testing folder.</li> <li>Create random image patches:</li> <li>Click Sample at the top</li> <li>Set your desired sampling configuration (e.g., number of patches).</li> <li>Set Select Label to Review</li> <li>Check \"Apply to all images\" and click Accept</li> </ul>"},{"location":"classify/#9-run-predictions","title":"9. Run predictions","text":"<ul> <li>To predict label for the new image patches:</li> <li>For a single image: press Ctrl + 1</li> <li>For all images:<ul> <li>Ultralytics \u2192 Batch Inference \u2192 Classify</li> <li>Check \"Apply to all images\" and \"Predict review annotation\"</li> </ul> </li> </ul>"},{"location":"classify/#10-review-and-correct-predictions","title":"10. Review and correct predictions","text":"<ul> <li>Predicted labels appear for each review image patch.</li> <li>Confidence levels are shown in the Confidence window.</li> <li>To fix incorrect predictions:</li> <li>Select the image patch so that it is shown in the Confidence window</li> <li>Select the correct label in the label window</li> </ul>"},{"location":"classify/#11-export-and-improve-dataset","title":"11. Export and improve dataset","text":"<ul> <li>To analyze results:</li> <li>Export annotation file as a .csv file: File \u2192 Export \u2192 CSV</li> <li>To improve your model:</li> <li>Create a new dataset with the corrected predictions</li> <li>Merge this with the original dataset:<ul> <li>Ultralytics \u2192 Merge Datasets \u2192 Classify</li> <li>Set a name and location for the merged dataset</li> <li>Click \u2192 Add Dataset and select the datasets you want to combine</li> </ul> </li> </ul>"},{"location":"classify/#12-improve-existing-model","title":"12. Improve existing model","text":"<ul> <li>The merged dataset can now be used to train a new model following Step 6.</li> <li>Instead of selecting a new YOLO model, you can use your existing model:</li> <li>Under Model Selection switch to Use Existing Model</li> <li>Browse to the model weights (best.pt)</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#how-to-install","title":"How to Install","text":""},{"location":"installation/#anaconda","title":"Anaconda","text":"<p>It's recommended to use <code>Anaconda</code> to create an environment for the <code>toolbox</code>: <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n</code></pre></p>"},{"location":"installation/#install","title":"Install","text":"<p>Once this has finished, install the <code>toolbox</code> using <code>uv</code>: </p> <pre><code># cmd\n\n# Install uv first\npip install uv\n\n# Install with uv\nuv pip install coralnet-toolbox\n</code></pre> <p>Although fast, <code>uv</code> is still relatively new; if this fails, simply fall back to using <code>pip</code>:</p> <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n</code></pre>"},{"location":"installation/#cuda","title":"CUDA","text":"<p>If you have <code>CUDA</code>, you should install the versions of <code>cuda-nvcc</code> and <code>cudatoolkit</code> that you need, and then install the corresponding versions of <code>torch</code> and <code>torchvision</code>. Below is an example of how that can be done using <code>CUDA</code> version 11.8:</p> <pre><code># cmd\n\n# Example for CUDA 11.8\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n</code></pre> <p>If <code>CUDA</code> is installed on your computer, and <code>torch</code> was built with it properly, you should see a <code>\ud83d\udc07</code> icon in the <code>toolbox</code> instead of a <code>\ud83d\udc22</code>; if you have multiple <code>CUDA</code> devices available, you should see a <code>\ud83d\ude80</code> icon, and if you're using a Mac with <code>Metal</code>, you should see an <code>\ud83c\udf4e</code> icon (click on the icon to see the device information).</p> <p>See here for more details on versions for the following: - <code>cuda-nvcc</code> - <code>cudatoolkit</code> - <code>torch</code></p>"},{"location":"installation/#run","title":"Run","text":"<p>Finally, you can run the <code>toolbox</code> from the command line:</p> <pre><code># cmd\n\n# Run\ncoralnet-toolbox\n</code></pre>"},{"location":"installation/#github-repository","title":"GitHub Repository","text":"<p>If you prefer to clone the repository and run the <code>toolbox</code> from the source code, you can do so with the following:</p> <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n\n# Clone and enter the repository\ngit clone https://github.com/Jordan-Pierce/CoralNet-Toolbox.git\ncd CoralNet-Toolbox\n\n# Install the latest\npip install -e .\n\n# Install CUDA requirements (if applicable)\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n\n# Run\ncoralnet-toolbox\n</code></pre> <p>Or, if you want to simply install from the <code>toolbox</code> from the GitHub repo directly you can do the following:</p> <pre><code># cmd\n\npip install git+https://github.com/Jordan-Pierce/CoralNet-Toolbox.git@main\n\n# replace @main with a different branch if you want to test out experimental code\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#overview","title":"Overview","text":"<p>The CoralNet Toolbox is a Python application built using PyQt5 for image annotation. This guide provides instructions on how to use the application, including key functionalities and hotkeys.</p>"},{"location":"usage/#annotations","title":"Annotations","text":"<ul> <li>PatchAnnotation: Represents a patch annotation.</li> <li>RectangleAnnotation: Represents a rectangular annotation.</li> <li>PolygonAnnotation: Represents a polygonal annotation.</li> </ul>"},{"location":"usage/#computer-vision-tasks","title":"Computer Vision Tasks","text":"<ul> <li>Classification: Assign a label to an image (Patch).</li> <li>Detection: Detect objects in an image (Rectangle).</li> <li>Segmentation: Segment objects in an image (Polygon).</li> </ul>"},{"location":"usage/#thresholds-for-computer-vision-tasks","title":"Thresholds for Computer Vision Tasks","text":"<ul> <li>Patch Size: Adjust the patch size in the status bar.</li> <li>Uncertaint Threshold: Adjust the uncertainty threshold in the status bar.</li> <li>IoU Threshold: Adjust the IoU threshold in the status bar.</li> <li>Area Threshold: Adjust the min and max area threshold in the status bar.</li> </ul>"},{"location":"usage/#main-window","title":"Main Window","text":"<p>The main window consists of several components: - Menu Bar: Contains import, export, and other actions. - Tool Bar: Contains tools for selection and annotation. - Status Bar: Displays the image size, cursor position, view extent, annotation transparency, and thresholds. - Annotation Window: Displays the image and annotations. - Label Window: Lists and manages labels. - Image Window: Displays imported images. - Confidence Window: Displays cropped images and confidence charts.</p>"},{"location":"usage/#menu-bar-actions","title":"Menu Bar Actions","text":"<ul> <li>New Project: Reload CoralNet-Toolbox (loss of data warning).</li> <li>Open Project: Open an existing CoralNet-Toolbox project JSON file.</li> <li> <p>Save Project: Save current CoralNet-Toolbox project to JSON file.</p> </li> <li> <p>Import:</p> </li> <li>Import Images: Load image files.</li> <li>Import Frames: Load video frames (Currently not available).</li> <li>Import Labels (JSON): Load label data from a JSON file.</li> <li>Import CoralNet Labels (CSV): Load label data from a CoralNet CSV file.</li> <li>Import TagLab Labels (JSON): Load label data from a TagLab JSON file.</li> <li>Import Annotations (JSON): Load annotation data from a JSON file.</li> <li>Import CoralNet Annotations: Load annotation data from a CoralNet CSV file.</li> <li>Import TagLab Annotations: Load annotation data from a TagLab JSON file.</li> <li>Import Viscore Annotations: Load annotation data from a Viscore CSV file.</li> <li> <p>Import Dataset: Import a YOLO dataset for machine learning (Detection, Segmentation).</p> </li> <li> <p>Export:</p> </li> <li>Export Labels (JSON): Save label data to a JSON file.</li> <li>Export TagLab Labels (JSON): Save label data to a TagLab JSON file.</li> <li>Export Annotations (JSON): Save annotation data to a JSON file.</li> <li>Export GeoJSON Annotations: Save annotations to GeoJSON file (only for GeoTIFFs with CRS and Transforms data)</li> <li>Export Mask Annotations (Raster): Save annotations as segmentation masks</li> <li>Export CoralNet Annotations: Save annotation data to a CoralNet CSV file.</li> <li>Export TagLab Annotations: Save annotation data to a TagLab JSON file.</li> <li>Export Viscore Annotations: Save annotation data to a Viscore CSV file.</li> <li> <p>Export Dataset: Create a YOLO dataset for machine learning (Classification, Detection, Segmentation).</p> </li> <li> <p>Sample:</p> </li> <li> <p>Sample Annotations: Automatically generate Patch annotations.</p> </li> <li> <p>Tile:</p> </li> <li> <p>Tile Dataset: Tile existing Classification, Detection or Segmention datasets using <code>yolo-tiling</code>.</p> </li> <li> <p>CoralNet: </p> </li> <li>Authenticate: Authenticate with CoralNet.</li> <li> <p>Download: Download data from CoralNet.</p> </li> <li> <p>Ultralytics:</p> </li> <li>Merge Datasets: Merge multiple Classification datasets.</li> <li>Train Model: Train a machine learning model.</li> <li>Evaluate Model: Evaluate a trained model.</li> <li>Optimize Model: Convert model format.</li> <li>Deploy Model: Make predictions using a trained model (Classification, Detection, Segmentation).</li> <li> <p>Batch Inference: Perform batch inferences.</p> </li> <li> <p>SAM:</p> </li> <li>Deploy Predictor: Deploy <code>EdgeSAM</code>, <code>MobileSAM</code>, <code>SAM</code>, etc, to use interactively (points, box).</li> <li>Deploy Generator: Deploy <code>FastSAM</code> to automatically segment the image.</li> <li> <p>Batch Inference: Perform batch inferencing using <code>FastSAM</code>.</p> </li> <li> <p>See Anything (YOLOE):</p> </li> <li>Train Model: Train a YOLOE Segmentation model using an existing YOLO Segmentation dataset.</li> <li>Deploy Predictor: Deploy an existing <code>YOLOE</code> model to use interactively.</li> <li> <p>Batch Inference: Perform batch inferencing using <code>YOLOE</code>.</p> </li> <li> <p>AutoDistill:</p> </li> <li>Deploy Model: Deploy a foundational model<ul> <li>Models Available: <code>Grounding DINO</code>, <code>OWLViT</code>, <code>OmDetTurbo</code></li> </ul> </li> <li>Batch Inference: Perform batch inferences.</li> </ul>"},{"location":"usage/#tool-bar","title":"Tool Bar","text":"<ul> <li> <p>Select Tool:   -</p> </li> <li> <p>Patch Tool:</p> </li> <li></li> <li> <p>Rectangle Tool:   -</p> </li> <li> <p>Polygon Tool:   -</p> </li> <li> <p>SAM Tool: After a model is loaded</p> </li> <li>Space Bar: Set working area; confirm prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Ctrl + Left-Click: Add positive point.</li> <li>Ctrl + Right-Click: Add negative point.</li> <li> <p>Backspace: Discard unfinalized predictions.</p> </li> <li> <p>See Anything (YOLOE) Tool: After a model is loaded</p> </li> <li>Space Bar: Set working area; run prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Backspace: Discard unfinalized predictions.</li> </ul>"},{"location":"usage/#status-bar","title":"Status Bar","text":"<ul> <li>Image Size: Displays the image size.</li> <li>Cursor Position: Displays the cursor position.</li> <li>View Extent: Displays the view extent.</li> <li>Annotation Transparency: Adjust the annotation transparency.</li> <li>Select All Labels: Select all labels, adjusting transparency for all labels.</li> <li>Unselect All Labels: Unselect all labels, adjusting transparency for only selected labels.</li> <li>Patch Size: Manipulate Patch Size (only active when using Patch Tool).</li> <li>Parameters: Adjust parameters including uncertainty, IoU, and area thresholds.</li> </ul>"},{"location":"usage/#annotation-window","title":"Annotation Window","text":"<ul> <li>Zoom: Use the mouse wheel to zoom in and out.</li> <li>Pan: Hold Ctrl + Right-click the mouse button to pan the image.</li> </ul>"},{"location":"usage/#label-window","title":"Label Window","text":"<ul> <li>Move Label: Right-click and drag to move labels.</li> <li>Add Label: Click the \"Add Label\" button to add a new label.</li> <li>Delete Label: Click the \"Delete Label\" button to delete the selected label.</li> <li>Edit Label: Click the \"Edit Label\" button to edit the selected label.</li> <li>Lock Label: Click the \"Lock Label\" button to lock the selected label.</li> </ul>"},{"location":"usage/#image-window","title":"Image Window","text":"<ul> <li>Load Image: Click on a row to load the image in the annotation window.</li> <li>Delete Image: Right-click on a row and select \"Delete Image\" to remove the image.</li> <li>Delete Annotations: Right-click on a row and select \"Delete Annotations\" to remove the image's annotations.</li> <li>Search / Filter:</li> <li>By Image: Filter for images by name or sub-string.</li> <li>By Label: Filter images by labels they contain.</li> <li>No Annotations: Filter images with no annotations.</li> <li>Has Annotations: Filter images with annotations.</li> <li>Has Predictions: Filter images with predictions.</li> <li>Selected: Filter images selected.</li> </ul>"},{"location":"usage/#confidence-window","title":"Confidence Window","text":"<ul> <li>Display Cropped Image: Shows the cropped image of the selected annotation.</li> <li>Confidence Chart: Displays a bar chart with confidence scores.</li> <li>Prediction Selection: Select a prediction from the list to change the label.</li> </ul>"},{"location":"usage/#hotkeys","title":"Hotkeys","text":"<ul> <li>Ctrl + W/A/S/D: Navigate through labels.</li> <li>Ctrl + Left/Right: Cycle through annotations.</li> <li>Ctrl + Up/Down: Cycle through images.</li> <li>Ctrl + Shift + &lt;: Select all annotations.</li> <li>Ctrl + Shift + &gt;: Unselect all annotations.</li> <li> <p>Escape: Exit the program.</p> </li> <li> <p>Machine Learning, SAM, and AutoDistill: After a model is loaded</p> </li> <li>Ctrl + 1: Make prediction on selected Patch annotation, else all in the image with Review label.</li> <li>Ctrl + 2: Make predictions using Object Detection model.</li> <li>Ctrl + 3: Make predictions using Instance Segmentation model.</li> <li>Ctrl + 4: Make predictions using FastSAM model.</li> <li>Ctrl + 5: Make predictions using AutoDistill model.</li> </ul>"}]}