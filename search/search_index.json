{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CoralNet-Toolbox \ud83e\udeb8\ud83e\uddf0","text":"[![python-version](https://img.shields.io/pypi/pyversions/CoralNet-Toolbox.svg)](https://pypi.org/project/CoralNet-Toolbox) [![version](https://img.shields.io/pypi/v/CoralNet-Toolbox.svg)](https://pypi.python.org/pypi/CoralNet-Toolbox) [![pypi-passing](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![windows](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/windows.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![macos](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/macos.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![ubuntu](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/ubuntu.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox)  \ud83d\udd0d Annotation <p>Create patches, rectangles, and polygons with AI assistance</p> \ud83e\udde0 AI-Powered <p>Leverage SAM, YOLOE, and various foundation models</p> \ud83d\ude80 Complete Workflow <p>From data collection to model training and deployment</p>"},{"location":"#quick-start","title":"\ud83d\udea6 Quick Start","text":"<p>Running the following command will install the <code>coralnet-toolbox</code>, which you can then run from the command line: <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n\n# Run\ncoralnet-toolbox\n</code></pre></p>"},{"location":"#guides","title":"\ud83d\udcda Guides\ud83c\udfa5 Watch the Video Demos","text":"<p>For further instructions please see the following guides: - Installation - Usage - Patch-based Image Classifier</p> <p> </p>"},{"location":"#tldr","title":"\u23e9 TL;Dr","text":"<p>The <code>CoralNet-Toolbox</code> is an unofficial codebase that can be used to augment processes associated with those on CoralNet.</p> <p>It uses\u2728<code>Ultralytics</code>\ud83d\ude80 as a  base, which is an open-source library for computer vision and deep learning built in <code>PyTorch</code>. For more information on their <code>AGPL-3.0</code> license, see here.</p>"},{"location":"#supported-models","title":"\ud83d\ude80 Supported Models","text":"<p>The <code>toolbox</code> integrates a variety of state-of-the-art models to help you create rectangle and polygon annotations efficiently. Below is a categorized overview of the supported models and frameworks:</p>   | Category                | Models                                                                                       | |-------------------------|---------------------------------------------------------------------------------------------------------| | **Trainable**           | - \ud83e\uddbe [YOLOv3](https://docs.ultralytics.com/models/)  - \ud83e\udd88 [YOLOv4](https://docs.ultralytics.com/models/)  - \ud83e\udd85 [YOLOv5](https://docs.ultralytics.com/models/)  - \ud83d\udc2c [YOLOv6](https://docs.ultralytics.com/models/)  - \ud83d\udc22 [YOLOv7](https://docs.ultralytics.com/models/)  - \ud83d\udc19 [YOLOv8](https://docs.ultralytics.com/models/)  - \ud83d\udc20 [YOLOv9](https://docs.ultralytics.com/models/)  - \ud83e\udd91 [YOLOv10](https://docs.ultralytics.com/models/)  - \ud83d\ude80 [YOLO11](https://docs.ultralytics.com/models/)  - \ud83d\udc33 [YOLO12](https://docs.ultralytics.com/models/) | | **Segment Anything**    | - \ud83e\udeb8 [SAM](https://github.com/facebookresearch/segment-anything)  - \ud83c\udf0a [CoralSCOP](https://github.com/zhengziqiang/CoralSCOP)  - \u26a1 [FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM)  - \ud83d\udd01 [RepViT-SAM](https://github.com/THU-MIG/RepViT)  - \u2702\ufe0f [EdgeSAM](https://github.com/chongzhou96/EdgeSAM)  - \ud83d\udcf1 [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) | | **Visual Prompting**    | - \ud83d\udc41\ufe0f [YOLOE](https://github.com/THU-MIG/yoloe)  - \ud83e\udd16 [AutoDistill](https://github.com/autodistill):  \u00a0\u00a0\u00a0\u2022 \ud83e\udd92 Grounding DINO  \u00a0\u00a0\u00a0\u2022 \ud83e\udd89 OWLViT  \u00a0\u00a0\u00a0\u2022 \u26a1 OmDetTurbo |   <p>These models enable fast, accurate, and flexible annotation workflows for a wide range of use cases for patch-based image classification, object detection, instance segmentation.</p>"},{"location":"#toolbox-features","title":"\ud83d\udee0\ufe0f Toolbox Features","text":"| ![Patch Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Patches.gif)<sub>**Patch Annotation**</sub> | ![Rectangle Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Rectangles.gif)<sub>**Rectangle Annotation**</sub> | ![Polygon Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Polygons.gif)<sub>**(Multi) Polygon Annotation**</sub> | |:--:|:--:|:--:| | ![Patch-based Image Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classification.gif)<sub>**Image Classification**</sub> | ![Object Detection](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Object_Detection.gif)<sub>**Object Detection**</sub> | ![Instance Segmentation](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Instance_Segmentation.gif)<sub>**Instance Segmentation**</sub> | | ![Segment Anything Model (SAM)](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Segment_Anything.gif)<sub>**Segment Anything (SAM)**</sub> | ![Polygon Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classifying_Polygons.gif)<sub>**Polygon Classification**</sub> | ![Region-based Detection](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Work_Areas.gif)<sub>**Region-based Detection**</sub> | | ![Cut](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Cut.gif)<sub>**Cut**</sub> | ![Combine](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Combine.gif)<sub>**Combine**</sub> | ![Simplify](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Simplify.gif)<sub>**Simplify**</sub> | | ![See Anything (YOLOE)](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/See_Anything.gif)<sub>**See Anything (YOLOE)**</sub> | ![Patch-based LAI Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classifying_Orthomosaics.gif)<sub>**Patch-based LAI Classification**</sub> | ![Video Inference](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Analytics.gif)<sub>**Video Inference**</sub> |   <p>Enhance your CoralNet experience with these tools: - \ud83d\udce5 Download: Retrieve Source data (images and annotations) from CoralNet - \ud83c\udfac Rasters: Import images, or extract frames directly from video files - \u270f\ufe0f Annotate: Create annotations freely - \ud83d\udc41\ufe0f Visualize: See CoralNet and CPCe annotations superimposed on images - \ud83d\udd2c Sample: Sample patches using various methods (Uniform, Random, Stratified) - \ud83e\udde9 Patches: Create patches (points) - \ud83d\udd33 Rectangles: Create rectangles (bounding boxes) - \ud83d\udfe3 Polygons: Create polygons (instance masks)   - \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 MultiPolygons: Combine multiple, non-overlapping polygons (i.e, genets) - \u270d\ufe0f Edit: Cut and Combine polygons and rectangles - \ud83e\uddbe SAM: Use <code>FastSAM</code>, <code>CoralSCOP</code>, <code>RepViT-SAM</code>, <code>EdgeSAM</code>, <code>MobileSAM</code>, and <code>SAM</code> to create polygons   - Uses <code>xSAM</code> - \ud83d\udc77\u200d\u2642\ufe0f Work areas: Perform region-specific detections / segmentations with any model - \ud83d\udc40 YOLOE (See Anything): Detect similar appearing objects using visual prompts automatically - \ud83e\uddea AutoDistill: Use <code>AutoDistill</code> to access the following for creating rectangles and polygons:   - Uses <code>Grounding DINO</code>, <code>OWLViT</code>, <code>OmDetTurbo</code> - \ud83d\udcfb Tune: Tune hyperparameters to identify ideal training conditions - \ud83e\udde0 Train: Build local patch-based classifiers, object detection, and instance segmentation models - \ud83d\udd2e Deploy: Use trained models for predictions - \ud83d\udcca Evaluation: Evaluate model performance - \ud83d\ude80 Optimize: Productionize models for faster inferencing - \u2699\ufe0f Batch Inference: Perform predictions on multiple images, automatically - \ud83c\udf9e\ufe0f Video Inference: Perform predictions on a video in real-time, record the output and analytics - \u2194\ufe0f I/O: Import and Export annotations from / to CoralNet, Viscore, and TagLab   - Export annotations as GeoJSONs, segmentation masks - \ud83d\udcf8 YOLO: Import and Export YOLO datasets for machine learning - \ud83e\uddf1 Tile Dataset: Tile existing Detection / Segmentation datasets   - Uses <code>yolo-tiling</code> - \ud83c\udfd7\ufe0f Tile Inference: Pre-compute multiple work areas for an entire image</p>"},{"location":"#todo","title":"\ud83d\udcdd TODO","text":"<ul> <li>\ud83e\udd17 Model Zoo: Download <code>Ultralytics</code> models from <code>HuggingFace</code> for use in <code>toolbox</code></li> <li>\ud83e\udd8a BioCLIP, MobileCLIP (AutoDistill): Automatically classify annotations</li> <li>\ud83d\udce6 Toolshed: Access tools from the old repository</li> </ul>"},{"location":"classify/","title":"Classify","text":"<p>A big thanks to the researchers at the research arm of the Seattle Aquarium for providing this guide.</p> <pre><code>@misc{williams2025,\n  author = {Williams, Megan},\n  title = {SOP to train a classification model in Toolbox},\n  institution = {Seattle Aquarium},\n  date = {2025-04-04}\n}\n</code></pre>"},{"location":"classify/#sop-to-train-a-classification-model-in-toolbox","title":"SOP to train a classification model in Toolbox","text":"<p>The following steps are required to create a training dataset, train a classification model using Ultralytics YOLO in Toolbox, and apply the model to make predictions.</p>"},{"location":"classify/#1-toolbox-installation-and-setup","title":"1. Toolbox installation and setup","text":"<ul> <li>Instructions for installing and running Toolbox can be found in the documentation linked here.</li> </ul>"},{"location":"classify/#2-prepare-training-and-testing-images","title":"2. Prepare training and testing images","text":"<ul> <li>Ensure images are color corrected and a quality desired for analysis.</li> <li>Split images into training and testing folder. Our technique is:</li> <li>Training folder: Move 2 out of every 3 images here.</li> <li>Testing folder: Move 1 out of every 3 images here.</li> </ul>"},{"location":"classify/#3-load-labelset","title":"3. Load labelset","text":"<ul> <li>Open Toolbox and import your classification labelset</li> <li>Go to Labelset \u2192 Import</li> <li>Our JSON labelset can be found here. You can also preview the labelset in Excel here.</li> </ul>"},{"location":"classify/#4-import-and-annotate-training-images","title":"4. Import and annotate training images","text":"<ul> <li>Import training images into Toolbox</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images</li> <li>Create image patches (classification annotations)</li> <li>Select a label in the lower label window.</li> <li>Choose the image patch tool (rectangle icon) from the toolbar on the left.</li> <li>In the annotation window (center window), left click the appropriate location in the image to add a patch for that label.</li> <li>Repeat for each label across your training images.</li> </ul>"},{"location":"classify/#5-export-classification-dataset","title":"5. Export classification dataset","text":"<ul> <li>After annotating, export the dataset:</li> <li>File \u2192 Export \u2192 Dataset \u2192 Classify</li> <li>Toolbox will generate a dataset directory containing train, validation, and test folders with labeled image patches.</li> </ul>"},{"location":"classify/#6-train-classification-model","title":"6. Train classification model","text":"<ul> <li>Start training a YOLO classification model</li> <li>Ultralytics \u2192 Train Model \u2192 Classify</li> <li>In the training window:</li> <li>Dataset: Click Browse and select the exported dataset folder.</li> <li>Model Selection: Choose the Ultralytics YOLO model that fits your needs (e.g., YOLOv8 or YOLOv11). A guide comparing model options is available here.</li> <li>Parameters:<ul> <li>Set the location where you want your trained model to be saved.</li> <li>You can use default training parameters or customize them. More information about the parameters can be found here.</li> </ul> </li> <li>Click OK to begin training. You can monitor training progress in the terminal.</li> </ul>"},{"location":"classify/#7-load-and-deploy-model","title":"7. Load and deploy model","text":"<ul> <li>After training completes:</li> <li>Go to Ultralytics \u2192 Deploy Model \u2192 Classify</li> <li>Under Actions, click Browse Model and select your trained weights file (best.pt).</li> <li>Click load model</li> </ul>"},{"location":"classify/#8-test-the-model-on-new-images","title":"8. Test the model on new images","text":"<ul> <li>Remove training images:</li> <li>In the image window, click \"Select All\", right-click and select delete all images to remove.</li> <li>Import test images:</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images and choose the testing folder.</li> <li>Create random image patches:</li> <li>Click Sample at the top</li> <li>Set your desired sampling configuration (e.g., number of patches).</li> <li>Set Select Label to Review</li> <li>Check \"Apply to all images\" and click Accept</li> </ul>"},{"location":"classify/#9-run-predictions","title":"9. Run predictions","text":"<ul> <li>To predict label for the new image patches:</li> <li>For a single image: press Ctrl + 1</li> <li>For all images:<ul> <li>Ultralytics \u2192 Batch Inference \u2192 Classify</li> <li>Check \"Apply to all images\" and \"Predict review annotation\"</li> </ul> </li> </ul>"},{"location":"classify/#10-review-and-correct-predictions","title":"10. Review and correct predictions","text":"<ul> <li>Predicted labels appear for each review image patch.</li> <li>Confidence levels are shown in the Confidence window.</li> <li>To fix incorrect predictions:</li> <li>Select the image patch so that it is shown in the Confidence window</li> <li>Select the correct label in the label window</li> </ul>"},{"location":"classify/#11-export-and-improve-dataset","title":"11. Export and improve dataset","text":"<ul> <li>To analyze results:</li> <li>Export annotation file as a .csv file: File \u2192 Export \u2192 CSV</li> <li>To improve your model:</li> <li>Create a new dataset with the corrected predictions</li> <li>Merge this with the original dataset:<ul> <li>Ultralytics \u2192 Merge Datasets \u2192 Classify</li> <li>Set a name and location for the merged dataset</li> <li>Click \u2192 Add Dataset and select the datasets you want to combine</li> </ul> </li> </ul>"},{"location":"classify/#12-improve-existing-model","title":"12. Improve existing model","text":"<ul> <li>The merged dataset can now be used to train a new model following Step 6.</li> <li>Instead of selecting a new YOLO model, you can use your existing model:</li> <li>Under Model Selection switch to Use Existing Model</li> <li>Browse to the model weights (best.pt)</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#how-to-install","title":"\ud83d\udcbe How to Install","text":""},{"location":"installation/#anaconda","title":"\ud83d\udc0d Anaconda","text":"<p>It's recommended to use <code>Anaconda</code> to create an environment for the <code>toolbox</code>: <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n</code></pre></p>"},{"location":"installation/#install","title":"\ud83d\udce6 Install","text":"<p>Once this has finished, install the <code>toolbox</code> using <code>uv</code>: </p> <pre><code># cmd\n\n# Install uv first\npip install uv\n\n# Install with uv\nuv pip install coralnet-toolbox\n</code></pre> <p>Although fast, <code>uv</code> is still relatively new; if this fails, simply fall back to using <code>pip</code>:</p> <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n</code></pre>"},{"location":"installation/#cuda","title":"\u26a1 CUDA","text":"<p>If you have <code>CUDA</code>, you should install the versions of <code>cuda-nvcc</code> and <code>cudatoolkit</code> that you need, and then install the corresponding versions of <code>torch</code> and <code>torchvision</code>. Below is an example of how that can be done using <code>CUDA</code> version 11.8: <pre><code># cmd\n\n# Example for CUDA 11.8\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\nuv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n</code></pre></p> <p>If <code>CUDA</code> is installed on your computer, and <code>torch</code> was built with it properly, you should see a <code>\ud83d\udc07</code> icon in the <code>toolbox</code> instead of a <code>\ud83d\udc22</code>; if you have multiple <code>CUDA</code> devices available, you should see a <code>\ud83d\ude80</code> icon, and if you're using a Mac with <code>Metal</code>, you should see an <code>\ud83c\udf4e</code> icon (click on the icon to see the device information).</p> <p>See here for more details on versions for the following: - <code>cuda-nvcc</code> - <code>cudatoolkit</code> - <code>torch</code></p>"},{"location":"installation/#run","title":"\u25b6\ufe0f Run","text":"<p>Finally, you can run the <code>toolbox</code> from the command line:</p> <pre><code># cmd\n\n# Run\ncoralnet-toolbox\n</code></pre>"},{"location":"installation/#how-to-upgrade","title":"How to Upgrade","text":"<p>When opening the <code>toolbox</code>, you will be notified if there is an update available, and you have the option to do so,  if you so choose. To upgrade, run the following command from your terminal:</p> <pre><code># cmd\n\nuv pip install -U coralnet-toolbox==[enter_newest_version_here]\n</code></pre> <p>Again, fall back to using just <code>pip</code> and not <code>uv</code> if this fails.</p>"},{"location":"installation/#github-repository","title":"GitHub Repository","text":"<p>If you prefer to clone the repository and run the <code>toolbox</code> from the source code, you can do so with the following:</p> <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n\n# Install git via conda, if not already installed\nconda install git -y\n\n# Change to the desired directory (e.g., Documents)\ncd Documents\n\n# Clone and enter the repository\ngit clone https://github.com/Jordan-Pierce/CoralNet-Toolbox.git\ncd CoralNet-Toolbox\n\n# Install the latest\npip install -e .\n\n# Install CUDA requirements (if applicable)\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n\n# Run\ncoralnet-toolbox\n</code></pre> <p>To update your repository to match the current version on <code>main</code>, run <code>fetch</code> and <code>pull</code> commands:</p> <pre><code># cmd\n\n# Change to the proper directory\ncd Coralnet-Toolbox\n\n# Ask for the updates on main\ngit fetch\n\n# Pull the updates from main\ngit pull\n\n# Update your conda environment \npip install -e . -U\n</code></pre> <p>Or, if you want to simply install the <code>toolbox</code> from the GitHub repo directly you can also do the following:</p> <pre><code># cmd\n\npip install git+https://github.com/Jordan-Pierce/CoralNet-Toolbox.git@main -U\n\n# replace @main with a different branch if you want to test out experimental code\n</code></pre>"},{"location":"overview/","title":"The CoralNet-Toolbox: A Comprehensive Guide to Advanced Benthic Image Analysis","text":""},{"location":"overview/#section-1-introduction-augmenting-coral-reef-analysis-in-the-age-of-ai","title":"Section 1: Introduction - Augmenting Coral Reef Analysis in the Age of AI","text":""},{"location":"overview/#11-the-challenge-the-manual-annotation-bottleneck-in-benthic-ecology","title":"1.1 The Challenge: The Manual Annotation Bottleneck in Benthic Ecology","text":"<p>The world's coral reefs are facing unprecedented threats from climate change and local stressors, leading to dramatic declines in coral coverage globally. Quantifying the state of these vital ecosystems, determining the impact of various causative factors, and measuring the efficacy of restoration efforts require carefully designed surveys that operate at immense spatial and temporal scales. Modern monitoring programs, utilizing technologies like downward-facing cameras on towed floats, remotely operated vehicles (ROVs), and autonomous underwater vehicles (AUVs), now generate vast quantities of high-resolution imagery, often numbering in the tens to hundreds of thousands of images per survey.</p> <p>While the acquisition of this imagery has become relatively straightforward, the subsequent analysis has historically posed a significant challenge. The traditional method of analysis requires a human expert to manually inspect each photograph, identifying and labeling the substrate under hundreds of randomly sampled points to estimate benthic percent cover. This process is extraordinarily resource-intensive, time-consuming, and susceptible to inter-observer variability, even among trained experts. This \"manual annotation bottleneck\" has been a primary limiting factor in marine science, hindering the ability of researchers and managers to assess reef health at the scales necessary to respond to rapid environmental change. The sheer volume of data often means that only a fraction can be analyzed, leaving valuable ecological information untapped.</p>"},{"location":"overview/#12-the-foundational-platform-coralnet","title":"1.2 The Foundational Platform: CoralNet","text":"<p>To address this critical bottleneck, a team of researchers at the University of California San Diego (UCSD) developed CoralNet, an open-source, web-based platform for benthic image analysis. Launched in its alpha version in 2011, CoralNet was conceived to make advanced computer vision methods accessible to the global coral reef research community. The platform serves three primary functions: it is a centralized data repository for benthic imagery, a collaboration platform for research teams, and, most importantly, a system for semi-automated image annotation powered by machine learning.</p> <p>The core workflow of CoralNet is centered on patch-based image classification. A user creates a \"Source,\" which is an organizational project containing images and a defined set of labels (a \"labelset\"). The user then annotates a subset of their images using an in-browser point-count interface. Once a sufficient number of points are manually confirmed (typically on at least 20 images), CoralNet's backend automatically trains a machine learning model. This model learns to classify the content of a small image patch (typically 224\u00d7224 pixels) centered on a given point. The trained classifier, or \"robot,\" can then be used to automatically suggest labels for the remaining unannotated points, significantly speeding up the analysis process and achieving 50-100% automation in many cases.</p> <p>CoralNet has evolved significantly over the years. The initial Alpha version used conventional computer vision techniques. The Beta version, launched in 2016, represented a major leap forward by incorporating a powerful deep learning backend based on the VGG16 architecture, which performed on par with human experts. The current iteration, CoralNet 1.0, released in 2021, features an even more advanced engine built on an EfficientNet-B0 backbone, pre-trained on a massive dataset of 16 million labeled patches from over 1200 classes of marine organisms.</p> <p>This evolution established CoralNet as an invaluable tool, proven to generate estimates of coral cover that are highly comparable to those from human analysts. However, its architecture was deliberately focused on solving one specific problem: patch-based classification for percent cover estimation. The platform does not natively support other critical computer vision tasks such as object detection (locating individual organisms with bounding boxes) or instance segmentation (delineating the precise pixel-wise outline of each organism). This architectural focus, while effective for its primary purpose, prevents researchers from addressing scientific questions that require counting, sizing, or analyzing the morphology of individual colonies. This limitation created a clear and pressing need within the research community for a tool that could leverage the vast data repositories within CoralNet to perform these more advanced analyses.</p>"},{"location":"overview/#13-the-unofficial-extension-the-coralnet-toolbox","title":"1.3 The Unofficial Extension: The CoralNet-Toolbox","text":"<p>The CoralNet-Toolbox is an unofficial, open-source Python application developed by Jordan Pierce to directly address the functional gaps of the official CoralNet platform. It is a locally-run desktop application that augments and extends the capabilities of CoralNet, acting as a powerful bridge between the CoralNet ecosystem and the cutting edge of computer vision research.</p> <p>The primary purpose of the toolbox is to provide a comprehensive suite of tools for advanced annotation, model training, and analysis, with a focus on object detection and instance segmentation\u2014tasks not available on the web platform. It is built upon the powerful and widely-used Ultralytics open-source library, which uses PyTorch as its deep learning backend. This foundation gives users direct access to the state-of-the-art \"You Only Look Once\" (YOLO) family of models, enabling flexible and reproducible machine learning workflows on their local machines. The toolbox is not a replacement for CoralNet but rather a synergistic partner, designed to interact with the CoralNet platform for data input/output while providing a local environment for more sophisticated analysis.</p>"},{"location":"overview/#table-1-feature-comparison-coralnet-vs-coralnet-toolbox","title":"Table 1: Feature Comparison: CoralNet vs. CoralNet-Toolbox","text":"Feature CoralNet (Official Platform) CoralNet-Toolbox (Unofficial Tool) Environment Web-based, cloud-hosted on AWS Local, runs on the user's desktop or personal cloud compute Primary Task Patch-based Image Classification for percent cover estimation Object Detection and Instance Segmentation for counting, sizing, and morphology Annotation Support Points only Points, Rectangles (Boxes), and Polygons (Masks) Model Training Automated, server-side, limited user control (\"black box\") User-controlled, local, fully configurable, and transparent Core Engine Custom, based on EfficientNet-B0 Ultralytics, based on the YOLO model series (e.g., YOLOv8) AI-Assisted Segmentation No Yes, integrates SAM, MobileSAM, FastSAM, and others Interoperability Provides a Deploy API for programmatic inference Provides a GUI for I/O with CoralNet, Viscore, TagLab, and YOLO formats"},{"location":"overview/#section-2-the-annotation-pipeline-from-points-to-polygons","title":"Section 2: The Annotation Pipeline - From Points to Polygons","text":"<p>The foundation of any successful supervised machine learning project is a high-quality, accurately labeled dataset. The CoralNet-Toolbox provides a rich suite of tools designed to facilitate the creation of annotations for a variety of computer vision tasks, moving beyond the simple point-based approach of its namesake to enable more sophisticated analyses. This section details the hierarchy of annotation types supported by the toolbox, methods for data ingestion, and the revolutionary impact of integrated AI-assisted tools like the Segment Anything Model (SAM) on annotation efficiency.</p>"},{"location":"overview/#21-the-annotation-hierarchy-the-language-of-computer-vision","title":"2.1 The Annotation Hierarchy: The Language of Computer Vision","text":"<p>The type of annotation a researcher creates directly dictates the type of machine learning model that can be trained and, consequently, the scientific questions that can be answered. The toolbox supports the three primary annotation primitives used in modern computer vision.</p> <ul> <li> <p>Patches (Points): This is the most fundamental form of annotation, where a single pixel coordinate is labeled. This method is primarily used for Image Classification. The model is not trained on the single pixel itself, but on a square image \"patch\" (e.g., 224\u00d7224 pixels) extracted around that point. The task of the classifier is to assign a single class label to the entire patch. This approach aligns with the traditional methodologies of CoralNet and Coral Point Count with CPCe, and it is highly effective for estimating the proportional area or percent cover of different benthic categories within an image. It answers the question, \"What is the dominant substrate at this specific location?\"</p> </li> <li> <p>Rectangles (Bounding Boxes): This annotation type involves drawing a rectangular box that tightly encloses an object of interest. Bounding boxes are the standard annotation format for Object Detection. An object detection model learns to predict the coordinates of these boxes and assign a class label to each one. This task answers the questions, \"What objects are in this image, and where are they located?\" It is a significant step up from classification, as it can count distinct objects, but it does not provide information about their precise shape, size, or orientation.</p> </li> <li> <p>Polygons (Masks): As the most detailed and informative annotation type, polygons involve tracing the exact pixel-wise boundary of each individual object instance. These annotations are used to train models for Instance Segmentation, a task that combines the object detection goal of distinguishing individual instances with the semantic segmentation goal of classifying every pixel. The output is a unique \"mask\" for each object. This level of detail is essential for advanced quantitative analysis, such as measuring the surface area, growth, or complex morphology of coral colonies. It provides the most comprehensive answer: \"What objects are in this image, where are they, and what are their exact shapes and sizes?\"</p> </li> </ul>"},{"location":"overview/#table-2-annotation-hierarchy-and-corresponding-ml-tasks","title":"Table 2: Annotation Hierarchy and Corresponding ML Tasks","text":"Annotation Type Visual Example ML Task Scientific Application Patch (Point) A single crosshair on a coral branch. Image Classification Estimating percent cover of benthic categories (e.g., coral, algae, sand) for broad-scale habitat assessment. Rectangle (Box) A box drawn around an entire coral colony. Object Detection Counting individual organisms (e.g., number of coral colonies, number of sea urchins) per unit area. Polygon (Mask) A precise outline traced around the perimeter of a coral colony. Instance Segmentation Measuring the precise surface area, perimeter, and morphological complexity of individual organisms to track growth, disease progression, or bleaching extent."},{"location":"overview/#22-data-ingestion-and-management","title":"2.2 Data Ingestion and Management","text":"<p>A key feature of the CoralNet-Toolbox is its flexibility in building and managing datasets. It provides a unified interface for sourcing imagery and annotations from multiple locations, breaking down the data silos that can often hinder research. The primary methods for data ingestion include:</p> <ul> <li> <p>Direct Download from CoralNet: The toolbox can programmatically interact with the CoralNet website to download entire public \"Sources,\" including the images and their associated point annotations. This allows researchers to leverage the vast, publicly available data already housed on the platform as a starting point for more advanced annotation.</p> </li> <li> <p>Local File Import: Users can directly import folders of images from their local machine or extract individual frames from video files, such as those from ROV or AUV transects. This is essential for working with new or private datasets.</p> </li> <li> <p>Interoperability with Other Tools: The toolbox is designed to be a central hub in a wider analysis ecosystem. It features dedicated import and export functions for compatibility with other specialized annotation software, such as Viscore and TagLab. This interoperability is critical for complex projects that may involve different stages of analysis across multiple platforms, such as annotating points in CoralNet, creating polygons in the toolbox, and visualizing results on a 3D model in Viscore.</p> </li> </ul>"},{"location":"overview/#23-accelerating-segmentation-with-segment-anything-models-sam","title":"2.3 Accelerating Segmentation with Segment Anything Models (SAM)","text":"<p>Manually tracing the precise outlines of hundreds or thousands of corals to create a polygon dataset is an incredibly laborious and time-consuming process, representing an even greater bottleneck than point-based annotation. The integration of Segment Anything Models (SAM) into the CoralNet-Toolbox represents a paradigm shift in annotation efficiency, dramatically lowering the barrier to entry for high-value instance segmentation research.</p> <p>The workflow leverages the unique capabilities of SAM, a powerful foundation model from Meta AI that is trained to \"segment anything\" in an image given a simple prompt. Instead of requiring a user to meticulously trace an entire object, SAM can generate a detailed mask from a much simpler input, such as a single point or a bounding box. This enables a novel and highly efficient annotation workflow within the toolbox:</p> <ol> <li>The user deploys one of the integrated SAM models within the toolbox.</li> <li>The user draws a rectangular bounding box around a coral colony, or provides one or multiple points as prompts.</li> <li>The SAM model processes the image using the provided prompt(s) and, in a fraction of a second, automatically generates a high-fidelity, pixel-perfect polygon mask that traces the coral's boundary.</li> </ol> <p>This workflow effectively bridges the gap between low-effort bounding boxes and high-effort, high-value segmentation masks. It allows researchers to create rich instance segmentation datasets in a fraction of the time it would take with manual tracing alone. This practical application directly realizes the concept of using an object detection model's outputs (bounding boxes) to feed a segmentation model (SAM) to generate instance segmentations.</p> <p>The CoralNet-Toolbox integrates a suite of SAM variants to suit different needs, including the original, high-accuracy SAM; the faster FastSAM; and the lightweight MobileSAM, which is optimized for speed and use on systems with less computational power. Furthermore, the toolbox incorporates other advanced AI-assisted annotation tools like AutoDistill, which can leverage models like Grounding DINO and OWLViT to perform zero-shot object detection from natural language text prompts, further reducing the manual annotation burden.</p>"},{"location":"overview/#section-3-training-and-tuning-models-with-the-ultralytics-engine","title":"Section 3: Training and Tuning Models with the Ultralytics Engine","text":"<p>Once a high-quality annotated dataset has been prepared, the CoralNet-Toolbox provides a powerful and flexible local environment for training custom machine learning models. By leveraging the state-of-the-art YOLOv8 architecture through the Ultralytics framework, the toolbox empowers researchers with a level of control and transparency that is not possible on the official CoralNet platform. This section details the process of preparing a dataset for training, understanding the YOLOv8 engine, and executing the local training and tuning workflow.</p>"},{"location":"overview/#31-preparing-a-training-dataset","title":"3.1 Preparing a Training Dataset","text":"<p>Before training can begin, the annotated data must be organized into a specific format that the Ultralytics training engine can understand. This typically involves two key components:</p> <ul> <li> <p>Directory Structure: The images and their corresponding annotation files (e.g., .txt files containing bounding box coordinates or polygon vertices) must be organized into specific folders for training, validation, and testing. This separation is crucial: the model learns from the train set, its performance is monitored and hyperparameters are adjusted based on the val (validation) set, and its final, unbiased performance is reported on the test set, which it has never seen before.</p> </li> <li> <p>YAML Configuration File: A configuration file (in .yaml format) must be created to tell the training script where to find the data directories and to define the list of class names and their corresponding integer indices.</p> </li> </ul> <p>The CoralNet-Toolbox streamlines this often-tedious process with its integrated YOLO Import/Export feature. This function can automatically convert the annotations created within the toolbox's interface into the required YOLO format, saving the user significant time and reducing the potential for formatting errors.</p>"},{"location":"overview/#32-the-yolov8-architecture-a-state-of-the-art-engine","title":"3.2 The YOLOv8 Architecture: A State-of-the-Art Engine","text":"<p>The toolbox's training capabilities are powered by YOLOv8, the latest iteration in the highly successful \"You Only Look Once\" family of models developed by Ultralytics. YOLOv8 introduces several key architectural innovations that result in significant improvements in both speed and accuracy over its predecessors. Understanding these features helps in appreciating the power of the engine being used:</p> <ul> <li> <p>New Backbone and Neck: The model's backbone (which extracts features from the input image) and neck (which combines features from different scales) are updated, replacing the C3 module of YOLOv5 with a new C2f module. This design, inspired by the ELAN concept from YOLOv7, allows for richer feature gradient flow and improved performance.</p> </li> <li> <p>Anchor-Free Detection Head: This is a fundamental shift from many previous object detection models. Instead of predicting offsets from a large set of predefined \"anchor boxes,\" YOLOv8's head directly predicts the center of an object. This anchor-free approach reduces the number of predictions, simplifies the post-processing pipeline (specifically, Non-Maximum Suppression or NMS), and contributes to both faster and more accurate detection.</p> </li> <li> <p>Decoupled Head: The model uses separate neural network heads to perform the tasks of classification (\"what is the object?\") and regression (\"what are the coordinates of its bounding box?\"). This decoupling allows each head to specialize, which has become a mainstream best practice for achieving higher accuracy in modern object detectors.</p> </li> <li> <p>Advanced Loss Function: YOLOv8 incorporates the Task-Aligned Assigner, which uses a more sophisticated method for selecting the positive training examples for each ground-truth object. It also introduces the Distribution Focal Loss for the regression branch, which helps the model learn a more flexible and accurate representation of bounding box locations.</p> </li> </ul> <p>YOLOv8 is offered in several sizes, typically denoted as n (nano), s (small), m (medium), l (large), and x (extra-large). Smaller models like YOLOv8n are extremely fast but less accurate, making them suitable for resource-constrained devices. Larger models like YOLOv8x are more accurate but slower and require more computational resources for training and inference. The toolbox allows users to select the model size that best fits their specific trade-off between speed and accuracy.</p>"},{"location":"overview/#33-the-local-training-workflow","title":"3.3 The Local Training Workflow","text":"<p>Perhaps the most significant advantage of the CoralNet-Toolbox is that it moves the model training process from a remote, opaque service to a local, transparent, and fully controllable environment. On the official CoralNet platform, model training is an automated, server-side process with fixed rules; a new classifier is trained only after a certain threshold of new annotations is met, and it is only accepted if it meets a predefined accuracy improvement. The user is largely a passive participant in this process.</p> <p>In contrast, the toolbox provides explicit Train and Tune functionalities that execute on the user's own machine (or on cloud compute resources that the user controls). This local control offers several profound benefits for scientific research:</p> <ul> <li> <p>Rapid Iteration: Researchers can quickly experiment with different model architectures (e.g., training a YOLOv8s vs. a YOLOv8m), data augmentation strategies, or other training parameters and see the results immediately.</p> </li> <li> <p>Full Control: Every aspect of the training process, from the number of epochs to the learning rate, is configurable through the toolbox's interface or associated scripts.</p> </li> <li> <p>Reproducibility: This transparent workflow is critical for scientific rigor. Researchers can precisely document, save, and share their entire model training configuration, including the exact model architecture, hyperparameters, and dataset version used. This allows their results to be independently verified and reproduced by others, addressing the challenge of non-standard and opaque procedures in AI-assisted analysis that has been noted in the field.</p> </li> </ul> <p>The training process typically employs transfer learning, where a YOLOv8 model pre-trained on a large, general-purpose dataset like COCO is used as a starting point. The model's weights are then fine-tuned on the researcher's smaller, domain-specific dataset of benthic imagery. This technique allows the model to leverage the general feature-recognition capabilities it has already learned (e.g., edges, textures, colors) and adapt them to the specific task of identifying corals, resulting in high performance even with a limited amount of custom training data.</p>"},{"location":"overview/#34-hyperparameter-tuning-and-optimization","title":"3.4 Hyperparameter Tuning and Optimization","text":"<p>Achieving the absolute best performance from a machine learning model often requires finding the optimal set of hyperparameters\u2014settings that control the learning process itself, such as the learning rate, momentum, and weight decay. The CoralNet-Toolbox includes a Tune feature that automates this search. This function systematically runs multiple training experiments with different combinations of hyperparameters to identify the set that yields the best performance on the validation dataset. While computationally intensive, this step can provide a significant boost in model accuracy and is a powerful optimization tool that is entirely absent from the standardized CoralNet web platform workflow.</p>"},{"location":"overview/#table-3-supported-models-in-coralnet-toolbox","title":"Table 3: Supported Models in CoralNet-Toolbox","text":"Category Model Name Primary Use Case Trainable Models YOLOv8, YOLOv9, YOLOv10, etc. Training custom models for Object Detection, Instance Segmentation, and Classification. Segment Anything Models SAM, MobileSAM, FastSAM, EdgeSAM, RepViT-SAM, CoralSCOP AI-assisted annotation; generating high-quality polygon masks from simple prompts (points or boxes). Visual Prompting / Zero-Shot Models YOLOE (See Anything), AutoDistill (Grounding DINO, OWLViT) AI-assisted annotation; detecting objects based on visual examples or text prompts without prior training."},{"location":"overview/#section-4-strategic-model-selection-a-comparative-analysis-for-instance-segmentation","title":"Section 4: Strategic Model Selection: A Comparative Analysis for Instance Segmentation","text":"<p>For researchers aiming to perform instance segmentation\u2014the task of delineating the precise boundaries of individual organisms\u2014the CoralNet-Toolbox offers two primary strategic pathways. The choice between these approaches is not a matter of one being definitively superior, but rather a critical decision based on a fundamental trade-off between computational efficiency and the quality of the resulting segmentation masks. This section provides a deep, nuanced comparison of these two strategies to guide researchers in selecting the optimal method for their specific scientific objectives.</p>"},{"location":"overview/#41-approach-1-end-to-end-instance-segmentation-eg-yolov8-seg","title":"4.1 Approach 1: End-to-End Instance Segmentation (e.g., YOLOv8-Seg)","text":"<p>This approach utilizes a single, unified model that is trained to perform all parts of the instance segmentation task simultaneously. During a single forward pass through the network, the model predicts the object's class, its bounding box, and its pixel-wise segmentation mask. The YOLOv8-Seg models (yolov8n-seg.pt, etc.) are designed specifically for this end-to-end task.</p> <p>Strengths:</p> <ul> <li> <p>Computational Efficiency and Speed: The primary advantage of the end-to-end approach is its speed. Because the entire process is encapsulated within a single network architecture, it requires only one forward pass to generate all predictions. This makes it significantly faster than multi-stage pipelines and is the preferred method for real-time applications, such as processing video streams from ROVs or analyzing large image datasets where throughput is a major concern. The computational cost is lower, making it more accessible for users with less powerful hardware.</p> </li> <li> <p>Simplicity of Training and Deployment: The training pipeline is more straightforward. A single model is trained and optimized for one consolidated task. Similarly, deployment is simpler as only one model file needs to be managed and loaded for inference.</p> </li> </ul> <p>Weaknesses:</p> <ul> <li>Mask Quality and Precision: The most significant drawback of many real-time, end-to-end models is the potential for lower-quality segmentation masks. The mask prediction head often operates on down-sampled feature maps from the network's backbone to maintain speed. The resulting low-resolution masks are then up-scaled to the original image size, which can lead to a loss of fine detail and produce masks with imprecise or blocky boundaries. This can be particularly problematic for small objects or organisms with highly complex and intricate perimeters, which are common in coral reef ecosystems.</li> </ul>"},{"location":"overview/#42-approach-2-hybrid-object-detection-promptable-segmentation-eg-yolov8-od-mobilesam","title":"4.2 Approach 2: Hybrid Object Detection + Promptable Segmentation (e.g., YOLOv8-OD + MobileSAM)","text":"<p>This approach employs a two-stage, hybrid pipeline that decouples the tasks of detection and segmentation, leveraging the strengths of specialized models for each step.</p> <ul> <li>Detection Stage: A high-performance object detection model (e.g., YOLOv8-OD) is trained specifically to produce accurate and reliable bounding boxes for the objects of interest.</li> <li>Segmentation Stage: The bounding boxes generated in the first stage are then passed as prompts to a separate, pre-trained, promptable segmentation model, such as MobileSAM. This model, which was not trained on the user's specific data, uses its powerful zero-shot generalization capabilities to generate a high-fidelity mask for the object contained within each prompt box.</li> </ul> <p>Strengths:</p> <ul> <li> <p>Superior Mask Quality: This is the defining advantage of the hybrid approach. It leverages the extraordinary power of large-scale foundation models like SAM, which was trained on over a billion masks and excels at producing highly detailed and accurate segmentations for a vast range of objects and image types without task-specific training. This results in \"extremely smooth masks\" with exceptional boundary fidelity, capturing the fine details that end-to-end models might miss. This directly confirms the observation that masks generated by SAM are often of higher quality than those from integrated segmentation models.</p> </li> <li> <p>Modularity and Flexibility: The two-stage pipeline is modular. A researcher can independently upgrade the object detector or the segmentation model as new, improved versions become available, without needing to retrain the entire system.</p> </li> </ul> <p>Weaknesses:</p> <ul> <li> <p>Computational Cost and Speed: The most significant drawback is the performance overhead. This approach requires running two separate models sequentially for each image, which inherently incurs higher latency and computational cost. The total inference time is the sum of the detector's inference time and the segmentor's inference time, making this method substantially slower and generally unsuitable for real-time video processing. This aligns perfectly with the observation that end-to-end instance segmentation is computationally more efficient than the object detection plus MobileSAM pipeline.</p> </li> <li> <p>Workflow Complexity: The inference pipeline is more complex to implement and manage, as it involves coordinating the inputs and outputs of two distinct models.</p> </li> </ul>"},{"location":"overview/#43-head-to-head-comparison-and-recommendations","title":"4.3 Head-to-Head Comparison and Recommendations","text":"<p>The decision between these two powerful strategies hinges entirely on the specific requirements of the research question. It is not a matter of which approach is universally \"better,\" but which is \"fitter for the purpose.\" The choice represents a direct and fundamental trade-off between the speed of inference and the quality of the final segmentation mask.</p> <ul> <li> <p>When to Choose End-to-End (YOLOv8-Seg): This approach is the logical choice when speed is the primary constraint. Applications include real-time analysis of video footage, rapid screening of massive image archives for object presence, or any scenario where high throughput is more critical than achieving the highest possible boundary precision. It provides a \"good enough\" segmentation at a much higher frame rate.</p> </li> <li> <p>When to Choose the Hybrid Approach (YOLOv8-OD + MobileSAM): This approach is superior when mask accuracy is paramount. It is the ideal choice for scientific analyses that depend on precise measurements, such as quantifying coral surface area for growth and mortality studies, calculating complex morphological indices, or assessing the exact area affected by bleaching or disease. In these cases, the additional computational cost is justified by the significant improvement in data quality and the scientific validity of the resulting measurements.</p> </li> </ul>"},{"location":"overview/#table-4-comparative-analysis-yolov8-seg-vs-yolov8-od-mobilesam","title":"Table 4: Comparative Analysis: YOLOv8-Seg vs. YOLOv8-OD + MobileSAM","text":"Criterion YOLOv8-Seg (End-to-End) YOLOv8-OD + MobileSAM (Hybrid) Mask Quality/Precision Lower to Moderate; potential loss of detail from up-sampling. Higher to Excellent; leverages powerful foundation models for high-fidelity boundaries. Inference Speed Fast; a single forward pass through one network. Slow; two sequential model passes, incurring additive latency. Computational Cost Lower; requires resources for one model. Higher; requires resources for two models. Training Complexity Simpler; a single model is trained for a unified task. More Complex; detector must be trained, then pipeline must integrate the pre-trained segmentor. Ideal Use Case Real-time video analysis (e.g., ROV surveys), high-throughput image counting, applications where speed is the priority. High-precision scientific measurements (e.g., surface area, morphology), applications where accuracy is the priority."},{"location":"overview/#section-5-model-evaluation-deployment-and-inference","title":"Section 5: Model Evaluation, Deployment, and Inference","text":"<p>The final stages of the machine learning lifecycle\u2014rigorously evaluating the trained model's performance, optimizing it for efficient use, and deploying it to make predictions on new data\u2014are critical for translating a trained artifact into a useful scientific tool. The CoralNet-Toolbox provides a comprehensive set of features to manage these crucial steps, offering a level of analytical depth and transparency that supports robust and reproducible science.</p>"},{"location":"overview/#51-evaluating-model-performance","title":"5.1 Evaluating Model Performance","text":"<p>After a model has been trained, it is essential to assess its performance on an unseen test dataset to understand its strengths and weaknesses. The toolbox's Evaluation feature facilitates this process, providing a much richer suite of metrics than the simple accuracy score used by the official CoralNet platform.</p> <p>The evaluation process within CoralNet is based on a straightforward accuracy metric (the percentage of correctly classified points) and an internal rule that a new classifier is only adopted if it is at least 1% more accurate than the previous one on a validation set. While functional for its internal ranking system, this single metric provides a limited view of model performance.</p> <p>In contrast, the toolbox, by virtue of its Ultralytics backend, generates a comprehensive set of industry-standard evaluation metrics that are common in the computer vision field. These include:</p> <ul> <li> <p>Precision and Recall: Precision measures the accuracy of the positive predictions (of the objects the model detected, how many were correct?), while Recall measures the model's ability to find all the actual positive instances (of all the true objects in the image, how many did the model find?).</p> </li> <li> <p>Mean Average Precision (mAP): This is the primary metric for object detection and instance segmentation tasks. It provides a single number that summarizes the model's performance across all classes and at various levels of Intersection over Union (IoU) thresholds. A higher mAP score indicates a better model. For example, mAP50 (or mAP@.5) evaluates performance when an IoU of 50% is required for a detection to be considered a true positive, while mAP50-95 averages the mAP over IoU thresholds from 50% to 95%.</p> </li> <li> <p>Confusion Matrix: This table visualizes the performance of a classification model, showing which classes are frequently confused with others. This is invaluable for identifying specific weaknesses in the classifier.</p> </li> <li> <p>Cohen's Kappa: This statistic measures inter-rater agreement for categorical items, correcting for the probability of agreement occurring by chance. It can be used to compare the model's predictions against a human expert's, providing a more robust measure of agreement than simple accuracy.</p> </li> </ul> <p>By providing these detailed metrics, the toolbox enables a more rigorous, transparent, and standardized evaluation. This allows researchers to deeply understand their model's performance and to report their results using metrics that are widely understood and accepted in the broader scientific and computer vision communities, thereby enhancing the credibility and reproducibility of their work.</p>"},{"location":"overview/#52-deployment-and-productionization","title":"5.2 Deployment and Productionization","text":"<p>Once a model has been trained and evaluated satisfactorily, the Deploy and Optimize features of the toolbox help prepare it for efficient inference. The native format for models trained in PyTorch is the .pt file, which contains the model architecture and its learned weights. While flexible for training, this format is not always the most efficient for prediction.</p> <p>The optimization process, often referred to as productionization, involves converting the .pt model into a format optimized for inference, such as ONNX (Open Neural Network Exchange) or NVIDIA's TensorRT. These formats can perform graph optimizations, fuse operations, and utilize lower-precision arithmetic to dramatically accelerate prediction speeds and reduce the model's memory footprint without a significant loss in accuracy. This step is analogous to the compilation process required to run models on specialized hardware like the Google Coral Edge TPU, but is applied here for deployment on standard CPUs or GPUs.</p>"},{"location":"overview/#section-6-practical-implementation-guide","title":"Section 6: Practical Implementation Guide","text":"<p>This section provides a concise, practical guide for installing the CoralNet-Toolbox and highlights its crucial role as an interoperability hub that connects various tools and platforms within the marine science analysis ecosystem.</p>"},{"location":"overview/#61-system-requirements-and-installation","title":"6.1 System Requirements and Installation","text":"<p>To ensure a stable and conflict-free environment, it is highly recommended to install the CoralNet-Toolbox within a dedicated Conda virtual environment. This isolates the toolbox and its specific dependencies from other Python projects on the system.</p> <p>The installation process follows these steps:</p> <ol> <li>Create and Activate a Conda Environment: Open a terminal or Anaconda Prompt and execute the following commands. A Python 3.10 environment is recommended.</li> </ol> <pre><code># Create the environment named 'coralnet10' with Python 3.10\nconda create --name coralnet10 python=3.10 -y\n\n# Activate the newly created environment\nconda activate coralnet10\n</code></pre> <ol> <li>Install the CoralNet-Toolbox: The toolbox can be installed from the Python Package Index (PyPI) using pip or the faster uv package installer.</li> </ol> <pre><code># Install the toolbox using pip\npip install coralnet-toolbox\n</code></pre> <ol> <li>Install PyTorch with GPU Support (Recommended): For users with an NVIDIA GPU, installing the CUDA-enabled version of PyTorch is essential for achieving acceptable performance in model training and inference. Training on a CPU is possible but can be prohibitively slow. The specific command depends on the user's CUDA version. For example, for CUDA 11.8, the installation would involve</li> </ol> <pre><code># Example installation for PyTorch with CUDA 11.8 support\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>The toolbox provides helpful visual cues in its interface to indicate the available hardware acceleration. A \ud83d\udc22 icon signifies CPU-only operation, a \ud83d\udc07 icon indicates a single CUDA-enabled GPU is detected, a \ud83d\ude80 icon means multiple GPUs are available, and an \ud83c\udf4e icon is shown for Macs with Metal support.</p> <ol> <li>Run the Toolbox: Once installed, the application can be launched from the command line within the activated Conda environment</li> </ol> <pre><code>coralnet-toolbox\n</code></pre>"},{"location":"overview/#62-the-toolbox-as-an-interoperability-hub","title":"6.2 The Toolbox as an Interoperability Hub","text":"<p>Beyond its standalone capabilities, one of the most powerful strategic functions of the CoralNet-Toolbox is its role as a \"glue\" that connects disparate systems and breaks down data silos in the marine science analysis workflow. The challenges of integrating different tools and standardizing procedures are significant hurdles in the field, and the toolbox is explicitly designed to address them.</p> <p>This interoperability is demonstrated through its extensive import and export functionalities, which allow for a seamless flow of data between platforms. Research papers and the developer's work show complex, multi-platform workflows enabled by the toolbox, such as invoking the CoralNet Deploy API from within the TagLab annotation software via the toolbox's interface. This establishes the toolbox as a central nexus for data conversion and management.</p> <p>A typical interoperable workflow might look like this:</p> <ol> <li>Ingest Data from CoralNet: A researcher downloads a public image source with existing point annotations from the CoralNet website directly through the toolbox's interface.</li> <li>Create Advanced Annotations: The researcher uses the toolbox's advanced features, such as the SAM integration, to convert the sparse point annotations into rich polygon masks for instance segmentation.</li> <li>Export for Training: The newly created polygon dataset is exported in the specific YOLO format required for training a custom instance segmentation model locally.</li> <li>Export for External Analysis: After inference, the results (e.g., predicted polygons) can be exported in standard formats like GeoJSON. This allows the data to be easily imported into Geographic Information System (GIS) software for spatial analysis or into other visualization tools for further investigation.</li> </ol> <p>This ability to fluidly move and transform data between specialized platforms\u2014from the cloud-based repository of CoralNet, to the local training environment of the toolbox, and out to external analysis software\u2014is key to enabling next-generation, integrated ecological analysis.</p>"},{"location":"overview/#section-7-ecosystem-integration-case-studies-and-future-directions","title":"Section 7: Ecosystem Integration, Case Studies, and Future Directions","text":"<p>The CoralNet-Toolbox does not exist in a vacuum; it is part of a rapidly evolving ecosystem of tools and methodologies aimed at leveraging artificial intelligence for marine conservation. By understanding the broader trends in the field, we can appreciate its significance and anticipate future developments.</p>"},{"location":"overview/#71-the-broader-trend-from-centralized-services-to-empowered-researchers","title":"7.1 The Broader Trend: From Centralized Services to Empowered Researchers","text":"<p>The emergence and evolution of the CoralNet ecosystem reflect a significant maturation in the field of computational marine science. This trend represents a shift away from a reliance on centralized, one-size-fits-all AI services towards a new paradigm where individual researchers are empowered with flexible, powerful, and locally-controlled toolkits to build custom solutions for their unique scientific questions.</p> <p>When CoralNet was first conceived, the significant compute resources, large annotated datasets, and specialized expertise required for deep learning were not widely accessible to most ecologists. A centralized, web-based service was a necessary and brilliant solution to democratize access to this technology. The success of this model led to the creation of a massive, invaluable repository of annotated benthic imagery and cultivated a global user base familiar with AI-assisted analysis.</p> <p>Simultaneously, the broader technology landscape was changing. Open-source deep learning frameworks like PyTorch became mature and easy to use, state-of-the-art models like the YOLO series were made publicly available, and powerful hardware like consumer-grade GPUs became more affordable and widespread.</p> <p>The CoralNet-Toolbox was born at the confluence of these trends. It leverages the rich data legacy of the official CoralNet platform while harnessing the power and flexibility of modern, open-source ML technology. This shift is transformative. It moves researchers from being passive users of a service to active builders of their own analytical tools. It enables them to conduct more sophisticated, customized, and, critically, more reproducible research, as they have full control and documentation of their entire analytical pipeline. The proliferation of related open-source projects on platforms like GitHub for coral reef analysis is a testament to this new era of empowered, community-driven science.</p>"},{"location":"overview/#72-future-directions-and-conclusion","title":"7.2 Future Directions and Conclusion","text":"<p>The CoralNet-Toolbox continues to be actively developed, with several planned features that promise to further enhance its capabilities. These include the integration of a \"Model Zoo\" for easily downloading pre-trained models, the addition of automatic classification of annotations using vision-language models like BioCLIP, and the implementation of tiled inference for efficiently processing very large-area orthoimages.</p> <p>In conclusion, the CoralNet-Toolbox stands as an indispensable instrument for the modern benthic researcher. It successfully addresses the limitations of the foundational CoralNet platform by providing a robust, flexible, and locally-controlled environment for advanced object detection and instance segmentation. By integrating state-of-the-art models like YOLOv8 and revolutionary annotation accelerators like SAM, it dramatically lowers the barrier to entry for sophisticated quantitative analysis. More than just a standalone application, it functions as a critical interoperability hub, enabling a seamless flow of data between platforms and empowering scientists to build transparent and reproducible workflows. As coral reef ecosystems face mounting pressures, tools like the CoralNet-Toolbox that enable faster, deeper, and more scalable analysis are not just a matter of academic interest\u2014they are essential for the future of marine conservation.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#overview","title":"Overview","text":"<p>The CoralNet Toolbox is a Python application built using PyQt5 for image annotation. This guide provides instructions on how to use the application, including key functionalities and hotkeys.</p>"},{"location":"usage/#annotations","title":"Annotations","text":"<ul> <li>PatchAnnotation: Represents a patch annotation.</li> <li>RectangleAnnotation: Represents a rectangular annotation.</li> <li>PolygonAnnotation: Represents a polygonal annotation.</li> <li>MultiPolygonAnnotation: Represents multiple, non-overlapping polygonal annotations.</li> </ul>"},{"location":"usage/#computer-vision-tasks","title":"Computer Vision Tasks","text":"<ul> <li>Classification: Assign a label to an image (Patch).</li> <li>Detection: Detect objects in an image (Rectangle).</li> <li>Segmentation: Segment objects in an image (Polygon).</li> </ul>"},{"location":"usage/#thresholds-for-computer-vision-tasks","title":"Thresholds for Computer Vision Tasks","text":"<ul> <li>Patch Size: Adjust the patch size in the status bar.</li> <li>Uncertaint Threshold: Adjust the uncertainty threshold in the status bar.</li> <li>IoU Threshold: Adjust the IoU threshold in the status bar.</li> <li>Area Threshold: Adjust the min and max area threshold in the status bar.</li> </ul>"},{"location":"usage/#main-window","title":"Main Window","text":"<p>The main window consists of several components: - Menu Bar: Contains import, export, and other actions. - Tool Bar: Contains tools for selection and annotation. - Status Bar: Displays the image size, cursor position, view extent, annotation transparency, and thresholds. - Annotation Window: Displays the image and annotations. - Label Window: Lists and manages labels. - Image Window: Displays imported images. - Confidence Window: Displays cropped images and confidence charts.</p>"},{"location":"usage/#menu-bar-actions","title":"Menu Bar Actions","text":"<ul> <li>New Project: Reload CoralNet-Toolbox (loss of data warning).</li> <li>Open Project: Open an existing CoralNet-Toolbox project JSON file.</li> <li> <p>Save Project: Save current CoralNet-Toolbox project to JSON file.</p> </li> <li> <p>Import:</p> </li> <li>Import Images: Load image files.</li> <li>Import Frames: Load video frames (Currently not available).</li> <li>Import Labels (JSON): Load label data from a JSON file.</li> <li>Import CoralNet Labels (CSV): Load label data from a CoralNet CSV file.</li> <li>Import TagLab Labels (JSON): Load label data from a TagLab JSON file.</li> <li>Import Annotations (JSON): Load annotation data from a JSON file.</li> <li>Import CoralNet Annotations: Load annotation data from a CoralNet CSV file.</li> <li>Import TagLab Annotations: Load annotation data from a TagLab JSON file.</li> <li>Import Viscore Annotations: Load annotation data from a Viscore CSV file.</li> <li> <p>Import Dataset: Import a YOLO dataset for machine learning (Detection, Segmentation).</p> </li> <li> <p>Export:</p> </li> <li>Export Labels (JSON): Save label data to a JSON file.</li> <li>Export TagLab Labels (JSON): Save label data to a TagLab JSON file.</li> <li>Export Annotations (JSON): Save annotation data to a JSON file.</li> <li>Export GeoJSON Annotations: Save annotations to GeoJSON file (only for GeoTIFFs with CRS and Transforms data).</li> <li>Export Mask Annotations (Raster): Save annotations as segmentation masks.</li> <li>Export CoralNet Annotations: Save annotation data to a CoralNet CSV file.</li> <li>Export TagLab Annotations: Save annotation data to a TagLab JSON file.</li> <li>Export Viscore Annotations: Save annotation data to a Viscore CSV file.</li> <li> <p>Export Dataset: Create a YOLO dataset for machine learning (Classification, Detection, Segmentation).</p> </li> <li> <p>Explorer:</p> </li> <li>Annotation Settings: Select images, annotation types, and labels to include / filter; press apply.</li> <li>Model Settings: Use Color Features, a pre-trained, or existing classification model.<ul> <li>Feature Mode: Embeddings (before classification), or Predictions (after classification).</li> </ul> </li> <li>Embedding Settings: Map high-dimensional features to 2D space using PCA, TSNE, or UMAP.</li> <li>Annotation Viewer: View, select, modify labels of annotations.<ul> <li>Controls:</li> <li>Left-Click: Select an annotation;</li> <li>Ctrl + Left-Click: Select multiple annotations;</li> <li>Shift + Left-Click: Select in-between annotations;</li> <li>Double Left Click: Unselect all annotations, exit from Isolation View;</li> <li>Ctrl + Right-Click: Update Annotation Window view, zoomed and centered on selected annotation.</li> <li>Toolbar:</li> <li>Isolate Selection: Subset view;</li> <li>Sort By: Sort annotations by image name, label, confidence;</li> <li>Find Similar: Selects and isolates N nearest annotations to currently selected;</li> <li>Size: Slider-bar to control annotation size in Annotation Viewer.</li> </ul> </li> <li> <p>Embedding Viewer: Selected, modify labels of annotations.</p> <ul> <li>Controls:</li> <li>Left-Click: Select an annotation;</li> <li>Ctrl + Left-Click: Select multiple annotations;</li> <li>Ctrl + Left-Click + Drag: Select multiple annotations within a draw rectangle;</li> <li>Double Left Click: Unselect all annotations, exit from Isolation View;</li> <li>Right-Click + Drag: Pan around Embedding Viewer;</li> <li>Scroll-wheel: Zoom in and out of Embedding Viewer.</li> <li>Toolbar:</li> <li>Isolate Selection: Subset view;</li> <li>Find Potential Mislabels: Select potentially incorrectly labeled annotations, based on location;</li> <li>Review Uncertain: Select annotations with lower Top-1 confidence scores (requires Predictions);</li> <li>Find Duplicates: Select annotations that are likely duplicates of another (only selects the duplicates); </li> <li>Home: Resets the Embedding Viewer zoom level.   Tip: </li> <li>Use dual monitors to assess selected annotations in Annotation Viewer, in the Annotation Window;</li> </ul> </li> <li> <p>Sample:</p> </li> <li> <p>Sample Annotations: Automatically generate Patch annotations.</p> <ul> <li>Sampling Method: Choose between Random, Stratified Random, or Uniform distribution.</li> <li>Number of Annotations: Specify how many annotations to generate.</li> <li>Annotation Size: Set the size of the generated patch annotations.</li> <li>Label As: Choose which label to assign to generated annotations.</li> <li>Apply Options: Apply sampling to current image, filtered images, previous/next images, or all images.</li> <li>Exclude Regions: Option to prevent sampling in areas with existing annotations.</li> <li>Margins: Define image boundary constraints for sampling:</li> <li>Set margins in pixels or percentage</li> <li>Configure different values for top, right, bottom, and left edges</li> <li>Annotations will only be placed within these margins</li> </ul> </li> <li> <p>Tile:</p> </li> <li>Tile Dataset: Tile existing Classification, Detection or Segmention datasets using <code>yolo-tiling</code>.</li> <li>Tile Creation: Pre-compute multiple work areas for selected images.</li> <li> <p>Tile Batch Inferece: Apply inference on all imagues with work areas.</p> </li> <li> <p>CoralNet: </p> </li> <li>Authenticate: Authenticate with CoralNet.<ul> <li>Enter your CoralNet username and password to access your sources.</li> <li>Authentication is required before downloading any CoralNet data.</li> </ul> </li> <li> <p>Download: Download data from CoralNet.</p> <ul> <li>Source ID: Enter the Source ID (or multiple IDs separated by commas).</li> <li>Output Directory: Select where to save downloaded files.</li> <li>Download Options: Choose what to download:</li> <li>Metadata: Source information and settings</li> <li>Labelset: All available labels from the source</li> <li>Annotations: Point annotations with their labels</li> <li>Images: Original images from the source</li> <li>Parameters: Configure download settings:</li> <li>Image Fetch Rate: Time between image downloads (seconds)</li> <li>Image Fetch Break Time: Pause duration between batches (seconds)</li> <li>Debug Mode: Toggle headless browser mode for troubleshooting.</li> </ul> </li> <li> <p>Ultralytics:</p> </li> <li>Merge Datasets: Merge multiple Classification datasets.</li> <li>Tune Model: Identify ideal hyperparameter values before fully training a model.</li> <li>Train Model: Train a machine learning model.</li> <li>Evaluate Model: Evaluate a trained model.</li> <li>Optimize Model: Convert model format.</li> <li>Deploy Model: Make predictions using a trained model (Classification, Detection, Segmentation).</li> <li>Batch Inference: Perform batch inferences.</li> <li> <p>Video Inference: Perform inferencing on videos in real-time, view analytics.</p> </li> <li> <p>SAM:</p> </li> <li>Deploy Predictor: Deploy <code>EdgeSAM</code>, <code>MobileSAM</code>, <code>SAM</code>, etc, to use interactively (points, box).</li> <li>Deploy Generator: Deploy <code>FastSAM</code> to automatically segment the image.</li> <li> <p>Batch Inference: Perform batch inferencing using <code>FastSAM</code>.</p> </li> <li> <p>See Anything (YOLOE):</p> </li> <li>Deploy Predictor: Deploy a <code>YOLOE</code> model to use interactively within the same image;</li> <li>Deploy Generator: Deploy a <code>YOLOE</code> model to use like a detector / segmentor; reference other images and labels;<ul> <li>Select the <code>YOLOE</code> model, parameters, and load it;</li> <li>Choose a reference label, then select the image(s) containing reference annotations (must be rectangles or polygons);</li> <li>Use the loaded model on images, or work areas, and with batch inferencing;</li> </ul> </li> <li> <p>Batch Inference: Perform batch inferencing using loaded <code>YOLOE</code> generator.</p> </li> <li> <p>AutoDistill:</p> </li> <li>Deploy Model: Deploy a foundational model<ul> <li>Models Available: <code>Grounding DINO</code>, <code>OWLViT</code></li> </ul> </li> <li>Batch Inference: Perform batch inferences.</li> </ul>"},{"location":"usage/#tool-bar-tools","title":"Tool Bar Tools","text":"<ul> <li>Select Tool: After selecting the tool</li> <li>Left-Click: Select an annotation; drag to move it.</li> <li>Ctrl + Left-Click: Add/remove annotation to current selection.</li> <li>Ctrl + Delete / Backspace: Remove selected annotation(s).</li> <li>Ctrl + Drag: Create rectangle selection to select multiple annotations.</li> <li>Ctrl + Mouse Wheel: Change size of the selected annotation.</li> <li>Ctrl + Shift: Show resize handles for the selected annotation.</li> <li>Ctrl + Shift + Mouse Wheel: Change the number of vertices for a polygon annotation.</li> <li>Ctrl + Space: Confirm prediction for selected annotation with top machine confidence.</li> <li>Ctrl + X: Enter cutting mode; left-click to start, draw line, left-click to end.</li> <li>Backspace/Ctrl + X: Cancel cutting mode.<ul> <li>MultiPolygonAnnotations: Break apart each PolygonAnnotation.    </li> </ul> </li> <li> <p>Ctrl + C: Combine multiple selected annotations (if same type and label).</p> <ul> <li>Combining Rules: </li> <li>All selected annotations must have the same label.</li> <li>All selected annotations must be verified (not machine predictions).</li> <li>RectangleAnnotations can only be combined with other rectangles.</li> <li>PatchAnnotations can be combined with other patches or polygons to create polygons.</li> <li>PolygonAnnotations can be combined with other overlapping polygons to create a polygon.</li> <li>MultiPolygonAnnotations can be made with multiple non-overlapping polygons.</li> </ul> </li> <li> <p>Patch Tool: After selecting the tool</p> </li> <li>Left-Click: Add a patch annotation at the clicked position.</li> <li>Ctrl + Mouse Wheel: Adjust the patch size up or down.</li> <li> <p>Mouse Movement: Shows a semi-transparent preview of the patch at the cursor position.</p> </li> <li> <p>Rectangle Tool: After selecting the tool</p> </li> <li>Left-Click: Start drawing a rectangle; click again to finish.</li> <li>Mouse Movement: Shows a preview of the rectangle while drawing.</li> <li> <p>Backspace: Cancel the current rectangle annotation.</p> </li> <li> <p>Polygon Tool: After selecting the tool</p> </li> <li>Left-Click (first): Start drawing a polygon.</li> <li>Left-Click (subsequent): Add points to the polygon; click near the first point to close.</li> <li>Ctrl + Left-Click: Enable straight line mode; click to add straight line segments.</li> <li>Mouse Movement: Shows a preview of the polygon as you draw.</li> <li> <p>Backspace: Cancel the current polygon annotation.</p> </li> <li> <p>SAM Tool: After a model is loaded</p> </li> <li>Left-Click: Start drawing a work area; click again to finish drawing.</li> <li>Backspace: Cancel drawing the current work area.</li> <li> <p>Space: Create a work area from the current view.</p> <ul> <li>Space: Set working area; confirm prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Ctrl + Left-Click: Add positive point.</li> <li>Ctrl + Right-Click: Add negative point.</li> <li>Backspace: Discard unfinalized predictions.</li> </ul> </li> <li> <p>See Anything (YOLOE) Tool: After a model is loaded</p> </li> <li>Left-Click: Start drawing a work area; click again to finish drawing.</li> <li>Backspace: Cancel drawing the current work area.</li> <li> <p>Space: Create a work area from the current view.</p> <ul> <li>Space: Set working area; run prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Backspace: Discard unfinalized predictions.</li> </ul> </li> <li> <p>Work Area Tool: For creating restricted areas for model prediction</p> </li> <li>Left-Click: Start drawing a work area; click again to finish drawing.</li> <li>Backspace: Cancel drawing the current work area.</li> <li>Space: Create a work area from the current view.</li> <li>Ctrl + Alt: Create temporary work area from current view (disappears when keys released / pressed again).</li> <li>Ctrl + Shift: Show removal buttons on existing work areas (click the \"X\" to remove).</li> <li>Ctrl + Shift + Backspace: Remove all work areas in the current image.</li> <li>Practical Use:<ul> <li>Define specific regions where models should make predictions.</li> <li>Useful for processing only relevant parts of large images.</li> <li>Work areas persist between tool changes and sessions.</li> </ul> </li> </ul>"},{"location":"usage/#status-bar","title":"Status Bar","text":"<ul> <li>Image Size: Displays the image size.</li> <li>Cursor Position: Displays the cursor position.</li> <li>View Extent: Displays the view extent.</li> <li>Annotation Visibility: Show / Hide all existing annotations.</li> <li>Annotation Transparency: Adjust the annotation transparency.</li> <li>Select All Labels: Select all labels, adjusting transparency for all labels.</li> <li>Unselect All Labels: Unselect all labels, adjusting transparency for only selected labels.</li> <li>Patch Size: Manipulate Patch Size (only active when using Patch Tool).</li> <li>Parameters: Adjust parameters including uncertainty, IoU, and area thresholds.</li> </ul>"},{"location":"usage/#annotation-window","title":"Annotation Window","text":"<ul> <li>Zoom: Use the mouse wheel to zoom in and out.</li> <li>Pan: Right-click and hold the mouse button to pan the image.</li> </ul>"},{"location":"usage/#label-window","title":"Label Window","text":"<ul> <li>Move Label: Right-click and drag to move labels.</li> <li>Add Label: Click the \"Add Label\" button to add a new label.</li> <li>Delete Label: Click the \"Delete Label\" button to delete the selected label.</li> <li>Edit Label: Click the \"Edit Label\" button to edit the selected label.</li> <li>Lock Label: Click the \"Lock Label\" button to lock the selected label.</li> <li>Filter Labels: Use the filter text box to search for specific labels.</li> <li>Label Count: Displays the total number of labels in the project.</li> <li>Annotation Count: Shows information about the current annotations:</li> <li>When no annotation is selected: Shows the total count of annotations.</li> <li>When a single annotation is selected: Shows the selected annotation's index.</li> <li>When multiple annotations are selected: Shows how many annotations are selected.</li> <li>Can be edited (when in select mode) to navigate to a specific annotation by index.</li> </ul>"},{"location":"usage/#image-window","title":"Image Window","text":"<ul> <li>Select Image: Double-click on a row to select and load the image in the annotation window.</li> <li>Highlight Image: Single-click on a row to highlight one or more rows in the image window.</li> <li>Ctrl + Left-click: Select multiple, non-adjacent rows.</li> <li>Shift + Left-click: Select multiple, adjacent rows.</li> <li>Open Context Menu:</li> <li>Right-click on a single highlighted row: Delete images / annotations for the highlighted row.</li> <li>Shift + Right-click on multiple highlighted rows: Delete images / annotations for highlighted rows.</li> <li>Search / Filter:</li> <li>By Image: Filter for images by name or sub-string.</li> <li>By Label: Filter images by labels they contain.</li> <li>No Annotations: Filter images with no annotations.</li> <li>Has Annotations: Filter images with annotations.</li> <li>Has Predictions: Filter images with predictions.</li> <li>Highlighted: Filter highlighted images.</li> <li>Navigation:</li> <li>Home Button: Click to center the table on the currently selected image.</li> <li>Highlight All: Highlight all images in the current filtered view.</li> <li>Unhighlight All: Unhighlight all images in the current filtered view.</li> <li>Image Preview:</li> <li>Tool Tip: Hover over a row to show image metadata.</li> <li>Thumbnail: Hold Ctrl while hovering over a row to show a thumbnail.</li> </ul>"},{"location":"usage/#confidence-window","title":"Confidence Window","text":"<ul> <li>Display Cropped Image: Shows the cropped image of the selected annotation.</li> <li>The dimensions shown include both original and scaled sizes when applicable.</li> <li>The border of the image is highlighted with the color of the top confident label.</li> <li>Confidence Chart: Displays a bar chart with confidence scores.</li> <li>Top 5 Predictions: Shows up to 5 predictions with their confidence scores.</li> <li>Prediction Selection: Click on any confidence bar to change the annotation's label.</li> <li>Numerical Keys: Press keys 1-5 to quickly select from the top 5 predictions.</li> <li>Confidence Mode Toggle: </li> <li>Click the icon button next to the dimensions to toggle between user and machine confidence views.</li> <li>User icon shows user-assigned confidence scores.</li> <li>Machine icon shows model-predicted confidence scores.</li> <li>The toggle is only enabled when both user and machine confidences are available.</li> <li>Visual Indicators:</li> <li>Each confidence bar shows the label color and confidence percentage.</li> <li>Numbered indicators (1-5) show the rank of each prediction.</li> <li>Hover over confidence bars to see a pointing hand cursor when selection is possible.</li> </ul>"},{"location":"usage/#secret-hotkeys","title":"Secret Hotkeys","text":"<ul> <li>Escape: Exit the program.</li> <li>Alt + Up/Down: Cycle through images.</li> <li>Ctrl + W/A/S/D: Cycle through labels.</li> <li>Ctrl + Left/Right: Cycle through annotations.</li> <li>Ctrl + Shift + &lt;: Select all annotations.</li> <li>Ctrl + Shift + &gt;: Unselect all annotations.</li> <li>Ctrl + Alt: Switch from SelectTool to the Annotation tool that matches the currently selected (1) Annotation (example below)</li> <li>When a PatchAnnotation is selected, this switches back to the PatchTool</li> <li> <p>When the PatchTool is active, this switches back to the SelectTool</p> </li> <li> <p>Machine Learning, SAM, and AutoDistill: After a model is loaded</p> </li> <li>Ctrl + 1: Make prediction on selected Patch annotation, else all in the image with Review label.</li> <li>Ctrl + 2: Make predictions using Object Detection model.</li> <li>Ctrl + 3: Make predictions using Instance Segmentation model.</li> <li>Ctrl + 4: Make predictions using FastSAM model.</li> <li>Ctrl + 5: Make predictions using YOLOE model.</li> <li> <p>Ctrl + 6: Make predictions using AutoDistill model.</p> </li> <li> <p>Tooltips: Hover over tool buttons, image / annotation rows for information about them.</p> </li> </ul>"}]}