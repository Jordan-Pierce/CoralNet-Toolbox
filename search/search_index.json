{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CoralNet-Toolbox \ud83e\udeb8\ud83e\uddf0","text":"[![python-version](https://img.shields.io/pypi/pyversions/CoralNet-Toolbox.svg)](https://pypi.org/project/CoralNet-Toolbox) [![version](https://img.shields.io/pypi/v/CoralNet-Toolbox.svg)](https://pypi.python.org/pypi/CoralNet-Toolbox) [![pypi-passing](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![windows](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/windows.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![macos](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/macos.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox) [![ubuntu](https://github.com/Jordan-Pierce/CoralNet-Toolbox/actions/workflows/ubuntu.yml/badge.svg)](https://pypi.org/project/CoralNet-Toolbox)  \ud83d\udd0d Annotation <p>Create patches, rectangles, and polygons with AI assistance</p> \ud83e\udde0 AI-Powered <p>Leverage SAM, YOLOE, and various foundation models</p> \ud83d\ude80 Complete Workflow <p>From data collection to model training and deployment</p>"},{"location":"#quick-start","title":"\ud83d\udea6 Quick Start","text":"<p>Running the following command will install the <code>coralnet-toolbox</code>, which you can then run from the command line: <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n\n# Run\ncoralnet-toolbox\n</code></pre></p>"},{"location":"#guides","title":"\ud83d\udcda Guides\ud83c\udfa5 Watch the Video Demos","text":"<p>For further instructions please see the following guides: - Installation - Usage - Patch-based Image Classifier</p> <p> </p>"},{"location":"#tldr","title":"\u23e9 TL;Dr","text":"<p>The <code>CoralNet-Toolbox</code> is an unofficial codebase that can be used to augment processes associated with those on CoralNet.</p> <p>It uses\u2728<code>Ultralytics</code>\ud83d\ude80 as a  base, which is an open-source library for computer vision and deep learning built in <code>PyTorch</code>. For more information on their <code>AGPL-3.0</code> license, see here.</p>"},{"location":"#supported-models","title":"\ud83d\ude80 Supported Models","text":"<p>The <code>toolbox</code> integrates a variety of state-of-the-art models to help you create rectangle and polygon annotations efficiently. Below is a categorized overview of the supported models and frameworks:</p>   | Category                | Models                                                                                       | |-------------------------|---------------------------------------------------------------------------------------------------------| | **Trainable**           | - \ud83e\uddbe [YOLOv3](https://docs.ultralytics.com/models/)  - \ud83e\udd88 [YOLOv4](https://docs.ultralytics.com/models/)  - \ud83e\udd85 [YOLOv5](https://docs.ultralytics.com/models/)  - \ud83d\udc2c [YOLOv6](https://docs.ultralytics.com/models/)  - \ud83d\udc22 [YOLOv7](https://docs.ultralytics.com/models/)  - \ud83d\udc19 [YOLOv8](https://docs.ultralytics.com/models/)  - \ud83d\udc20 [YOLOv9](https://docs.ultralytics.com/models/)  - \ud83e\udd91 [YOLOv10](https://docs.ultralytics.com/models/)  - \ud83d\ude80 [YOLO11](https://docs.ultralytics.com/models/)  - \ud83d\udc33 [YOLO12](https://docs.ultralytics.com/models/) | | **Segment Anything**    | - \ud83e\udeb8 [SAM](https://github.com/facebookresearch/segment-anything)  - \ud83c\udf0a [CoralSCOP](https://github.com/zhengziqiang/CoralSCOP)  - \u26a1 [FastSAM](https://github.com/CASIA-IVA-Lab/FastSAM)  - \ud83d\udd01 [RepViT-SAM](https://github.com/THU-MIG/RepViT)  - \u2702\ufe0f [EdgeSAM](https://github.com/chongzhou96/EdgeSAM)  - \ud83d\udcf1 [MobileSAM](https://github.com/ChaoningZhang/MobileSAM) | | **Visual Prompting**    | - \ud83d\udc41\ufe0f [YOLOE](https://github.com/THU-MIG/yoloe)  - \ud83e\udd16 [AutoDistill](https://github.com/autodistill):  \u00a0\u00a0\u00a0\u2022 \ud83e\udd92 Grounding DINO  \u00a0\u00a0\u00a0\u2022 \ud83e\udd89 OWLViT  \u00a0\u00a0\u00a0\u2022 \u26a1 OmDetTurbo |   <p>These models enable fast, accurate, and flexible annotation workflows for a wide range of use cases for patch-based image classification, object detection, instance segmentation.</p>"},{"location":"#toolbox-features","title":"\ud83d\udee0\ufe0f Toolbox Features","text":"| ![Patch Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Patches.gif)<sub>**Patch Annotation**</sub> | ![Rectangle Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Rectangles.gif)<sub>**Rectangle Annotation**</sub> | ![Polygon Annotation Tool](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Polygons.gif)<sub>**Polygon Annotation**</sub> | |:--:|:--:|:--:| | ![Patch-based Image Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classification.gif)<sub>**Image Classification**</sub> | ![Object Detection](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Object_Detection.gif)<sub>**Object Detection**</sub> | ![Instance Segmentation](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Instance_Segmentation.gif)<sub>**Instance Segmentation**</sub> | | ![Segment Anything Model (SAM)](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Segment_Anything.gif)<sub>**Segment Anything (SAM)**</sub> | ![Polygon Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classifying_Polygons.gif)<sub>**Polygon Classification**</sub> | ![Patch-based LAI Classification](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Classifying_Orthomosaics.gif)<sub>**Patch-based LAI Classification**</sub> | | ![Cut](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Cut.gif)<sub>**Cut**</sub> | ![Combine](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Combine.gif)<sub>**Combine**</sub> | ![See Anything (YOLOE)](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/See_Anything.gif)<sub>**See Anything (YOLOE)**</sub> | |  | ![Region-based Detection](https://raw.githubusercontent.com/Jordan-Pierce/CoralNet-Toolbox/refs/heads/main/figures/tools/Work_Areas.gif)<sub>**Region-based Detection**</sub> |  |   <p>Enhance your CoralNet experience with these tools: - \ud83d\udce5 Download: Retrieve Source data (images and annotations) from CoralNet - \ud83c\udfac Rasters: Import images, or extract frames directly from video files - \u270f\ufe0f Annotate: Create annotations freely - \ud83d\udc41\ufe0f Visualize: See CoralNet and CPCe annotations superimposed on images - \ud83d\udd2c Sample: Sample patches using various methods (Uniform, Random, Stratified) - \ud83e\udde9 Patches: Create patches (points) - \ud83d\udd33 Rectangles: Create rectangles (bounding boxes) - \ud83d\udfe3 Polygons: Create polygons (instance masks) - \u270d\ufe0f Edit: Cut and Combine polygons and rectangles - \ud83e\uddbe SAM: Use <code>FastSAM</code>, <code>CoralSCOP</code>, <code>RepViT-SAM</code>, <code>EdgeSAM</code>, <code>MobileSAM</code>, and <code>SAM</code> to create polygons   - Uses <code>xSAM</code> - \ud83d\udc40 YOLOE (See Anything): Detect similar appearing objects using visual prompts automatically - \ud83e\uddea AutoDistill: Use <code>AutoDistill</code> to access the following for creating rectangles and polygons:   - Uses <code>Grounding DINO</code>, <code>OWLViT</code>, <code>OmDetTurbo</code> - \ud83e\udde0 Train: Build local patch-based classifiers, object detection, and instance segmentation models - \ud83d\udd2e Deploy: Use trained models for predictions - \ud83d\udcca Evaluation: Evaluate model performance - \ud83d\ude80 Optimize: Productionize models for faster inferencing - \u2699\ufe0f Batch Inference: Perform predictions on multiple images, automatically - \u2194\ufe0f I/O: Import and Export annotations from / to CoralNet, Viscore, and TagLab   - Export annotations as GeoJSONs, segmentation masks - \ud83d\udcf8 YOLO: Import and Export YOLO datasets for machine learning - \ud83e\uddf1 Tile Dataset: Tile existing Detection / Segmentation datasets   - Uses <code>yolo-tiling</code></p>"},{"location":"#todo","title":"\ud83d\udcdd TODO","text":"<ul> <li>\ud83e\udd17 Model Zoo: Download <code>Ultralytics</code> models from <code>HuggingFace</code> for use in <code>toolbox</code></li> <li>\ud83e\udd8a BioCLIP, MobileCLIP (AutoDistill): Automatically classify annotations</li> <li>\ud83d\udce6 Toolshed: Access tools from the old repository</li> </ul>"},{"location":"classify/","title":"Classify","text":"<p>A big thanks to the researchers at the research arm of the Seattle Aquarium for providing this guide.</p> <pre><code>@misc{williams2025,\n  author = {Williams, Megan},\n  title = {SOP to train a classification model in Toolbox},\n  institution = {Seattle Aquarium},\n  date = {2025-04-04}\n}\n</code></pre>"},{"location":"classify/#sop-to-train-a-classification-model-in-toolbox","title":"SOP to train a classification model in Toolbox","text":"<p>The following steps are required to create a training dataset, train a classification model using Ultralytics YOLO in Toolbox, and apply the model to make predictions.</p>"},{"location":"classify/#1-toolbox-installation-and-setup","title":"1. Toolbox installation and setup","text":"<ul> <li>Instructions for installing and running Toolbox can be found in the documentation linked here.</li> </ul>"},{"location":"classify/#2-prepare-training-and-testing-images","title":"2. Prepare training and testing images","text":"<ul> <li>Ensure images are color corrected and a quality desired for analysis.</li> <li>Split images into training and testing folder. Our technique is:</li> <li>Training folder: Move 2 out of every 3 images here.</li> <li>Testing folder: Move 1 out of every 3 images here.</li> </ul>"},{"location":"classify/#3-load-labelset","title":"3. Load labelset","text":"<ul> <li>Open Toolbox and import your classification labelset</li> <li>Go to Labelset \u2192 Import</li> <li>Our JSON labelset can be found here. You can also preview the labelset in Excel here.</li> </ul>"},{"location":"classify/#4-import-and-annotate-training-images","title":"4. Import and annotate training images","text":"<ul> <li>Import training images into Toolbox</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images</li> <li>Create image patches (classification annotations)</li> <li>Select a label in the lower label window.</li> <li>Choose the image patch tool (rectangle icon) from the toolbar on the left.</li> <li>In the annotation window (center window), left click the appropriate location in the image to add a patch for that label.</li> <li>Repeat for each label across your training images.</li> </ul>"},{"location":"classify/#5-export-classification-dataset","title":"5. Export classification dataset","text":"<ul> <li>After annotating, export the dataset:</li> <li>File \u2192 Export \u2192 Dataset \u2192 Classify</li> <li>Toolbox will generate a dataset directory containing train, validation, and test folders with labeled image patches.</li> </ul>"},{"location":"classify/#6-train-classification-model","title":"6. Train classification model","text":"<ul> <li>Start training a YOLO classification model</li> <li>Ultralytics \u2192 Train Model \u2192 Classify</li> <li>In the training window:</li> <li>Dataset: Click Browse and select the exported dataset folder.</li> <li>Model Selection: Choose the Ultralytics YOLO model that fits your needs (e.g., YOLOv8 or YOLOv11). A guide comparing model options is available here.</li> <li>Parameters:<ul> <li>Set the location where you want your trained model to be saved.</li> <li>You can use default training parameters or customize them. More information about the parameters can be found here.</li> </ul> </li> <li>Click OK to begin training. You can monitor training progress in the terminal.</li> </ul>"},{"location":"classify/#7-load-and-deploy-model","title":"7. Load and deploy model","text":"<ul> <li>After training completes:</li> <li>Go to Ultralytics \u2192 Deploy Model \u2192 Classify</li> <li>Under Actions, click Browse Model and select your trained weights file (best.pt).</li> <li>Click load model</li> </ul>"},{"location":"classify/#8-test-the-model-on-new-images","title":"8. Test the model on new images","text":"<ul> <li>Remove training images:</li> <li>In the image window, click \"Select All\", right-click and select delete all images to remove.</li> <li>Import test images:</li> <li>File \u2192 Import \u2192 Rasters \u2192 Images and choose the testing folder.</li> <li>Create random image patches:</li> <li>Click Sample at the top</li> <li>Set your desired sampling configuration (e.g., number of patches).</li> <li>Set Select Label to Review</li> <li>Check \"Apply to all images\" and click Accept</li> </ul>"},{"location":"classify/#9-run-predictions","title":"9. Run predictions","text":"<ul> <li>To predict label for the new image patches:</li> <li>For a single image: press Ctrl + 1</li> <li>For all images:<ul> <li>Ultralytics \u2192 Batch Inference \u2192 Classify</li> <li>Check \"Apply to all images\" and \"Predict review annotation\"</li> </ul> </li> </ul>"},{"location":"classify/#10-review-and-correct-predictions","title":"10. Review and correct predictions","text":"<ul> <li>Predicted labels appear for each review image patch.</li> <li>Confidence levels are shown in the Confidence window.</li> <li>To fix incorrect predictions:</li> <li>Select the image patch so that it is shown in the Confidence window</li> <li>Select the correct label in the label window</li> </ul>"},{"location":"classify/#11-export-and-improve-dataset","title":"11. Export and improve dataset","text":"<ul> <li>To analyze results:</li> <li>Export annotation file as a .csv file: File \u2192 Export \u2192 CSV</li> <li>To improve your model:</li> <li>Create a new dataset with the corrected predictions</li> <li>Merge this with the original dataset:<ul> <li>Ultralytics \u2192 Merge Datasets \u2192 Classify</li> <li>Set a name and location for the merged dataset</li> <li>Click \u2192 Add Dataset and select the datasets you want to combine</li> </ul> </li> </ul>"},{"location":"classify/#12-improve-existing-model","title":"12. Improve existing model","text":"<ul> <li>The merged dataset can now be used to train a new model following Step 6.</li> <li>Instead of selecting a new YOLO model, you can use your existing model:</li> <li>Under Model Selection switch to Use Existing Model</li> <li>Browse to the model weights (best.pt)</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#how-to-install","title":"How to Install","text":""},{"location":"installation/#anaconda","title":"Anaconda","text":"<p>It's recommended to use <code>Anaconda</code> to create an environment for the <code>toolbox</code>: <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n</code></pre></p>"},{"location":"installation/#install","title":"Install","text":"<p>Once this has finished, install the <code>toolbox</code> using <code>uv</code>: </p> <pre><code># cmd\n\n# Install uv first\npip install uv\n\n# Install with uv\nuv pip install coralnet-toolbox\n</code></pre> <p>Although fast, <code>uv</code> is still relatively new; if this fails, simply fall back to using <code>pip</code>:</p> <pre><code># cmd\n\n# Install\npip install coralnet-toolbox\n</code></pre>"},{"location":"installation/#cuda","title":"CUDA","text":"<p>If you have <code>CUDA</code>, you should install the versions of <code>cuda-nvcc</code> and <code>cudatoolkit</code> that you need, and then install the corresponding versions of <code>torch</code> and <code>torchvision</code>. Below is an example of how that can be done using <code>CUDA</code> version 11.8:</p> <pre><code># cmd\n\n# Example for CUDA 11.8\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n</code></pre> <p>If <code>CUDA</code> is installed on your computer, and <code>torch</code> was built with it properly, you should see a <code>\ud83d\udc07</code> icon in the <code>toolbox</code> instead of a <code>\ud83d\udc22</code>; if you have multiple <code>CUDA</code> devices available, you should see a <code>\ud83d\ude80</code> icon, and if you're using a Mac with <code>Metal</code>, you should see an <code>\ud83c\udf4e</code> icon (click on the icon to see the device information).</p> <p>See here for more details on versions for the following: - <code>cuda-nvcc</code> - <code>cudatoolkit</code> - <code>torch</code></p>"},{"location":"installation/#run","title":"Run","text":"<p>Finally, you can run the <code>toolbox</code> from the command line:</p> <pre><code># cmd\n\n# Run\ncoralnet-toolbox\n</code></pre>"},{"location":"installation/#github-repository","title":"GitHub Repository","text":"<p>If you prefer to clone the repository and run the <code>toolbox</code> from the source code, you can do so with the following:</p> <pre><code># cmd\n\n# Create and activate an environment\nconda create --name coralnet10 python=3.10 -y\nconda activate coralnet10\n\n# Install git via conda, if not already installed\nconda install git -y\n\n# Change to the desired directory (e.g., Documents)\ncd Documents\n\n# Clone and enter the repository\ngit clone https://github.com/Jordan-Pierce/CoralNet-Toolbox.git\ncd CoralNet-Toolbox\n\n# Install the latest\npip install -e .\n\n# Install CUDA requirements (if applicable)\nconda install nvidia/label/cuda-11.8.0::cuda-nvcc -y\nconda install nvidia/label/cuda-11.8.0::cuda-toolkit -y\n\n# Example for torch w/ CUDA 11.8\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 --upgrade\n\n# Run\ncoralnet-toolbox\n</code></pre> <p>To update your repository to match the current version on <code>main</code>, run <code>fetch</code> and <code>pull</code> commands:</p> <pre><code># cmd\n\n# Change to the proper directory\ncd Coralnet-Toolbox\n\n# Ask for the updates on main\ngit fetch\n\n# Pull the updates from main\ngit pull\n\n# Update your conda environment \npip install -e . -U\n</code></pre> <p>Or, if you want to simply install the <code>toolbox</code> from the GitHub repo directly you can also do the following:</p> <pre><code># cmd\n\npip install git+https://github.com/Jordan-Pierce/CoralNet-Toolbox.git@main -U\n\n# replace @main with a different branch if you want to test out experimental code\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#overview","title":"Overview","text":"<p>The CoralNet Toolbox is a Python application built using PyQt5 for image annotation. This guide provides instructions on how to use the application, including key functionalities and hotkeys.</p>"},{"location":"usage/#annotations","title":"Annotations","text":"<ul> <li>PatchAnnotation: Represents a patch annotation.</li> <li>RectangleAnnotation: Represents a rectangular annotation.</li> <li>PolygonAnnotation: Represents a polygonal annotation.</li> <li>MultiPolygonAnnotation: Represents multiple, non-overlapping polygonal annotations.</li> </ul>"},{"location":"usage/#computer-vision-tasks","title":"Computer Vision Tasks","text":"<ul> <li>Classification: Assign a label to an image (Patch).</li> <li>Detection: Detect objects in an image (Rectangle).</li> <li>Segmentation: Segment objects in an image (Polygon).</li> </ul>"},{"location":"usage/#thresholds-for-computer-vision-tasks","title":"Thresholds for Computer Vision Tasks","text":"<ul> <li>Patch Size: Adjust the patch size in the status bar.</li> <li>Uncertaint Threshold: Adjust the uncertainty threshold in the status bar.</li> <li>IoU Threshold: Adjust the IoU threshold in the status bar.</li> <li>Area Threshold: Adjust the min and max area threshold in the status bar.</li> </ul>"},{"location":"usage/#main-window","title":"Main Window","text":"<p>The main window consists of several components: - Menu Bar: Contains import, export, and other actions. - Tool Bar: Contains tools for selection and annotation. - Status Bar: Displays the image size, cursor position, view extent, annotation transparency, and thresholds. - Annotation Window: Displays the image and annotations. - Label Window: Lists and manages labels. - Image Window: Displays imported images. - Confidence Window: Displays cropped images and confidence charts.</p>"},{"location":"usage/#menu-bar-actions","title":"Menu Bar Actions","text":"<ul> <li>New Project: Reload CoralNet-Toolbox (loss of data warning).</li> <li>Open Project: Open an existing CoralNet-Toolbox project JSON file.</li> <li> <p>Save Project: Save current CoralNet-Toolbox project to JSON file.</p> </li> <li> <p>Import:</p> </li> <li>Import Images: Load image files.</li> <li>Import Frames: Load video frames (Currently not available).</li> <li>Import Labels (JSON): Load label data from a JSON file.</li> <li>Import CoralNet Labels (CSV): Load label data from a CoralNet CSV file.</li> <li>Import TagLab Labels (JSON): Load label data from a TagLab JSON file.</li> <li>Import Annotations (JSON): Load annotation data from a JSON file.</li> <li>Import CoralNet Annotations: Load annotation data from a CoralNet CSV file.</li> <li>Import TagLab Annotations: Load annotation data from a TagLab JSON file.</li> <li>Import Viscore Annotations: Load annotation data from a Viscore CSV file.</li> <li> <p>Import Dataset: Import a YOLO dataset for machine learning (Detection, Segmentation).</p> </li> <li> <p>Export:</p> </li> <li>Export Labels (JSON): Save label data to a JSON file.</li> <li>Export TagLab Labels (JSON): Save label data to a TagLab JSON file.</li> <li>Export Annotations (JSON): Save annotation data to a JSON file.</li> <li>Export GeoJSON Annotations: Save annotations to GeoJSON file (only for GeoTIFFs with CRS and Transforms data).</li> <li>Export Mask Annotations (Raster): Save annotations as segmentation masks.</li> <li>Export CoralNet Annotations: Save annotation data to a CoralNet CSV file.</li> <li>Export TagLab Annotations: Save annotation data to a TagLab JSON file.</li> <li>Export Viscore Annotations: Save annotation data to a Viscore CSV file.</li> <li> <p>Export Dataset: Create a YOLO dataset for machine learning (Classification, Detection, Segmentation).</p> </li> <li> <p>Sample:</p> </li> <li> <p>Sample Annotations: Automatically generate Patch annotations.</p> <ul> <li>Sampling Method: Choose between Random, Stratified Random, or Uniform distribution.</li> <li>Number of Annotations: Specify how many annotations to generate.</li> <li>Annotation Size: Set the size of the generated patch annotations.</li> <li>Label As: Choose which label to assign to generated annotations.</li> <li>Apply Options: Apply sampling to current image, filtered images, previous/next images, or all images.</li> <li>Exclude Regions: Option to prevent sampling in areas with existing annotations.</li> <li>Margins: Define image boundary constraints for sampling:</li> <li>Set margins in pixels or percentage</li> <li>Configure different values for top, right, bottom, and left edges</li> <li>Annotations will only be placed within these margins</li> </ul> </li> <li> <p>Tile:</p> </li> <li>Tile Dataset: Tile existing Classification, Detection or Segmention datasets using <code>yolo-tiling</code>.</li> <li> <p>Tile Inference: Pre-compute multiple work areas for the current image.</p> </li> <li> <p>CoralNet: </p> </li> <li>Authenticate: Authenticate with CoralNet.<ul> <li>Enter your CoralNet username and password to access your sources.</li> <li>Authentication is required before downloading any CoralNet data.</li> </ul> </li> <li> <p>Download: Download data from CoralNet.</p> <ul> <li>Source ID: Enter the Source ID (or multiple IDs separated by commas).</li> <li>Output Directory: Select where to save downloaded files.</li> <li>Download Options: Choose what to download:</li> <li>Metadata: Source information and settings</li> <li>Labelset: All available labels from the source</li> <li>Annotations: Point annotations with their labels</li> <li>Images: Original images from the source</li> <li>Parameters: Configure download settings:</li> <li>Image Fetch Rate: Time between image downloads (seconds)</li> <li>Image Fetch Break Time: Pause duration between batches (seconds)</li> <li>Debug Mode: Toggle headless browser mode for troubleshooting.</li> </ul> </li> <li> <p>Ultralytics:</p> </li> <li>Merge Datasets: Merge multiple Classification datasets.</li> <li>Train Model: Train a machine learning model.</li> <li>Evaluate Model: Evaluate a trained model.</li> <li>Optimize Model: Convert model format.</li> <li>Deploy Model: Make predictions using a trained model (Classification, Detection, Segmentation).</li> <li> <p>Batch Inference: Perform batch inferences.</p> </li> <li> <p>SAM:</p> </li> <li>Deploy Predictor: Deploy <code>EdgeSAM</code>, <code>MobileSAM</code>, <code>SAM</code>, etc, to use interactively (points, box).</li> <li>Deploy Generator: Deploy <code>FastSAM</code> to automatically segment the image.</li> <li> <p>Batch Inference: Perform batch inferencing using <code>FastSAM</code>.</p> </li> <li> <p>See Anything (YOLOE):</p> </li> <li>Train Model: Train a YOLOE Segmentation model using an existing YOLO Segmentation dataset.</li> <li>Deploy Predictor: Deploy an existing <code>YOLOE</code> model to use interactively.</li> <li> <p>Batch Inference: Perform batch inferencing using <code>YOLOE</code>.</p> </li> <li> <p>AutoDistill:</p> </li> <li>Deploy Model: Deploy a foundational model<ul> <li>Models Available: <code>Grounding DINO</code>, <code>OWLViT</code></li> </ul> </li> <li>Batch Inference: Perform batch inferences.</li> </ul>"},{"location":"usage/#tool-bar-tools","title":"Tool Bar Tools","text":"<ul> <li>Select Tool: After selecting the tool</li> <li>Left-Click: Select an annotation; drag to move it.</li> <li>Ctrl + Left-Click: Add/remove annotation to current selection.</li> <li>Ctrl + Drag: Create rectangle selection to select multiple annotations.</li> <li>Delete / Backspace: Remove selected annotation(s).</li> <li>Ctrl + Shift: Show resize handles for the selected annotation.</li> <li>Ctrl + Mouse Wheel: Change size of the selected annotation.</li> <li>Ctrl + Space: Confirm prediction for selected annotation with top machine confidence.</li> <li>Ctrl + X: Enter cutting mode; left-click to start, draw line, left-click to end.</li> <li>Backspace/Ctrl + X: Cancel cutting mode.<ul> <li>MultiPolygonAnnotations: Break apart each PolygonAnnotation.    </li> </ul> </li> <li> <p>Ctrl + C: Combine multiple selected annotations (if same type and label).</p> <ul> <li>Combining Rules: </li> <li>All selected annotations must have the same label.</li> <li>All selected annotations must be verified (not machine predictions).</li> <li>RectangleAnnotations can only be combined with other rectangles.</li> <li>PatchAnnotations can be combined with other patches or polygons to create polygons.</li> <li>PolygonAnnotations can be combined with other overlapping polygons to create a polygon.</li> <li>MultiPolygonAnnotations can be made with multiple non-overlapping polygons.</li> </ul> </li> <li> <p>Patch Tool: After selecting the tool</p> </li> <li>Left-Click: Add a patch annotation at the clicked position.</li> <li>Ctrl + Mouse Wheel: Adjust the patch size up or down.</li> <li> <p>Mouse Movement: Shows a semi-transparent preview of the patch at the cursor position.</p> </li> <li> <p>Rectangle Tool: After selecting the tool</p> </li> <li>Left-Click: Start drawing a rectangle; click again to finish.</li> <li>Mouse Movement: Shows a preview of the rectangle while drawing.</li> <li> <p>Backspace: Cancel the current rectangle annotation.</p> </li> <li> <p>Polygon Tool: After selecting the tool</p> </li> <li>Left-Click (first): Start drawing a polygon.</li> <li>Left-Click (subsequent): Add points to the polygon; click near the first point to close.</li> <li>Ctrl + Left-Click: Enable straight line mode; click to add straight line segments.</li> <li>Mouse Movement: Shows a preview of the polygon as you draw.</li> <li> <p>Backspace: Cancel the current polygon annotation.</p> </li> <li> <p>SAM Tool: After a model is loaded</p> </li> <li>Space Bar: Set working area; confirm prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li>Ctrl + Left-Click: Add positive point.</li> <li>Ctrl + Right-Click: Add negative point.</li> <li> <p>Backspace: Discard unfinalized predictions.</p> </li> <li> <p>See Anything (YOLOE) Tool: After a model is loaded</p> </li> <li>Space Bar: Set working area; run prediction; finalize predictions and exit working area.</li> <li>Left-Click: Start a box; press again to end a box.</li> <li> <p>Backspace: Discard unfinalized predictions.</p> </li> <li> <p>Work Area Tool: For creating restricted areas for model prediction</p> </li> <li>Left-Click: Start drawing a work area; click again to finish drawing.</li> <li>Backspace: Cancel drawing the current work area.</li> <li>Ctrl + Space: Create a work area from the current view.</li> <li>Ctrl + Alt: Create temporary work area from current view (disappears when keys released / pressed again).</li> <li>Ctrl + Shift: Show removal buttons on existing work areas (click the \"X\" to remove).</li> <li>Ctrl + Shift + Backspace: Remove all work areas in the current image.</li> <li>Practical Use:<ul> <li>Define specific regions where models should make predictions.</li> <li>Useful for processing only relevant parts of large images.</li> <li>Work areas persist between tool changes and sessions.</li> </ul> </li> </ul>"},{"location":"usage/#status-bar","title":"Status Bar","text":"<ul> <li>Image Size: Displays the image size.</li> <li>Cursor Position: Displays the cursor position.</li> <li>View Extent: Displays the view extent.</li> <li>Annotation Transparency: Adjust the annotation transparency.</li> <li>Select All Labels: Select all labels, adjusting transparency for all labels.</li> <li>Unselect All Labels: Unselect all labels, adjusting transparency for only selected labels.</li> <li>Patch Size: Manipulate Patch Size (only active when using Patch Tool).</li> <li>Parameters: Adjust parameters including uncertainty, IoU, and area thresholds.</li> </ul>"},{"location":"usage/#annotation-window","title":"Annotation Window","text":"<ul> <li>Zoom: Use the mouse wheel to zoom in and out.</li> <li>Pan: Hold Ctrl + Right-click the mouse button to pan the image.</li> </ul>"},{"location":"usage/#label-window","title":"Label Window","text":"<ul> <li>Move Label: Right-click and drag to move labels.</li> <li>Add Label: Click the \"Add Label\" button to add a new label.</li> <li>Delete Label: Click the \"Delete Label\" button to delete the selected label.</li> <li>Edit Label: Click the \"Edit Label\" button to edit the selected label.</li> <li>Lock Label: Click the \"Lock Label\" button to lock the selected label.</li> <li>Filter Labels: Use the filter text box to search for specific labels.</li> <li>Label Count: Displays the total number of labels in the project.</li> <li>Annotation Count: Shows information about the current annotations:</li> <li>When no annotation is selected: Shows the total count of annotations.</li> <li>When a single annotation is selected: Shows the selected annotation's index.</li> <li>When multiple annotations are selected: Shows how many annotations are selected.</li> <li>Can be edited (when in select mode) to navigate to a specific annotation by index.</li> </ul>"},{"location":"usage/#image-window","title":"Image Window","text":"<ul> <li>Select Image: Double-click on a row to select and load the image in the annotation window.</li> <li>Highlight Image: Single-click on a row to highlight one or more rows in the image window.</li> <li>Ctrl + Left-click: Select multiple, non-adjacent rows.</li> <li>Shift + Left-click: Select multiple, adjacent rows.</li> <li>Open Context Menu:</li> <li>Right-click on a single highlighted row: Delete images / annotations for the highlighted row.</li> <li>Shift + Right-click on multiple highlighted rows: Delete images / annotations for highlighted rows.</li> <li>Search / Filter:</li> <li>By Image: Filter for images by name or sub-string.</li> <li>By Label: Filter images by labels they contain.</li> <li>No Annotations: Filter images with no annotations.</li> <li>Has Annotations: Filter images with annotations.</li> <li>Has Predictions: Filter images with predictions.</li> <li>Highlighted: Filter highlighted images.</li> <li>Navigation:</li> <li>Home Button: Click to center the table on the currently selected image.</li> <li>Highlight All: Highlight all images in the current filtered view.</li> <li>Unhighlight All: Unhighlight all images in the current filtered view.</li> <li>Image Preview:</li> <li>Tool Tip: Hover over a row to show image metadata.</li> <li>Thumbnail: Hold Ctrl while hovering over a row to show a thumbnail.</li> </ul>"},{"location":"usage/#confidence-window","title":"Confidence Window","text":"<ul> <li>Display Cropped Image: Shows the cropped image of the selected annotation.</li> <li>The dimensions shown include both original and scaled sizes when applicable.</li> <li>The border of the image is highlighted with the color of the top confident label.</li> <li>Confidence Chart: Displays a bar chart with confidence scores.</li> <li>Top 5 Predictions: Shows up to 5 predictions with their confidence scores.</li> <li>Prediction Selection: Click on any confidence bar to change the annotation's label.</li> <li>Numerical Keys: Press keys 1-5 to quickly select from the top 5 predictions.</li> <li>Confidence Mode Toggle: </li> <li>Click the icon button next to the dimensions to toggle between user and machine confidence views.</li> <li>User icon shows user-assigned confidence scores.</li> <li>Machine icon shows model-predicted confidence scores.</li> <li>The toggle is only enabled when both user and machine confidences are available.</li> <li>Visual Indicators:</li> <li>Each confidence bar shows the label color and confidence percentage.</li> <li>Numbered indicators (1-5) show the rank of each prediction.</li> <li>Hover over confidence bars to see a pointing hand cursor when selection is possible.</li> </ul>"},{"location":"usage/#secret-hotkeys","title":"Secret Hotkeys","text":"<ul> <li>Escape: Exit the program.</li> <li>Alt + Up/Down: Cycle through images.</li> <li>Ctrl + W/A/S/D: Cycle through labels.</li> <li>Ctrl + Left/Right: Cycle through annotations.</li> <li>Ctrl + Shift + &lt;: Select all annotations.</li> <li>Ctrl + Shift + &gt;: Unselect all annotations.</li> <li>Ctrl + Tab: Switch from SelectTool to the Annotation tool that matches the currently selected (1) Annotation</li> <li>When a PatchAnnotation is selected, switches back to the PatchTool</li> <li> <p>When the PatchTool is active, switches to the SelectTool</p> </li> <li> <p>Machine Learning, SAM, and AutoDistill: After a model is loaded</p> </li> <li>Ctrl + 1: Make prediction on selected Patch annotation, else all in the image with Review label.</li> <li>Ctrl + 2: Make predictions using Object Detection model.</li> <li>Ctrl + 3: Make predictions using Instance Segmentation model.</li> <li>Ctrl + 4: Make predictions using FastSAM model.</li> <li>Ctrl + 5: Make predictions using AutoDistill model.</li> </ul>"}]}